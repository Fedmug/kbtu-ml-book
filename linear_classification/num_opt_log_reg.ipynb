{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numeric optimization\n",
    "\n",
    "To find the optimal weights of the logistic regression, we can use {prf:ref}`gradient descent <GD>` algorithm. To apply this algorithm, one need to calculate the gradient of the loss function.\n",
    "\n",
    "## Binary logistic regression\n",
    "\n",
    "Multiply the loss function {eq}`bin-log-reg-loss` by $n$:\n",
    "\n",
    "$$\n",
    "\\mathcal L(\\boldsymbol w) = \n",
    "-\\sum\\limits_{i=1}^n \\big(y_i \\log(\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)) + (1- y_i)\\log(1 - \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w))\\big).\n",
    "$$\n",
    "\n",
    "To find $\\nabla \\mathcal L(\\boldsymbol w)$ observe that\n",
    "\n",
    "$$\n",
    "   \\nabla \\log(\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)) = \\frac {\\nabla \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)}{\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)} = \n",
    "   \\frac{\\sigma'(\\boldsymbol x_i^\\top \\boldsymbol w) \\nabla(\\boldsymbol x_i^\\top \\boldsymbol w)}{{\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)}}.\n",
    "$$\n",
    "\n",
    "Also,\n",
    "\n",
    "$$\n",
    "   \\nabla \\log(1 - \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)) = -\\frac {\\nabla  \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)}{1 - \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)} = \n",
    "   \\frac{\\sigma'(\\boldsymbol x_i^\\top \\boldsymbol w) \\nabla(\\boldsymbol x_i^\\top \\boldsymbol w)}{{1 - \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)}}.\n",
    "$$\n",
    "\n",
    "**Q**. What is $\\nabla(\\boldsymbol x_i^\\top \\boldsymbol w)$?\n",
    "\n",
    "Putting it altogeter, we get\n",
    "\n",
    "$$\n",
    "   \\nabla \\mathcal L(\\boldsymbol w) = -\\sum\\limits_{i=1}^n \\big(y_i(1 - \\sigma(\\boldsymbol x_i^\\top \\boldsymbol w))\\boldsymbol x_i - (1-y_i)\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w)\\boldsymbol x_i\\big) = \\sum\\limits_{i=1}^n (\\sigma(\\boldsymbol x_i^\\top \\boldsymbol w) - y_i)\\boldsymbol x_i.\n",
    "$$\n",
    "\n",
    "````{admonition} Question\n",
    ":class: important\n",
    "How to write $\\nabla \\mathcal L(\\boldsymbol w)$ as a product of a matrix and a vector, avoiding the explicit summation?\n",
    "\n",
    "```{hint}\n",
    ":class: dropdown\n",
    "The shape of $\\nabla \\mathcal L(\\boldsymbol w)$ is the same as of $\\boldsymbol w$, i.e., $d\\times 1$. Now observe that\n",
    "\n",
    "$$\n",
    "   \\begin{pmatrix}\n",
    "   \\sigma(\\boldsymbol x_1^\\top \\boldsymbol w) - y_1 \\\\\n",
    "   \\vdots \\\\\n",
    "   \\sigma(\\boldsymbol x_n^\\top \\boldsymbol w) - y_n\n",
    "   \\end{pmatrix}\n",
    "   = \\sigma(\\boldsymbol X \\boldsymbol w )- \\boldsymbol y \\in \\mathbb R^n.\n",
    "$$\n",
    "\n",
    "What should we multiply by this vector to obtain $\\nabla \\mathcal L$?\n",
    "```\n",
    "````\n",
    "\n",
    "````{admonition} Question\n",
    ":class: important\n",
    " What is hessian $\\nabla^2 L(\\boldsymbol w)$?\n",
    "\n",
    "```{admonition} Answer\n",
    ":class: tip, dropdown\n",
    "$$\n",
    "\\nabla^2 L(\\boldsymbol w) = \\boldsymbol X^\\top \\boldsymbol S \\boldsymbol X,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "   \\boldsymbol S = \\mathrm{diag}\\{\\sigma(\\boldsymbol X \\boldsymbol w )- \\boldsymbol y\\} = \\begin{pmatrix}\n",
    "   \\sigma(\\boldsymbol x_1^{\\boldsymbol{\\top}} \\boldsymbol w) - y_1  & \\ldots & 0 \\\\\n",
    "   \\vdots & \\ddots & \\vdots \\\\\n",
    "   0 & \\ldots & \\sigma(\\boldsymbol x_n^{\\boldsymbol{\\top}} \\boldsymbol w) - y_n\n",
    "   \\end{pmatrix}\n",
    "$$\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast cancer dataset: numeric optimization \n",
    "\n",
    "Fetch the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the {prf:ref}`gradient descent <GD>` algorithm to the logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "def logistic_regression_gd(X, y, C=1, learning_rate=0.01, tol=1e-3, max_iter=10000):\n",
    "    w = np.random.normal(size=X.shape[1])\n",
    "    gradient = X.T.dot(expit(X.dot(w)) - y) + C * w\n",
    "    for i in range(max_iter):\n",
    "        if np.linalg.norm(gradient) <= tol:\n",
    "            return w\n",
    "        w -= learning_rate * gradient\n",
    "        gradient = X.T.dot(expit(X.dot(w)) - y) + C * w\n",
    "    print(\"max_iter exceeded\")\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the logistic regresion on the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter exceeded\n",
      "CPU times: user 1min 1s, sys: 5.17 s, total: 1min 6s\n",
      "Wall time: 36.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.07357527,  0.31060659,  0.6663033 , -0.02618116,  0.08298685,\n",
       "       -0.96314197, -1.15776337, -0.94432551, -0.76477366,  0.15558787,\n",
       "       -0.19618218,  1.11058726, -1.01542852, -0.12326595,  0.06391155,\n",
       "       -0.10387021,  0.35510887,  1.54255196, -0.90647454, -0.12195762,\n",
       "        1.65596306, -0.68388714, -0.4705366 , -0.04170157,  0.1577867 ,\n",
       "        0.57481927, -0.57937639, -0.2332493 ,  1.25820274, -0.39930211])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time w = logistic_regression_gd(X, y, learning_rate=2e-7, max_iter=10**6)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9490333919156415"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(expit(X.dot(w)) > 0.5, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9595782073813708\n",
      "0.9595782073813708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.18332329,  0.11789056, -0.0696952 , -0.00350176, -0.16519865,\n",
       "        -0.4118028 , -0.67385012, -0.36497435, -0.23983217, -0.02406538,\n",
       "        -0.02186803,  1.20613335,  0.04838948, -0.0970984 , -0.01870574,\n",
       "         0.01626219, -0.03861864, -0.04276028, -0.04322983,  0.00846606,\n",
       "         1.29672397, -0.34212421, -0.12576754, -0.02460068, -0.30612789,\n",
       "        -1.14244163, -1.62895872, -0.6992171 , -0.72520831, -0.11275416]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(fit_intercept=False, max_iter=5000)\n",
    "log_reg.fit(X, y)\n",
    "print(log_reg.score(X, y))\n",
    "print(accuracy_score(log_reg.predict(X), y))\n",
    "log_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9143604758471366"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(w - log_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial logistic regression\n",
    "\n",
    "Recall that the loss function in this case is\n",
    "\n",
    "$$\n",
    "    \\begin{multline*}\n",
    "    \\mathcal L(\\boldsymbol W) = -\\sum\\limits_{i=1}^n \\sum\\limits_{k=1}^K  y_{ik} \\bigg(\\boldsymbol x_i^\\top\\boldsymbol w_{k} -\\log\\Big(\\sum\\limits_{k=1}^K \\exp(\\boldsymbol x_i^\\top\\boldsymbol w_{k})\\Big)\\bigg) = \\\\\n",
    "    =\n",
    "    -\\sum\\limits_{i=1}^n \\sum\\limits_{k=1}^K  y_{ik} \\bigg(\\sum\\limits_{j=1}^d x_{ij} w_{jk} -\\log\\Big(\\sum\\limits_{k=1}^K \\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)\\Big)\\bigg)\n",
    "    \\end{multline*}\n",
    "$$\n",
    "\n",
    "One can show that \n",
    "\n",
    "$$\n",
    "    \\nabla \\mathcal L(\\boldsymbol W) = \\boldsymbol X^\\top (\\boldsymbol {\\widehat Y} - \\boldsymbol Y) = \\boldsymbol X^\\top ( \\sigma(\\boldsymbol{XW}) - \\boldsymbol Y).\n",
    "$$\n",
    "\n",
    "<!-- Observe that\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial}{\\partial w_{pq}} (x_{ij} w_{jk}) = x_{ij} \\delta_{pj} \\delta_{qk},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_{pq}}\\bigg(\\log\\Big(\\sum\\limits_{k=1}^K \\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)\\Big)\\bigg) = \\frac{\\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)}{\\sum\\limits_{k=1}^K \\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)} x_{ip} \\delta_{qk}\n",
    "$$\n",
    "\n",
    "Hence, \n",
    "\n",
    "$$\n",
    "    \\frac{\\partial \\mathcal L}{\\partial w_{pq}} = \\sum\\limits_{i=1}^n \\sum\\limits_{k=1}^K y_{ik}\\bigg(\\sum\\limits_{j=1}^d  \\bigg(  x_{ij} \\delta_{pj} \\delta_{qk} - \\frac{\\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)}{\\sum\\limits_{k=1}^K \\exp\\Big(\\sum\\limits_{j=1}^d x_{ij} w_{jk}\\Big)} x_{ip} \\delta_{qk}\\bigg)\\bigg)\n",
    "$$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} TODO\n",
    ":class: warning\n",
    "* Try numerical optimization on several datasets\n",
    "* Apply Newton's method and compare it's performance with GD\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
