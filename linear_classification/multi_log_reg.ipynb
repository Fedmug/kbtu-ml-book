{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Multinomial Logistic Regression\n",
    "\n",
    "Logistic regression has been used in the field of biological research since the early twentieth century. Then it began to be used in many social sciences. Logistic regression is applicable when the dependent variable (target value) is categorical.\n",
    "\n",
    "For example, we need to predict:\n",
    "\n",
    "- whether the email is spam (1) or not (0);\n",
    "- whether the tumor is malignant (1) or benign (0).\n",
    "\n",
    "$\\boldsymbol{Multinomial}$ $\\boldsymbol{logistic}$ $\\boldsymbol{regression}$ is a statistical method used for classification problems where the outcome can take on more than two categories. It's an extension of binary logistic regression. The goal is to model the probabilities of the different possible outcomes of a categorically distributed dependent variable, given a set of independent variables.\n",
    "\n",
    "```{note} Let's look at the following examples:\n",
    "- Classification of Texts: Determining the topic of the text (for example, sports, politics, technology, art).\n",
    "\n",
    "- Medical Diagnosis: Classification of the type of disease (eg, infectious, inflammatory, genetic, metabolic).\n",
    "```\n",
    "\n",
    "In each of these scenarios, the outcome is multiple categories, and multinomial logistic regression can be used to predict the probability of each category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](https://ekababisong.org/assets/seminar_IEEE/multinomial-example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Data Structure  \n",
    "Let's imagine a data set $\\mathcal D$, where each element consists of a pair of feature vectors $\\boldsymbol x_i$ and a class label $y_i$:\n",
    "\n",
    "Let $\\mathcal{D}$ be a dataset defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{(\\boldsymbol{x}_i, y_i)\\}_{i=1}^n \\quad \\text{where } \\boldsymbol{x}_i \\in \\mathbb{R}^d \\text{ and } y_i \\in \\mathcal{Y}.\n",
    "$$\n",
    "\n",
    "Here $\\mathcal Y$ —  is the set of possible categories.\n",
    "\n",
    "##### Probability Prediction\n",
    "Multinomial logistic regression predicts the probability of membership in each category:\n",
    "\n",
    "The predicted probability vector $\\boldsymbol{\\hat{y}}$ is defined as:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\hat{y}} = (p_1, \\ldots, p_K), \\quad \\text{where } p_k > 0 \\text{ and } \\sum_{k=1}^K p_k = 1.\n",
    "$$\n",
    "\n",
    "\n",
    "##### Calculating Logits and Softmax Transformation\n",
    "Logits are calculated as a linear combination of the input features and then converted to probabilities using the softmax function:\n",
    "\n",
    "The logits $\\boldsymbol{z}$ are calculated as a linear combination of the input features $\\boldsymbol{x}$ and the weights $\\boldsymbol{w}_k$ for each class, and the predicted probability vector $\\boldsymbol{\\hat{y}}$ is obtained through the softmax function:\n",
    "\n",
    "$$\n",
    "z_k = \\boldsymbol{x}^\\top \\boldsymbol{w}_k, \\quad \\boldsymbol{\\hat{y}} = \\text{Softmax}(\\boldsymbol{z}) = \\left( \\frac{e^{z_1}}{\\sum_{k=1}^K e^{z_k}}, \\ldots , \\frac{e^{z_K}}{\\sum_{k=1}^K e^{z_k}} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "##### Class Selection\n",
    "The predicted class is given by the argument that maximizes the probabilities:\n",
    "\n",
    "$$\n",
    "\\text{Predicted class} = \\arg\\max_{1 \\leq k \\leq K} p_k\n",
    "$$\n",
    "\n",
    "##### Model parameters\n",
    "The weight matrix $\\boldsymbol{W}$ is defined as:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{W} = [\\boldsymbol{w}_1 \\ldots \\boldsymbol{w}_K]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz: Basic Concepts of Multinomial Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyterquiz import display_quiz\n",
    "display_quiz(\"quizzes/intro.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-03T13:26:58.810510Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "X, Y = fetch_openml('mnist_784', return_X_y=True, parser='auto')\n",
    "\n",
    "X = X.astype(float).values / 255\n",
    "Y = Y.astype(int).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784), (60000,), (10000,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=10000)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " array([ 949, 1143, 1023, 1024,  998,  877,  969, 1051,  994,  972]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fitting and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, labels):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred, model.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction Probability Distribution for a Selected Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_probabilities(model, X_test, class_idx):\n",
    "    probabilities = model.predict_proba(X_test)[:, class_idx]\n",
    "    sns.histplot(probabilities, kde=True, bins=30)\n",
    "    plt.title(f'Predicted Probability Distribution for Class {class_idx}')\n",
    "    plt.xlabel('Probability')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "plot_prediction_probabilities(model, X_test, class_idx=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample Digit Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(samples, labels, n_rows=2, n_cols=5):\n",
    "    plt.figure(figsize=(n_cols * 2, n_rows * 2))\n",
    "    for i in range(n_rows * n_cols):\n",
    "        plt.subplot(n_rows, n_cols, i + 1)\n",
    "        plt.imshow(samples[i].reshape(28, 28), cmap='gray')\n",
    "        plt.title(f\"Digit: {labels[i]}\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_digits(X_test[:10], y_pred[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Feature Distribution Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_pixel_intensity_distribution(X_test):\n",
    "    # Flatten the images to get the distribution of pixel values\n",
    "    pixel_values = X_test.flatten()\n",
    "\n",
    "    # Plotting the distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(pixel_values, bins=50, kde=True)\n",
    "    plt.title('Distribution of Pixel Intensities in MNIST Dataset')\n",
    "    plt.xlabel('Pixel Intensity (normalized)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with the MNIST data\n",
    "plot_pixel_intensity_distribution(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Mathematical Foundations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Problem: Predicting Election Outcomes\n",
    "Imagine a political analyst wants to predict the outcome of a future election based on various socioeconomic indicators. There are three political parties (A, B, and C) that voters can choose from. The analyst has collected a dataset containing previous election outcomes and corresponding indicators such as economic growth, unemployment rate, and education level.\n",
    "\n",
    "##### Dataset Structure\n",
    "The dataset $\\mathcal{D}$ consists of $n$ samples with the following structure:\n",
    "\n",
    "- Economic growth rate ($x_1$)\n",
    "- Unemployment rate ($x_2$)\n",
    "- Average education level ($x_3$)\n",
    "- Outcome of the election (Party A, Party B, Party C)\n",
    "- Each instance in the dataset is a vector $\\boldsymbol{x}_i \\in \\mathbb{R}^3$, and the corresponding label $y_i$ is one of ${1, 2, 3}$, representing the three parties.\n",
    "\n",
    "##### Problem Formulation\n",
    "The multinomial logistic regression model can be applied to this problem to estimate the probability of each election outcome based on the socioeconomic indicators.\n",
    "\n",
    "##### Mathematical Formulation\n",
    "For a given instance $\\boldsymbol{x}$, the probability that the outcome is for party $k$ is modeled as:\n",
    "\n",
    "\n",
    "$$\n",
    "P(y = k|\\boldsymbol{x}) = \\frac{\\exp(\\boldsymbol{x}^\\top \\boldsymbol{w}_k)}{\\sum_{j=1}^{3} \\exp(\\boldsymbol{x}^\\top \\boldsymbol{w}_j)}\n",
    "$$\n",
    "\n",
    "Here, $\\boldsymbol{w}_k$ represents the weight vector associated with party $k$, which the model will learn during training.\n",
    "\n",
    "##### Training the Model\n",
    "Using historical election data, the analyst trains a multinomial logistic regression model. The model estimates the parameters $\\boldsymbol{w}_k$ for each party by maximizing the likelihood of the observed data.\n",
    "\n",
    "##### Making Predictions\n",
    "Once trained, the model can predict the probability distribution over the three parties for a new set of indicators. If a new data point has indicators $\\boldsymbol{x}_{\\text{new}} = [2.5%, 5%, 12 \\text{ years}]$, the model will provide three probabilities corresponding to the likelihood of each party winning the election.\n",
    "\n",
    "\n",
    "```{admonition} Solution\n",
    "The analyst trains the model and makes predictions for a new set of indicators. The output probabilities might look something like this:\n",
    "\n",
    "$$P(\\text{Party A}|\\boldsymbol{x}_{\\text{new}}) = 0.70$$\n",
    "$$P(\\text{Party B}|\\boldsymbol{x}_{\\text{new}}) = 0.20$$\n",
    "$$P(\\text{Party C}|\\boldsymbol{x}_{\\text{new}}) = 0.10$$\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz: Mathematics-focused questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyterquiz import display_quiz\n",
    "display_quiz(\"quizzes/math.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preparation for Multinomial Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Building\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Case Study: Real-world Application\n",
    "\n",
    "Multinomial Logistic Regression (MLR) is a powerful statistical method used in various real-world scenarios where the outcome variable can take more than two categories. This versatility makes it an invaluable tool across multiple disciplines. We will explore a case study to illustrate its practical application. There are some case studies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adjusting the HTML content for center alignment of all texts in the cards\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "html_content_fully_center_aligned = \"\"\"\n",
    "<div style='display: grid; grid-template-columns: repeat(4, 1fr); grid-gap: 15px; padding: 10px;'>\n",
    "    <div style='border: 2px solid #4CAF50; border-radius: 10px; background-color: #E8F5E9; padding: 20px; text-align: center;'>\n",
    "        <h3 style='color: #388E3C;'>Case study 1</h3>\n",
    "        <p style='color: #388E3C; font-weight: bold; text-align: center;'>Diagnosing Medical Conditions</p>\n",
    "    </div>\n",
    "    <div style='border: 2px solid #2196F3; border-radius: 10px; background-color: #E3F2FD; padding: 20px; text-align: center;'>\n",
    "        <h3 style='color: #1565C0;'>Case study 2</h3>\n",
    "        <p style='color: #1565C0; font-weight: bold; text-align: center;'>Customer Segmentation and Targeting</p>\n",
    "    </div>\n",
    "    <div style='border: 2px solid #FFC107; border-radius: 10px; background-color: #FFF8E1; padding: 20px; text-align: center;'>\n",
    "        <h3 style='color: #FFA000;'>Case study 3</h3>\n",
    "        <p style='color: #FFA000; font-weight: bold; text-align: center;'>Credit Risk Assessment</p>\n",
    "    </div>\n",
    "    <div style='border: 2px solid #FF5722; border-radius: 10px; background-color: #FBE9E7; padding: 20px; text-align: center;'>\n",
    "        <h3 style='color: #D84315;'>Case study 4</h3>\n",
    "        <p style='color: #D84315; font-weight: bold; text-align: center;'>Predicting Student Performance</p>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# Displaying the fully center-aligned HTML content\n",
    "display(HTML(html_content_fully_center_aligned))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Case Study 1: Diagnosing Medical Conditions\n",
    "\n",
    "### Multinomial Logistic Regression in Medical Diagnosis\n",
    "\n",
    "A significant application of Multinomial Logistic Regression (MLR) is in the medical field, particularly in diagnosing diseases. This case study focuses on a healthcare setting where doctors diagnose patients with one of several possible diseases based on symptoms and test results.\n",
    "\n",
    "---\n",
    "\n",
    "#### Data Collection\n",
    "\n",
    "The collected data includes patient demographics, symptoms, medical history, and results from various tests. For instance, in a study on respiratory diseases, data might include symptoms like cough, fever, shortness of breath, and test results like X-rays or blood tests.\n",
    "\n",
    "---\n",
    "\n",
    "#### Model Development\n",
    "\n",
    "**Dependent Variable:** The type of respiratory disease (e.g., Asthma, Bronchitis, Pneumonia, None).\n",
    "\n",
    "**Independent Variables:** Symptoms, test results, age, gender, smoking history, etc.\n",
    "\n",
    "The MLR model is trained on this dataset. The training process involves adjusting the model's parameters to best fit the observed data, using methods like maximum likelihood estimation.\n",
    "\n",
    "---\n",
    "\n",
    "#### Application in Diagnosis\n",
    "\n",
    "Once the model is trained, it can be used in clinical settings. When a new patient arrives, their symptoms and test results are input into the model, which then predicts the probabilities of each disease. The disease with the highest probability is considered the most likely diagnosis.\n",
    "\n",
    "---\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- **Accuracy:** MLR can handle multiple influencing factors, leading to more accurate diagnoses.\n",
    "- **Preventive Care:** Identifying disease probabilities helps in preventive care and early intervention.\n",
    "\n",
    "---\n",
    "\n",
    "#### Challenges\n",
    "\n",
    "- **Data Quality:** The accuracy of the model heavily depends on the quality and quantity of the data.\n",
    "- **Complexity:** Medical data can be complex, and ensuring the model captures all relevant factors is challenging.\n",
    "\n",
    "---\n",
    "\n",
    "#### Results and Impact\n",
    "\n",
    "In practice, MLR models have shown promising results in improving diagnostic accuracy. They also aid in personalized patient care by considering a wide range of individual factors. This, in turn, leads to better treatment plans and outcomes.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study 2: Marketing - Customer Segmentation and Targeting\n",
    "\n",
    "### Context\n",
    "In marketing, understanding customer behavior is crucial for targeted campaigns. Businesses categorize customers into different segments based on purchasing patterns, interests, and demographics.\n",
    "\n",
    "#### Data Collection\n",
    "Data includes customer demographics, purchase history, browsing behavior, and engagement with marketing campaigns.\n",
    "\n",
    "#### Model Development\n",
    "**Dependent Variable:** Customer segment (e.g., High-Spenders, Bargain Hunters, Occasional Shoppers, etc.).\n",
    "**Independent Variables:** Age, income, browsing history, past purchases, and responses to previous marketing campaigns.\n",
    "\n",
    "#### Advantages\n",
    "- **Effective Targeting:** Enhances the ability to target marketing efforts more effectively.\n",
    "- **Increased ROI:** Better targeting leads to higher returns on investment for marketing campaigns.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study 3: Financial Services - Credit Risk Assessment\n",
    "\n",
    "### Context\n",
    "Banks and financial institutions use MLR to assess the credit risk associated with lending to individuals or businesses.\n",
    "\n",
    "#### Data Collection\n",
    "Data involves credit history, repayment records, income levels, and other financial metrics.\n",
    "\n",
    "#### Model Development\n",
    "**Dependent Variable:** Risk category (e.g., Low, Medium, High risk).\n",
    "**Independent Variables:** Credit score, debt-to-income ratio, employment status, etc.\n",
    "\n",
    "#### Advantages\n",
    "- **Risk Mitigation:** Helps in identifying high-risk applicants, reducing defaults.\n",
    "- **Customized Lending:** Enables offering tailored interest rates based on risk profiles.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study 4: Education - Predicting Student Performance\n",
    "\n",
    "### Context\n",
    "Educational institutions use MLR to predict student performance and identify those who may need additional support.\n",
    "\n",
    "#### Data Collection\n",
    "Data includes previous grades, attendance records, participation in extracurricular activities, and demographic information.\n",
    "\n",
    "#### Model Development\n",
    "**Dependent Variable:** Performance category (e.g., High Achiever, Average, Below Average).\n",
    "**Independent Variables:** Past academic performance, attendance, socio-economic status, etc.\n",
    "\n",
    "#### Advantages\n",
    "- **Support for Students:** Facilitates targeted support for students who might struggle.\n",
    "- **Curriculum Development:** Assists in adjusting teaching methods and curriculum based on student needs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Customer Segmentation in Marketing Using MLR\n",
    "Let's say we have a customer data set that includes information about demographics, purchasing behavior, and interactions with marketing campaigns. Our goal is to predict which segment each customer belongs to (e.g., \"High-Spenders\", \"Bargain Hunters\", \"Occasional Shoppers\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "np.random.seed(42)\n",
    "data = pd.DataFrame({\n",
    "    'Age': np.random.randint(18, 65, size=200),\n",
    "    'Income': np.random.randint(30000, 80000, size=200),\n",
    "    'Browsing_History': np.random.randint(0, 100, size=200),\n",
    "    'Past_Purchases': np.random.randint(0, 50, size=200),\n",
    "    'Customer_Segment': np.random.choice(['High-Spenders', 'Bargain Hunters', 'Occasional Shoppers'], size=200)\n",
    "})\n",
    "\n",
    "features = ['Age', 'Income', 'Browsing_History', 'Past_Purchases']\n",
    "target = 'Customer_Segment'\n",
    "\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code starts by generating data, then preprocesses it, trains an MLR model on that data, and finally visualizes the results of the model's predictions on a test data set. The error matrix shows how the model classifies different customer segments, which helps evaluate its effectiveness and prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyterquiz import display_quiz\n",
    "display_quiz(\"quizzes/chapter-6-quiz.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Common Pitfalls and Best Practices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Linear Regression (MLR) is a powerful statistical technique used for predicting a continuous dependent variable based on two or more independent variables. While it's widely used across various fields for its simplicity and interpretability, MLR is not without its challenges.\n",
    "\n",
    "This topic delves into the common pitfalls that practitioners often encounter when using MLR. These include overfitting, where a model is too complex for the data; underfitting, where the model is too simplistic; multicollinearity, where independent variables are highly correlated; and heteroscedasticity, where the variability of a variable is unequal across the range of values of a second variable. Each of these pitfalls can lead to inaccurate predictions and misleading conclusions if not properly addressed.\n",
    "\n",
    "However, understanding these challenges is only half the battle. The other half lies in adopting best practices to mitigate these issues. This includes rigorous data preparation, careful model validation, the application of regularization techniques, and a focus on model interpretability. Additionally, continuous model assessment ensures that the model remains relevant and accurate over time.\n",
    "\n",
    "In the following sections, we will explore each of these pitfalls and best practices in detail, equipping you with the knowledge to effectively use MLR in your analyses. Whether you're a seasoned statistician or a beginner in the field, understanding these aspects of MLR is crucial for developing robust and reliable models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Creating a graph object\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Adding nodes\n",
    "nodes = [\"MLR\", \"Overfitting/Underfitting\", \"Multicollinearity\", \"Heteroscedasticity\",\n",
    "         \"Non-linearity\", \"Data Quality\", \"Data Preparation\", \"Model Validation\",\n",
    "         \"Regularization\", \"Interpretability\", \"Continuous Assessment\"]\n",
    "G.add_nodes_from(nodes)\n",
    "\n",
    "# Adding edges\n",
    "edges = [(\"MLR\", \"Overfitting/Underfitting\"), (\"MLR\", \"Multicollinearity\"), (\"MLR\", \"Heteroscedasticity\"),\n",
    "         (\"MLR\", \"Non-linearity\"), (\"MLR\", \"Data Quality\"), (\"MLR\", \"Data Preparation\"),\n",
    "         (\"MLR\", \"Model Validation\"), (\"MLR\", \"Regularization\"), (\"MLR\", \"Interpretability\"),\n",
    "         (\"MLR\", \"Continuous Assessment\")]\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Adjusting the layout\n",
    "pos = nx.spring_layout(G, k=0.15)  # Decreasing the 'k' value to compact the layout\n",
    "\n",
    "# Drawing the graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=1800, edge_color='black', linewidths=1, font_size=13)\n",
    "plt.title(\"Multiple Linear Regression (MLR) Topics\", size=18)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting and Underfitting\n",
    "Overfitting occurs when an MLR model captures noise in the data instead of the underlying pattern. It performs well on training data but poorly on unseen data. Underfitting, on the other hand, happens when the model is too simple to capture the complexities of the data, leading to poor performance on both training and testing sets. Avoiding these issues requires a balance - not too complex, but not overly simplistic. Techniques like cross-validation and regularizing (Lasso, Ridge) can be employed to find the right model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity \n",
    "Multicollinearity in MLR refers to high correlations among independent variables, leading to unreliable and unstable coefficient estimates. This can make model interpretation difficult. Detecting multicollinearity can be done using Variance Inflation Factor (VIF) or correlation matrices. To address multicollinearity, consider removing highly correlated predictors, combining them, or using principal component analysis (PCA) for dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heteroscedasticity\n",
    "Heteroscedasticity refers to the condition where the variance of residuals (errors) is not constant across all levels of the independent variables. This violates one of the key assumptions of MLR and can lead to inefficient estimates. Visual inspection of residual plots can help detect heteroscedasticity. Remedies include transforming the dependent variable (e.g., using log transformation) or using heteroscedasticity-consistent standard error estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignoring Non-linearity \n",
    "Assuming a linear relationship between variables in MLR can be misleading if the true relationship is non-linear. This ignorance can lead to poor model performance and misleading conclusions. Residual plots can be used to detect non-linearity. Addressing non-linearity might involve transforming variables, adding polynomial terms, or considering non-linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Issues \n",
    "High-quality data is crucial for MLR. Issues like missing values, outliers, and erroneous data can significantly distort model outcomes. Effective strategies include thorough data cleaning, dealing with missing data through imputation or exclusion, and handling outliers by either removing them or using robust regression techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proper Data Preparation \n",
    "Effective MLR begins with proper data preparation. This involves exploring data to understand its characteristics, handling missing values and outliers, and ensuring that variables are appropriately scaled or transformed. Data preparation sets the stage for a more reliable and interpretable model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Validation Techniques \n",
    "Model validation is crucial to ensure that an MLR model generalizes well to new data. Techniques like the train-test split and k-fold cross-validation help in assessing model performance reliably. These methods provide a more accurate understanding of how the model will perform in real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization Techniques\n",
    "Regularization techniques such as Lasso, Ridge, and Elastic Net help prevent overfitting in MLR by penalizing large coefficients. These techniques are particularly useful when dealing with high-dimensional data or when multicollinearity is present. They help in selecting more significant features and improving model interpretability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Interpretability \n",
    "Interpretability is key in MLR. It involves understanding how the model makes predictions and explaining these in a meaningful way. Clear interpretation of coefficients, understanding the influence of each predictor, and communicating these findings in a non-technical manner are essential aspects of model interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Model Assessment \n",
    "Continuous assessment of an MLR model ensures its relevance over time. This involves regularly updating the model with new data, monitoring its performance, and making adjustments as necessary. This ongoing process helps in maintaining the accuracy and reliability of the model in changing conditions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these articles offers a focused overview of a specific aspect of MLR, providing valuable insights into both the challenges and effective strategies in the field of linear regression modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyterquiz import display_quiz\n",
    "display_quiz(\"quizzes/chapter-7-quiz.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Interactive Exercises\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_results(X_pca, y_true, y_pred):\n",
    "    # Create a figure without specifying subplot titles\n",
    "    fig = make_subplots(rows=1, cols=1)\n",
    "\n",
    "    # Add scatter plot for true labels\n",
    "    fig.add_trace(go.Scatter(x=X_pca[:, 0], y=X_pca[:, 1], mode='markers',\n",
    "                             marker=dict(color=y_true, size=5, colorscale='Viridis', symbol='circle'),\n",
    "                             name='True Label'))\n",
    "\n",
    "    # Add scatter plot for predicted labels\n",
    "    fig.add_trace(go.Scatter(x=X_pca[:, 0], y=X_pca[:, 1], mode='markers',\n",
    "                             marker=dict(color=y_pred, size=5, colorscale='Viridis', symbol='x'),\n",
    "                             name='Predicted Label'))\n",
    "\n",
    "    # Update layout with a main title instead of a subplot title\n",
    "    fig.update_layout(height=400, width=600, title_text=\"PCA of MNIST Data with True and Predicted Labels\")\n",
    "\n",
    "    # Show figure\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d0632b9a274e469f32e5697cf529e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatLogSlider(value=1.0, description='C (Regularization strength):', min=-4.0), IntSlid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Load and normalize MNIST data\n",
    "X, Y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False, parser='auto')\n",
    "X = X / 255.0\n",
    "Y = Y.astype(int)\n",
    "\n",
    "# Subsample the data for quicker processing\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "for train_index, _ in sss.split(X, Y):\n",
    "    X_sub, Y_sub = X[train_index], Y[train_index]\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=0.80)\n",
    "X_pca = pca.fit_transform(X_sub)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_pca)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, Y_sub, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the training and evaluation function\n",
    "def train_and_evaluate(C, max_iter, solver):\n",
    "    # Create logistic regression model\n",
    "    model = LogisticRegression(C=C, max_iter=max_iter, solver=solver, multi_class='multinomial')\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    # Predict the classes\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model trained with C={C}, max_iter={max_iter}, solver='{solver}'\")\n",
    "    print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    # Call the plot_results function\n",
    "    plot_results(X_test, y_test, y_pred)\n",
    "\n",
    "# Create interactive widgets for model parameters\n",
    "C_slider = widgets.FloatLogSlider(value=1.0, base=10, min=-4, max=4, step=0.1, description='C (Regularization strength):')\n",
    "max_iter_slider = widgets.IntSlider(value=1000, min=100, max=2000, step=100, description='Max Iterations:')\n",
    "solver_dropdown = widgets.Dropdown(options=['lbfgs', 'saga'], value='lbfgs', description='Solver:')\n",
    "\n",
    "# Display the widgets and link to the train_and_evaluate function\n",
    "interactive_plot = widgets.interactive(train_and_evaluate, C=C_slider, max_iter=max_iter_slider, solver=solver_dropdown)\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Conclusion and Further Resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, Multinomial Logistic Regression is an advanced statistical method that broadens the scope of logistic regression to address multiple category outcomes. It's particularly useful in fields requiring complex classification, such as text classification and medical diagnostics. The technique involves using feature vectors and class labels from a dataset, calculating logits, applying softmax to derive probabilities, and identifying the most probable class. This method effectively models the probabilities of each category in multi-category scenarios, showcasing its versatility and utility in various research and application domains.\n",
    "\n",
    "Building upon this foundation, the application of Multinomial Logistic Regression in predicting election outcomes showcases its effectiveness in handling complex, real-world problems. By leveraging socioeconomic indicators, such as economic growth, unemployment rates, and education levels, the model provides nuanced predictions of election results for multiple political parties. This approach, which learns and applies weight vectors specific to each party, offers valuable insights for political analysts, enabling them to forecast election outcomes with a higher degree of precision based on current socioeconomic data.\n",
    "\n",
    "Data Preparation for Multinomial Logistic Regression is a crucial step that involves cleaning and structuring data in a way that is suitable for the model. Key steps include handling missing values, encoding categorical variables, and normalizing features. Following this, Model Building focuses on constructing the multinomial logistic regression model, selecting relevant features, and defining the model's architecture. Then, during Model Evaluation, the model's performance is assessed using metrics such as accuracy, precision, recall, and confusion matrices to ensure its reliability and validity.\n",
    "\n",
    "Furthermore, the practical application of this model is demonstrated through a Case Study: Real-world Application, involving applying the model to a real-world scenario to demonstrate its practical utility and effectiveness in making predictions or classifications. However, it is important to be aware of Common Pitfalls and Best Practices, as this section highlights typical mistakes to avoid when working with multinomial logistic regression and provides best practices to improve model performance and accuracy.\n",
    "\n",
    "Understanding each of these steps is crucial for successfully applying multinomial logistic regression. From preparing data to evaluating and applying the model in real-world situations, each phase plays a significant role in ensuring the model is both accurate and applicable. Awareness of common pitfalls and adherence to best practices further enhances the model's effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
