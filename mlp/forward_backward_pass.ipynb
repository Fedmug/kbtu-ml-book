{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "### Binary classification\n",
    "\n",
    "The output of the neural network is usually a number from $[0, 1]$ which is the probability of the positive class. Sigmoid is the typical choice for the output layer:\n",
    "\n",
    "$$\n",
    "\\widehat y = x_L = x_{\\mathrm{out}} = \\sigma(x_{L-1})\n",
    "$$\n",
    "\n",
    "Loss function: \n",
    "\n",
    "$$\n",
    "\\mathcal L(\\widehat y, y) = -y\\log(\\widehat y) -(1-y) \\log(1-\\widehat y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classification\n",
    "\n",
    "* For $K$ classes the output contains $K$ numbers $(\\widehat y_1, \\ldots, \\widehat y_K)$\n",
    "* $\\widehat y_k$ is the probability of class $k$\n",
    "* Now the output of the neural network is\n",
    "\n",
    "$$\n",
    "    \\boldsymbol{\\widehat y} = \\boldsymbol x_L = \\boldsymbol x_{\\mathrm{out}} = \\mathrm{SoftMax}(\\boldsymbol x_{L-1}), \n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\mathrm{SoftMax}(\\boldsymbol z)_i = \\frac{e^{z_i}}{\\sum_i\n",
    " e^{z_i}}$$\n",
    " \n",
    " * Finally, plug the predictions into the cross-entropy loss:\n",
    " \n",
    " $$\n",
    "\\mathcal L(\\boldsymbol{\\widehat y}, \\boldsymbol y) = -\\sum\\limits_{k=1}^K y_k\\log(\\widehat y_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "\n",
    "* Predict a real number $\\widehat y = x_L = x_{\\mathrm{out}}$\n",
    "\n",
    "* The loss function is usually quadratic: \n",
    "\n",
    "$$\n",
    "\\mathcal L(\\widehat y, y) = (\\widehat y - y)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and backward pass\n",
    "\n",
    "```{figure} forward_pass.png\n",
    ":align: center\n",
    "```\n",
    "\n",
    "The goal is to minimize the loss function with respect to parameters $\\boldsymbol \\theta$,\n",
    "\n",
    "$$\n",
    "\\mathcal L = \\mathcal L_{\\boldsymbol\\theta}(\\boldsymbol {\\widehat y}, \\boldsymbol y) \\to \\min\\limits_{\\boldsymbol \\theta}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\boldsymbol \\theta = (\\boldsymbol W_1, \\boldsymbol b_1, \\boldsymbol W_2, \\boldsymbol b_2, \\ldots, \\boldsymbol W_L, \\boldsymbol b_L)\n",
    "$$\n",
    "\n",
    "Let's use the standard technique â€” the gradient descent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start from some random parameters $\\boldsymbol \\theta_0$\n",
    "\n",
    "2. Given a training sample $(\\boldsymbol x, \\boldsymbol y)$, do the **forward pass** and get the output $\\boldsymbol {\\widehat y} = F_{\\boldsymbol \\theta}(\\boldsymbol x)$\n",
    "\n",
    "3. Calculate the loss function\n",
    "$\\mathcal L_{\\boldsymbol\\theta}(\\boldsymbol {\\widehat y}, \\boldsymbol y)$\n",
    "\n",
    "4. Do the **backward pass** and calculate gradients\n",
    "\n",
    "    $$\n",
    "  \\nabla_{\\boldsymbol\\theta}\\mathcal L_{\\boldsymbol\\theta}(\\boldsymbol {\\widehat y}, \\boldsymbol y)  \n",
    "    $$\n",
    "\n",
    "    i.e., \n",
    "\n",
    "    $$\n",
    "\\nabla_{\\boldsymbol W_\\ell}\\mathcal L_{\\boldsymbol\\theta}(\\boldsymbol {\\widehat y}, \\boldsymbol y)\n",
    "\\text{ and }\n",
    "\\nabla_{\\boldsymbol b_\\ell}\\mathcal L_{\\boldsymbol\\theta}(\\boldsymbol {\\widehat y}, \\boldsymbol y), \\quad 1\\leqslant \\ell \\leqslant L.\n",
    "    $$\n",
    "\n",
    "4. Update the parameters:\n",
    "\n",
    "$$\n",
    "    \\boldsymbol \\theta = \\boldsymbol \\theta - \\eta \\nabla_{\\boldsymbol\\theta}\\mathcal L_{\\boldsymbol\\theta}(\\boldsymbol {\\widehat y}, \\boldsymbol y)\n",
    "$$\n",
    "\n",
    "5. Go to step 2 with next training sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch training\n",
    "\n",
    "It is compuationally inefficient to update all the parameters every time after passing a training sample. Instead, take a **batch** of size $B$ of training samples at a time and form the matrix\n",
    "$\\boldsymbol X_{\\mathrm{in}}$ if the shape $B\\times n_0$. Now each hidden representation is a matrix of the shape $B \\times n_i$:\n",
    "\n",
    "$$\n",
    "\\boldsymbol X_i = \\psi_i(\\boldsymbol X_{i-1} \\boldsymbol W_i +\\boldsymbol B_i)\n",
    "$$\n",
    "\n",
    "The output also has $B$ rows. For instance, in the case of multiclassification task we have\n",
    "\n",
    "$$\n",
    "    \\boldsymbol X_L = \\boldsymbol {\\widehat Y} \\in \\mathbb R^{B\\times K},\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\mathcal L(\\boldsymbol {\\widehat Y}, \\boldsymbol Y) = -\\frac 1B\\sum\\limits_{i = 1}^B \\sum\\limits_{k=1}^K Y_{ik}\\log(\\widehat Y_{ik})\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
