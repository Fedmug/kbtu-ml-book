{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers of MLP\n",
    "\n",
    "From mathematical point of view MLP is a smooth function $F$ which is constructed as a composition of some other functions\n",
    "\n",
    "$$\n",
    "F(\\boldsymbol x) = (f_{L} \\circ f_{L-1} \\circ\\ldots \\circ f_2 \\circ f_1)(\\boldsymbol x),\\quad\n",
    "\\boldsymbol x \\in \\mathbb R^{n_0}\n",
    "$$\n",
    "\n",
    "Each function \n",
    "\n",
    "$$\n",
    "    f_\\ell \\colon \\mathbb R^{n_{\\ell - 1}} \\to \\mathbb R^{n_\\ell}\n",
    "$$\n",
    "\n",
    "is called a **layer**; it converts representation of $(\\ell-1)$-th layer \n",
    "\n",
    "$$\n",
    "    \\boldsymbol x_{\\ell -1} \\in \\mathbb R^{n_{\\ell - 1}} \n",
    "$$\n",
    "\n",
    "to the representation of $\\ell$-th layer \n",
    "\n",
    "$$\n",
    "   \\boldsymbol x_{\\ell} \\in \\mathbb R^{n_{\\ell}}.\n",
    "$$\n",
    "\n",
    "Thus, the **input layer** $\\boldsymbol x_0 \\in \\mathbb R^{n_0}$ is converted to the **output layer** $\\boldsymbol x_L \\in \\mathbb R^{n_L}$. All other layers $\\boldsymbol x_\\ell$, $1\\leqslant \\ell < L$, are called **hidden layers**.\n",
    "\n",
    "```{figure} https://www.researchgate.net/publication/354817375/figure/fig2/AS:1071622807097344@1632506195651/Multi-layer-perceptron-MLP-NN-basic-Architecture.jpg\n",
    ":align: center\n",
    "```\n",
    "\n",
    "```{warning}\n",
    "The terminology about layers is a bit ambiguous. Both functions $f_\\ell$ and their outputs $\\boldsymbol x_\\ell = f(\\boldsymbol x_{\\ell - 1})$ are called $\\ell$-th layer in different sources.\n",
    "```\n",
    "\n",
    "## Linear regression\n",
    "\n",
    "Linear regression is one the simplest MLPs. There are several types of linear regression model:\n",
    "\n",
    "* simple linear regression {eq}`simple-lin-reg` $y = ax +b$\n",
    "* {ref}`multiple linear regression <linear-regression>` $y = \\boldsymbol x^\\mathsf{T} \\boldsymbol w + w_0$,\n",
    "\n",
    "    $$\n",
    "        \\boldsymbol x = \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_d \\end{pmatrix}, \\quad\n",
    "        \\boldsymbol w = \\begin{pmatrix} w_1 \\\\ \\vdots \\\\ w_d \\end{pmatrix}\n",
    "    $$\n",
    "    \n",
    "* multivariate linear regression $\\boldsymbol y^\\mathsf{T} = \\boldsymbol x^\\mathsf{T} \\boldsymbol W + \\boldsymbol b^\\mathsf{T}$, $\\boldsymbol W\\in\\mathbb R^{d \\times m}$, $\\boldsymbol y, \\boldsymbol b \\in \\mathbb R^m$\n",
    "\n",
    "```{admonition} Question\n",
    ":class: important\n",
    "How many layers does all these kinds of linear regression have? What are the sizes of input and output?\n",
    "```\n",
    "\n",
    "## Logistic regression\n",
    "\n",
    "Logistic regression predicts probabilites of classes:\n",
    "\n",
    "* {ref}`binary logistic regression <simple-log-reg>` $y = \\sigma(\\boldsymbol x^\\mathsf{T} \\boldsymbol w + w_0)$\n",
    "* {ref}`multinomial logistic regression <multi-log-reg>` \n",
    "\n",
    "    $$\n",
    "    \\boldsymbol y^\\mathsf{T} = \\mathrm{Softmax}(\\boldsymbol x^\\mathsf{T} \\boldsymbol W + \\boldsymbol b^\\mathsf{T}), \\quad \\boldsymbol W\\in\\mathbb R^{d \\times K}, \\quad \\boldsymbol y, \\boldsymbol b \\in \\mathbb R^K\n",
    "    $$\n",
    "    \n",
    "```{admonition} Question\n",
    ":class: important\n",
    "How many layers does logistic regression have? What are the sizes of input and output?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters of MLP\n",
    "\n",
    "However, one important element is missing in this description of MLP: parameters! Each layer $f_\\ell$ has a vector of parameters $\\boldsymbol \\theta_\\ell\\in\\mathbb R^{m_\\ell}$ (sometimes empty). Hence, a layer should be defined as\n",
    "\n",
    "$$\n",
    "    f_\\ell \\colon \\mathbb R^{n_{\\ell - 1}} \\times \\mathbb R^{m_\\ell} \\to \\mathbb R^{n_\\ell}.\n",
    "$$\n",
    "\n",
    "The representation $\\boldsymbol x_\\ell$ is calculated from $\\boldsymbol x_{\\ell -1}$ by the formula \n",
    "\n",
    "$$\n",
    "\\boldsymbol x_\\ell = f_\\ell(\\boldsymbol x_{\\ell - 1},\\boldsymbol \\theta_\\ell)\n",
    "$$\n",
    "\n",
    "with some fixed $\\boldsymbol \\theta_\\ell\\in\\mathbb R^{m_\\ell}$. The whole MLP $F$ depends on parameters of all layers:\n",
    "\n",
    "$$\n",
    "    F(\\boldsymbol x, \\boldsymbol \\theta), \\quad \\boldsymbol \\theta = (\\boldsymbol \\theta_1, \\ldots, \\boldsymbol \\theta_L).\n",
    "$$\n",
    "\n",
    "All these parameters are trained simultaneously by the {ref}`backpropagation method <backprop>`.\n",
    "\n",
    "\n",
    "## Dense layer\n",
    "\n",
    "Edges between two consequetive layers denote **linear** (or **dense**) layer:\n",
    "\n",
    "$$\n",
    "    \\boldsymbol x_\\ell^{\\mathsf T} = f(\\boldsymbol x_{\\ell - 1}; \\boldsymbol W, \\boldsymbol b) = \\boldsymbol x_{\\ell - 1}^{\\mathsf T} \\boldsymbol W + \\boldsymbol b.\n",
    "$$\n",
    "\n",
    "The matrix $\\boldsymbol W \\in \\mathbb R^{n_{\\ell - 1}\\times n_\\ell}$ and vector $\\boldsymbol b \\in \\mathbb R^{n_\\ell}$ (bias) are parameters of the linear layer which defines the linear transformation from $\\boldsymbol x_{\\ell - 1}$ to $\\boldsymbol x_{\\ell}$.\n",
    "\n",
    "**Q**. How many numeric parameters does such linear layer have?\n",
    "\n",
    "```{admonition} Exercise\n",
    ":class: important\n",
    "\n",
    "Suppose that we apply one more dense layer:\n",
    "\n",
    "$$\n",
    "    \\boldsymbol x_{\\ell + 1} = \\boldsymbol {W'x}_{\\ell} + \\boldsymbol{b'}\n",
    "$$\n",
    "\n",
    "Express $\\boldsymbol x_{\\ell + 1}$ as a function of $\\boldsymbol x_{\\ell - 1}$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear layer in PyTorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4484,  0.5759, -0.3938],\n",
       "        [-0.5506, -0.1603,  0.3134],\n",
       "        [-0.2858, -0.0493, -0.0959],\n",
       "        [-0.0627,  0.3831,  0.4740]], requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer = torch.nn.Linear(3, 4, bias=False)\n",
    "linear_layer.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the linear transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2664, -0.3975, -0.4310,  0.7944], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this layer a nonlinear **activation function** $\\psi$ is applied element-wise to its input:\n",
    "\n",
    "$$\n",
    "    \\psi(\\boldsymbol x^{\\mathsf T}) = \\psi\\big((x_1, \\ldots, x_n)\\big) = \\big(\\psi(x_1), \\ldots, \\psi(x_n)\\big)  = \\boldsymbol z^{\\mathsf T}\n",
    "$$\n",
    "\n",
    "In the origial work by Rosenblatt the activation function was $\\psi(t) = [t > 0]$. However, this function is discontinuous, that's why in modern neural networks some other smooth alternatives are used.\n",
    "\n",
    "Sometimes linear and activation layers are combined into a single layer. Then each MLP layer looks like\n",
    "\n",
    "$$\n",
    "    \\boldsymbol x_i^{\\mathsf T} = \\psi_i(\\boldsymbol x_{i-1}^{\\mathsf T} \\boldsymbol W_{i} + \\boldsymbol b_{i})\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $\\boldsymbol W_{i}$ is a matrix of the shape $n_{i-1}\\times n_i$\n",
    "* $\\boldsymbol x_i, \\boldsymbol b_i \\in \\mathbb R^{n_i}$ and $\\boldsymbol x_{i-1} \\in \\mathbb R^{n_{i-1}}$\n",
    "* $\\psi_i(t)$ is an activation function which acts element-wise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
