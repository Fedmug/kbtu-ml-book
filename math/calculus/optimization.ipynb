{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "499f9617",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "Almost every task of machine learning is reduced to some optimization problem in multidimensional space. Sometimes the solution can be found analytically. \n",
    "\n",
    "```{admonition} Necessary condition of extremum\n",
    "If $\\mathcal L(\\theta)$ is a smooth function and\n",
    "\n",
    "$$\n",
    "    \\boldsymbol{\\widehat \\theta} = \\arg\\min \\mathcal L(\\theta),\n",
    "$$\n",
    "\n",
    "then $\\nabla \\mathcal L(\\boldsymbol{\\widehat \\theta}) = \\boldsymbol 0$.\n",
    "```\n",
    "\n",
    "A simple example $f(x) = x^3$ shows that this condition is not sufficient.\n",
    "\n",
    "```{admonition} Sufficient condition of extremum\n",
    "If $\\mathcal L(\\theta)$ is twice differentiable, $\\nabla \\mathcal L(\\boldsymbol {\\widehat \\theta}) = \\boldsymbol 0$ and $\\nabla^2\\mathcal L(\\boldsymbol{\\widehat \\theta}$ is positive (negative) definite, then\n",
    "then $\\widehat{\\boldsymbol \\theta}$ is a point of local minimum (maximum).\n",
    "```\n",
    "\n",
    "## Linear regression\n",
    "\n",
    "\n",
    "The loss function of {ref}`linear regression <linear-regression>` model has from\n",
    "\n",
    "```{math} \n",
    ":label: lin-reg-opt-loss\n",
    "    f(\\boldsymbol x) = \\frac 12\\Vert \\boldsymbol{Ax} - \\boldsymbol b\\Vert_2^2, \\quad\n",
    "    \\boldsymbol A \\in \\mathbb R^{m\\times n}, \\quad\\boldsymbol b \\in \\mathbb R^m.\n",
    "```\n",
    "\n",
    "Since\n",
    "\n",
    "$$\n",
    "    f(\\boldsymbol x) = \\frac 12 (\\boldsymbol{Ax} - \\boldsymbol b)^\\mathsf{T}(\\boldsymbol{Ax} - \\boldsymbol b),\n",
    "$$\n",
    "\n",
    "we have\n",
    "\n",
    "$$\n",
    "    df(\\boldsymbol x) = (\\boldsymbol{Ax} - \\boldsymbol b)^\\mathsf{T}\\big(d(\\boldsymbol{Ax} - \\boldsymbol b)\\big) = \\underbrace{(\\boldsymbol{Ax} - \\boldsymbol b)^\\mathsf{T} \\boldsymbol A}_{\\nabla f(\\boldsymbol x)^\\mathsf{T}}d\\boldsymbol x.\n",
    "$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\n",
    "    \\nabla f(\\hat{\\boldsymbol x}) = \\boldsymbol 0 \\iff \\boldsymbol A^\\mathsf{T} (\\boldsymbol{A\\hat{x}} - \\boldsymbol b) = \\boldsymbol 0  \\iff \\boldsymbol A^\\mathsf{T} \\boldsymbol{A\\hat{x}} = \\boldsymbol A^\\mathsf{T} \\boldsymbol b.\n",
    "$$\n",
    "\n",
    "Assuming that $\\boldsymbol A$ has full column rank, we obtain\n",
    "\n",
    "```{math} \n",
    ":label: lin-reg-opt-solution\n",
    "    \\boldsymbol{\\hat{x}} =  (\\boldsymbol A^\\mathsf{T} \\boldsymbol A)^{-1} \\boldsymbol A^\\mathsf{T} \\boldsymbol b = \\boldsymbol A^\\dagger \\boldsymbol b,\n",
    "```\n",
    "\n",
    "where $\\boldsymbol A^\\dagger$ is {ref}`pseudo inverse <pseudo-inverse>` matrix.\n",
    "\n",
    "We've checked only necessary condition of extremum. To verify the fulfillment of the sufficient condition, calculate $d^2f$:\n",
    "\n",
    "$$\n",
    "    d^2f = d(df) = d\\big((\\boldsymbol{Ax} - \\boldsymbol b)^\\mathsf{T} \\boldsymbol A d\\boldsymbol x_1\\big) = \n",
    "    (\\boldsymbol A d\\boldsymbol x_2)^\\mathsf{T} \\boldsymbol A d\\boldsymbol x_1 = d\\boldsymbol x_2^\\mathsf{T} \\boldsymbol A^\\mathsf{T} \\boldsymbol A d\\boldsymbol x_1.\n",
    "$$\n",
    "\n",
    "Therefore, $\\nabla^2 f = \\boldsymbol A^\\mathsf{T} \\boldsymbol A$. Note that it does not depend on $\\boldsymbol x$ and it is a positive definite matrix since\n",
    "\n",
    "```{math}\n",
    ":label: lin-reg-pos-def\n",
    "    \\boldsymbol x^\\mathsf{T}\\boldsymbol A^\\mathsf{T} \\boldsymbol{Ax} = \\Vert \\boldsymbol{Ax} \\Vert_2^2 >0 \\text{ if } \\boldsymbol x \\ne \\boldsymbol 0.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a7acb4",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "Analytic solution like {eq}`lin-reg-opt-solution` is a rare case in machine learning tasks. It also requires inversion of a matrix which is computationally expensive. This is why it is important to elaborate alternative ways of numeric optimization. One of the simplest methods is **gradient descent**.\n",
    "\n",
    "```{prf:algorithm} Gradient descent\n",
    ":label: GD\n",
    ":nonumber:\n",
    "\n",
    "To solve the optimization problem\n",
    "\n",
    "$$\n",
    "    \\mathcal L(\\boldsymbol w) \\to \\min\\limits_{\\boldsymbol w}\n",
    "$$\n",
    "\n",
    "do the followind steps:\n",
    "\n",
    "1. initialize $\\boldsymbol w$ by some random values (e.g., from $\\mathcal N(0, 1$))\n",
    "2. choose **tolerance** $\\varepsilon > 0$ and **learning rate** $\\eta > 0$\n",
    "3. while $\\Vert \\nabla\\mathcal L(\\boldsymbol w) \\Vert > \\varepsilon$ do the **gradient step**\n",
    "\n",
    "    $$\n",
    "    \\boldsymbol w := \\boldsymbol w - \\eta\\nabla\\mathcal L(\\boldsymbol w)\n",
    "    $$\n",
    "4. return $\\boldsymbol w$\n",
    "```\n",
    "\n",
    "```{note}\n",
    "If condition $\\Vert \\nabla\\mathcal L(\\boldsymbol w) \\Vert > \\varepsilon$ holds for too long, the loop in step 3 terminates after some number iterations `max_iter`.\n",
    "```\n",
    "\n",
    "Application of gradient descent algorithm to linear regression is described {ref}`here <num-opt-lin-reg>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d7e2e",
   "metadata": {},
   "source": [
    "## Newton's method\n",
    "\n",
    "If the optimized function is twice differentiable, then an optimization method of the second order can be applied. One of such methods is **Newton's method**. In this method weights $\\boldsymbol w$ are updated by the formula\n",
    "\n",
    "$$\n",
    "    \\boldsymbol w := \\boldsymbol w - \\eta \\big(\\nabla^2 L(\\boldsymbol w)\\big)^{-1}\\nabla\\mathcal L(\\boldsymbol w)\n",
    "$$\n",
    "\n",
    "\n",
    "Computation of hessian $\\nabla^2 L(\\boldsymbol w)$ in Newton's method could be costly. However, this is usually compensated by much faster convergence than that of first order methods like gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266c3b22",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Show that {eq}`lin-reg-opt-solution` is the point of global minimium of the function {eq}`lin-reg-opt-loss`.\n",
    "\n",
    "2. Prove {eq}`lin-reg-pos-def` if $\\boldsymbol A$ has full column rank.\n",
    "\n",
    "3. Find global minimum of the function $f(\\boldsymbol x) = \\boldsymbol a^\\mathsf{T}\\boldsymbol x + \\frac \\sigma 3\\Vert \\boldsymbol x\\Vert_2^3$, $\\boldsymbol a \\ne \\boldsymbol 0$, $\\sigma > 0$.\n",
    "\n",
    "4. Find \n",
    "\n",
    "    $$\n",
    "        \\min\\limits_{\\boldsymbol X} \\Vert \\boldsymbol{AX} - \\boldsymbol B  \\Vert_F^2, \\quad \\boldsymbol A \\in \\mathbb R^{k\\times m},\\quad \\boldsymbol B \\in \\mathbb R^{k\\times n},\\quad \\mathrm{rank}(\\boldsymbol A) = m.\n",
    "    $$\n",
    "    \n",
    "5. Let $\\boldsymbol A \\in \\mathbb R^{n\\times n}$ be a symmetric positive definite matrix and $f(\\boldsymbol X) = \\mathrm{tr} (\\boldsymbol X^{-1} \\boldsymbol A) + \\ln \\vert \\boldsymbol X\\vert$. Find $\\min\\limits_{\\boldsymbol X \\in \\mathbb R^{n\\times n}} f(\\boldsymbol X)$.\n",
    "\n",
    "6. Let $\\boldsymbol A \\in \\mathbb R^{n\\times n}$ be a symmetric matrix and\n",
    "\n",
    "    $$\n",
    "    f(\\boldsymbol x) = \\frac{\\boldsymbol x^\\mathsf{T} \\boldsymbol{Ax}}{\\boldsymbol x^\\mathsf{T} \\boldsymbol x}, \\quad \\boldsymbol x \\in \\mathbb R^n.\n",
    "    $$ \n",
    "\n",
    "    * Calculate $\\nabla f(\\boldsymbol x)$ and prove that $\\nabla f(\\boldsymbol x) = \\boldsymbol 0$ iff $\\boldsymbol x$ is an eigenvector of $\\boldsymbol A$.\n",
    "\n",
    "    * Prove that if $\\boldsymbol A$ is positive definite then\n",
    "\n",
    "        $$\n",
    "            \\max\\limits_{\\boldsymbol x \\ne \\boldsymbol 0} f(\\boldsymbol x) = \\lambda_{\\max}(\\boldsymbol A), \\quad\n",
    "            \\min\\limits_{\\boldsymbol x \\ne \\boldsymbol 0} f(\\boldsymbol x) = \\lambda_{\\min}(\\boldsymbol A)\n",
    "        $$\n",
    "        \n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
