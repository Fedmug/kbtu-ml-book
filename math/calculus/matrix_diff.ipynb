{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix calculus\n",
    "\n",
    "While training a machine learning model, often some function $\\mathcal L(\\boldsymbol X)$ of a matrix $\\boldsymbol X$ is being minimized or maximized. That's why it is important to elaborate a neat notation for differentials and gradients of such functions.\n",
    "\n",
    "(matrix-gradient)=\n",
    "## Differentiating functions of matrices\n",
    "\n",
    "Let $f\\colon \\mathbb R^{m\\times n} \\to \\mathbb R$ be a real-valued function of matrix $\\boldsymbol X$. Such function can be differentiated similar {eq}`vector-func-diff` as a function of vector from $\\mathbb R^{mn}$:\n",
    "\n",
    "```{math}\n",
    ":label: matrix-func-diff\n",
    "\tDf(\\boldsymbol X)[\\boldsymbol H] = \\sum\\limits_{i=1}^m \\sum\\limits_{j=1}^n \\frac{\\partial f (\\boldsymbol X)}{\\partial X_{ij}} H_{ij} = \\mathrm{tr}\\big(\\nabla f(\\boldsymbol X)^\\mathsf{T} \\boldsymbol H\\big).\n",
    "```\n",
    "    \n",
    "Здесь мы упаковали градиент функции $f$ в матрице того же размера, что и матрица $\\boldsymbol X$, а также воспользовались равенством\n",
    "\n",
    "$$\n",
    "    \\sum\\limits_{i=1}^m \\sum\\limits_{j=1}^n A_{ij} B_{ij} = \\mathrm{tr} (\\boldsymbol A^\\mathsf{T} \\boldsymbol B), \\quad A, B \\in \\mathbb R^{m\\times n}.\n",
    "$$\n",
    "\n",
    "Using more traditional notation, we can rewrite {eq}`matrix-func-diff` as\n",
    "\n",
    "$$\n",
    "    df(\\boldsymbol X) = \\mathrm{tr}\\big(\\nabla f(\\boldsymbol X)^\\mathsf{T} d\\boldsymbol X\\big).\n",
    "$$\n",
    "\n",
    "Note that once again gradient $\\nabla f(\\boldsymbol X)$ has the same shape as input variable $\\boldsymbol X$. Here it is an $m\\times n$ matrix:\n",
    "\n",
    "$$\n",
    "    \\big(\\nabla f(\\boldsymbol X)\\big)_{ij} = \\frac{\\partial f (\\boldsymbol X)}{\\partial X_{ij}}, \\quad 1\\leqslant i \\leqslant m, \\quad 1 \\leqslant j \\leqslant n.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules of matrix calculus\n",
    "\n",
    "|  |\n",
    "|---|\n",
    "| $d\\boldsymbol A = \\boldsymbol 0$  |\n",
    "| $d(\\boldsymbol{AXB}) = \\boldsymbol A(d\\boldsymbol X)\\boldsymbol B$  |\n",
    "| $d(\\mathrm{tr}(\\boldsymbol {AX})) = \\mathrm{tr}(\\boldsymbol A\\cdot d\\boldsymbol X)$ |\n",
    "| $d(\\alpha \\boldsymbol X + \\beta \\boldsymbol Y) = \\alpha d\\boldsymbol X + \\beta d\\boldsymbol Y.$  |\n",
    "| $d(\\boldsymbol X^\\mathsf{T}) = (d\\boldsymbol X)^{\\mathsf T}$  |\n",
    "| $d(\\boldsymbol{XY}) = d\\boldsymbol X \\cdot \\boldsymbol Y + \\boldsymbol X \\cdot d\\boldsymbol Y.$  |\n",
    "| $d\\langle\\boldsymbol X, \\boldsymbol Y\\rangle = \\langle d\\boldsymbol X , \\boldsymbol Y\\rangle + \\langle\\boldsymbol X , d\\boldsymbol Y\\rangle.$ |\n",
    "\n",
    "Here $\\boldsymbol A$ and $\\boldsymbol B$ are constant matrices, $\\boldsymbol X$ and $\\boldsymbol Y$ are variable matrices, $\\alpha, \\beta \\in \\mathbb R$.\t\n",
    "\n",
    "\n",
    "## Examples of matrix calculus\n",
    "\n",
    "### Differential of inverse matrix\n",
    "\n",
    "Вычислим дифференциал обратной матрицы: $f(\\boldsymbol X) = \\boldsymbol X^{-1}$, где $\\boldsymbol X$ — невырожденная квадратная матрица.\n",
    "\n",
    "\n",
    "Продифференцируем равенство $\\boldsymbol I = \\boldsymbol X\\cdot \\boldsymbol X^{-1}$:\n",
    "\n",
    "$$\n",
    "\t\\boldsymbol 0 = d\\boldsymbol I = d\\boldsymbol X \\cdot \\boldsymbol X^{-1} + \\boldsymbol X \\cdot d\\boldsymbol X^{-1}.\n",
    "$$\n",
    "\n",
    "Отсюда уже легко выражается искомый дифференциал:\n",
    "\n",
    "$$ \n",
    "\td\\boldsymbol X^{-1} = - \\boldsymbol X^{-1}\\cdot d\\boldsymbol X \\cdot \\boldsymbol X^{-1}.\n",
    "$$\n",
    "\n",
    "```{warning}\n",
    "Внимательный читатель заметит, что данное решение предполагает, что дифференцируемость функции $f$ уже известна. Полное решение можно прочитать [здесь](http://www.machinelearning.ru/wiki/images/a/ab/MOMO18_Seminar1.pdf) (пример А.14).\n",
    "```\n",
    "\n",
    "### Gradient of determinant\n",
    "\n",
    "Вычислим градиент определителя: $f(\\boldsymbol X) = \\det(\\boldsymbol X)$, где $\\boldsymbol X$ — квадратная матрица.\n",
    "\n",
    "В предыдущих примерах мы изо всех сил старались не писать матричных элементов, но сейчас, увы, придётся. Попробуем вычислить $\\frac{\\partial f}{\\partial{X_{ij}}}$. Для этого разложим определитель по $i$-й строке:\n",
    "\n",
    "$$\n",
    "\\det(\\boldsymbol X) = \\sum_{j}X_{ij}\\cdot(-1)^{i + j}M_{ij},\n",
    "$$\n",
    "\n",
    "где $M_{ij}$ — это определитель подматрицы, полученной из исходной выбрасыванием $i$-й строки и $j$-го столбца. Теперь мы видим, что определитель линеен по переменной $X_{ij}$, причём коэффициент при ней равен $(-1)^{i + j}M_{ij}$. Таким образом,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial{X_{ij}}} = (-1)^{i + j}M_{ij}.\n",
    "$$\n",
    "\n",
    "Чтобы записать матрицу, составленную из таких определителей, покороче, вспомним, что\n",
    "\n",
    "$$\n",
    "\\boldsymbol X^{-1} = \\frac1{\\det(\\boldsymbol X)}\\left((-1)^{i+j}M_{\\color{red}{ji}}\\right)_{i,j}.\n",
    "$$\n",
    "\n",
    "Обратите внимание на переставленные индексы $i$ и $j$ (отмечены красным). Но всё равно похоже! Таким образом, для невырожденной матрицы $\\boldsymbol X$ мы можем представить градиент в виде\n",
    "\n",
    "$$\n",
    "\\nabla f(\\boldsymbol X) = \\det(\\boldsymbol X)\\cdot \\boldsymbol X^{-\\mathsf{T}},\n",
    "$$\n",
    "\n",
    "где $\\boldsymbol X^{-\\mathsf{T}}$ — более короткая запись для $(\\boldsymbol X^{-1})^\\mathsf{T}$.  \t\n",
    "\n",
    "\n",
    "(linear-diff)=\n",
    "### Linear matrix function\n",
    "\n",
    "Let $f(\\boldsymbol X) = \\boldsymbol{XW}$, где $\\boldsymbol X$ и $\\boldsymbol W$ — матрицы подходящего размера. Тогда\n",
    "\t\n",
    "$$\n",
    "f(\\boldsymbol X + \\boldsymbol H) - f(\\boldsymbol X) = (\\boldsymbol X + \\boldsymbol H) \\boldsymbol W - \\boldsymbol X \\boldsymbol W = \\boldsymbol H \\boldsymbol W.\n",
    "$$\n",
    "\n",
    "Получилась линейная по $\\boldsymbol H$ функция, поэтому она и является дифференциалом функции $f$ в точке $\\boldsymbol X$: $Df(\\boldsymbol X) [\\boldsymbol H] = \\boldsymbol{HW}$. Допустима также запись $df(\\boldsymbol X) = d\\boldsymbol X\\cdot \\boldsymbol W$.  \n",
    "\n",
    "More rules and examples of matrix calculus can be found in [this cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) (from p.8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Calculate $\\nabla f$ if $f(\\boldsymbol X) = \\boldsymbol a^\\mathsf{T} \\boldsymbol X \\boldsymbol b$. \n",
    "\n",
    "2. Calculate $\\nabla f$ if $f(\\boldsymbol X) = \\log(\\det(\\boldsymbol X))$.\n",
    "\n",
    "3. Calculate $\\nabla f$ if $f(\\boldsymbol X) = \\mathrm{tr}(\\boldsymbol{AX}^{\\mathsf T}\\boldsymbol X)$. \n",
    "\n",
    "4. Let $f(\\boldsymbol X) = \\det\\left(\\boldsymbol{AX}^{-1}\\boldsymbol B\\right)$.\n",
    "Calculate $\\nabla f$ if\n",
    "    * all matrices are square;\n",
    "    * matrices $\\boldsymbol A$ and $\\boldsymbol B$ are rectangular.\n",
    "    \n",
    "5. Find differential of $f(\\boldsymbol W) = \\boldsymbol{XW}$.\n",
    "\n",
    "    \n",
    "6. Show that inner product in the space of matrices of shape  $\\mathbb R^{m\\times n}$ can be defined as $\\langle \\boldsymbol A, \\boldsymbol B \\rangle = \\mathrm{tr} (\\boldsymbol A^\\mathsf{T} \\boldsymbol B)$.\n",
    "    ```{admonition} Hint\n",
    "    :class: tip, dropdown\n",
    "    You need to verify three properties:\n",
    "    \n",
    "    - $\\langle \\boldsymbol A, \\boldsymbol A \\rangle \\geq 0$,  $\\langle \\boldsymbol A, \t\t\\boldsymbol A \\rangle = 0 \\iff \\boldsymbol A = \\boldsymbol 0$;\n",
    "    - $\\langle \\boldsymbol A, \\boldsymbol B \\rangle = \\langle \\boldsymbol B, \\boldsymbol A \\rangle$;\n",
    "    - $\\langle \\alpha \\boldsymbol A + \\beta \\boldsymbol B, \\boldsymbol C \\rangle = \\alpha\\langle \\boldsymbol A, \\boldsymbol C \\rangle + \\beta\\langle \\boldsymbol B, \\boldsymbol C \\rangle$.\n",
    "    ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
