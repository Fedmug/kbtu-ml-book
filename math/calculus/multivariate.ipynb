{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate calculus\n",
    "\n",
    "Перейдём теперь к более общей ситуации. Пусть заданы два конечномерных пространства $U$ и $V$ (например, векторов или матриц). Функция $f \\colon X \\to V$, $X \\subset U$, дифференцируема в точке $x\\in X$, если найдётся такой линейный оператор $L\\colon U \\to V$, что справедливо равенство\n",
    "\n",
    "$$\n",
    "\tf(x+h) - f(x) = L[h] + o(\\Vert h\\Vert) \\text{ при } \\Vert h\\Vert \\to 0.\n",
    "$$\n",
    "\n",
    "Линейное отображение $L$ из мира $x$-ов в мир значений $f$ называется **дифференциалом** (**производной**, **производным отображением**) функции $f$ в точке $x$ и обозначается $df(x)$ или $Df(x)[h]$. Фактически дифференциал зависит от двух аргументов: $df \\colon X\\times U \\to V$, причём по второму аргументу он линеен. Среди всех линейных операторов из $U$ в $V$ дифференциал наилучшим образом приближает приращение функции: \n",
    "\n",
    "$$\n",
    "f(x+h) - f(x) \\approx Df(x)[h].\n",
    "$$ \n",
    "\n",
    "\n",
    "```{note}\n",
    "Нотация $Df(x)[h]$ для дифференциала выглядит несколько громоздко, зато в неё явно включены обе переменные, от которых зависит дифференциал. При вычислении дифференциала функции $f$ в произвольной точке $x$ часто используют более короткую запись $df(x)$ или даже $df$; переменная приращения в таких случаях традиционно обозначается через $dx$. Обозначение $dx$ следует воспринимать как единый неделимый символ, по смыслу полностью тождественный приращению $h$. \n",
    "``` \n",
    "\n",
    "## Differentials of scalar functions\n",
    "\n",
    "In the previous section we considered the simplest case when $f \\colon \\mathbb R \\to \\mathbb R$ — скалярная функция скалярного аргумента. Тогда её дифференциал записывается как $df(x) = f'(x) dx$ или $Df(x)[h] = f'(x)h$. Линейный оператор из $\\mathbb R$ в $\\mathbb R$ задаётся одним действительным числом, и в данном случае оно равно $f'(x)$. Now see what happens when the function $f$ has multidimensional domain.\n",
    "\t\n",
    "\n",
    "### Functions of vectors\n",
    "\n",
    "Пусть $f\\colon \\mathbb R^n \\to \\mathbb R$ — скалярная функция векторного аргумента $\\boldsymbol x = (x_1, \\ldots, x_n)^\\top$ (под вектором по умолчанию понимается вектор-столбец). Тогда \n",
    "\n",
    "```{math}\n",
    ":label: vector-func-diff\n",
    "\tDf(\\boldsymbol x)[\\boldsymbol h] = \\sum\\limits_{i=1}^n\n",
    "\t\\frac{\\partial f(\\boldsymbol x)}        {\\partial \t x_i} h_i = \n",
    "\t\\langle \\nabla f(\\boldsymbol x), \\boldsymbol h\\rangle = \n",
    "\t\\nabla     f       (\\boldsymbol x)^\\top \t\t\\boldsymbol h,\n",
    "```\n",
    "\n",
    "где \n",
    "    \n",
    "$$\n",
    "        \\nabla f = \\Big(\\frac{\\partial f}{\\partial x_1}, \\ldots,\\frac{\\partial f}\t{\\partial x_n}\\Big)^\\top\n",
    "$$\n",
    "\t\n",
    " — **градиент** функции $f$,\n",
    "а $\\langle \\bullet, \\bullet \\rangle$ — стандартное скалярное произведение в $\\mathbb R^n$. Также дифференциал записывают как $df(\\boldsymbol x) = \\nabla f(\\boldsymbol x)^\\top d\\boldsymbol \tx$. Обратите внимание, что приращение $d\\boldsymbol x$ и градиент $\\nabla f$ представляют собой вектор-столбец того же размера, что и переменная $\\boldsymbol x$. Компоненты градиента — это частные производные функции $f$, которые сами являются функциями от переменной $\\boldsymbol x$, поэтому градиент также зависит от $\\boldsymbol x$.\n",
    "\n",
    "**Вопрос на подумать**. Как записать в векторной форме дифференциал функции $f\\colon \\mathbb R^n \\to \\mathbb R$, принимающей на вход вектор-строку, а не вектор-столбец?\n",
    "\n",
    "<details>\n",
    "<summary markdown=\"span\">Попробуйте прийти к ответу самостоятельно, прежде чем смотреть решение. </summary>\n",
    "<div>\n",
    "В координатах разницы в записи дифференциала нет, по-прежнему \n",
    "    \n",
    "$$\n",
    "    df = \\sum\\limits_{i=1}^n \\frac{\\partial f}{\\partial \t x_i} dx_i.\n",
    "$$\n",
    "    \n",
    "Поскольку $\\boldsymbol x$ теперь вектор-строка, такую же форму имеют и градиент, и приращение. Подружить их в скалярном произведении, в котором всегда вектор-строка умножается на вектор-столбец, можно так: \n",
    "    \n",
    "$$\n",
    "    df(\\boldsymbol x) = \\nabla f(\\boldsymbol x) d\\boldsymbol \tx^\\top,\n",
    "$$\n",
    "    \n",
    "а в другой нотации — так: \n",
    "\t\n",
    "$$\n",
    "\tDf(\\boldsymbol x)[\\boldsymbol h] = \\nabla    f(\\boldsymbol x)\t\t\\boldsymbol h ^\\top.\n",
    "$$\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "```{admonition} Exercise\n",
    ":class: important\n",
    "Let $\\boldsymbol a \\in \\mathbb R^n$ be a fixed vector and\n",
    "\n",
    "$$\n",
    "\tf(\\boldsymbol x) = \\langle \\boldsymbol a, \\boldsymbol x\\rangle = \\boldsymbol a^\\top \\boldsymbol x.\n",
    "$$\n",
    "\n",
    "Find $\\nabla f(\\boldsymbol x)$.\n",
    "```\n",
    "\n",
    "### Functions of matrices\n",
    "\n",
    "Пусть $f\\colon \\mathbb R^{m\\times n} \\to \\mathbb R$ — скалярная функция матричного аргумента $\\boldsymbol X$. Такую функцию можно представлять себе как функцию от вектора из $\\mathbb R^{mn}$ в $\\mathbb R$ и дифференцировать аналогично {eq}`vector-func-diff`:\n",
    "\n",
    "```{math}\n",
    ":label: matrix-func-diff\n",
    "\tDf(\\boldsymbol X)[\\boldsymbol H] = \\sum\\limits_{i=1}^m \\sum\\limits_{j=1}^n \\frac{\\partial f (\\boldsymbol X)}{\\partial X_{ij}} H_{ij} = \\mathrm{tr}(\\nabla f(\\boldsymbol X)^\\top \\boldsymbol H).\n",
    "```\n",
    "    \n",
    "Здесь мы упаковали градиент функции $f$ в матрице того же размера, что и матрица $\\boldsymbol X$, а также воспользовались равенством\n",
    "\n",
    "$$\n",
    "    \\sum\\limits_{i=1}^m \\sum\\limits_{j=1}^n A_{ij} B_{ij} = \\mathrm{tr} (\\boldsymbol A^\\top \\boldsymbol B), \\quad A, B \\in \\mathbb R^{m\\times n}.\n",
    "$$\n",
    "\n",
    "**Упражнение.** Докажите, что формула $\\langle \\boldsymbol A, \\boldsymbol B \\rangle = \\mathrm{tr} (\\boldsymbol A^\\top \\boldsymbol B)$ задаёт скалярное произведение в пространстве матриц $\\mathbb R^{m\\times n}$.\n",
    "\n",
    "```{admonition} Hint\n",
    ":class: tip, dropdown\n",
    "Требуется проверить следующие свойства:\n",
    "- $\\langle \\boldsymbol A, \\boldsymbol A \\rangle \\geq 0$, причём $\\langle \\boldsymbol A, \t\t\\boldsymbol A \\rangle = 0 \\iff \\boldsymbol A = \\boldsymbol 0$;\n",
    "- $\\langle \\boldsymbol A, \\boldsymbol B \\rangle = \\langle \\boldsymbol B, \\boldsymbol A \\rangle$;\n",
    "- $\\langle \\alpha \\boldsymbol A + \\beta \\boldsymbol B, \\boldsymbol C \\rangle = \\alpha\\langle \\boldsymbol A, \\boldsymbol C \\rangle + \\beta\\langle \\boldsymbol B, \\boldsymbol C \\rangle$.\n",
    "```\n",
    "\n",
    "### Gradients shape\n",
    "\n",
    "В {eq}`vector-func-diff` и {eq}`matrix-func-diff` пришлось столкнуться с давним знакомым из матанализа: градиентом $\\nabla f$ скалярной функции $f$, который состоит из частных производных этой функции по всем координатам её аргумента $x$. При этом его обычно упаковывают в ту же форму, что и сам аргумент: если $x$ — вектор-строка, то и градиент записывается вектор-строкой, а если $x$ — матрица, то и градиент тоже будет матрицей того же размера. Это важно, например, для алгоритма градиентного спуска, при осуществлении которого мы должны уметь прибавлять градиент к точке, в которой он посчитан.\n",
    "\n",
    "```{note}\n",
    "Иногда приходится дифференцировать функции, зависящие от различных наборов аргументов, и тогда во избежание неоднозачности в индексе у буквы $\\nabla$ указывают аргумент, по которому осуществляется дифференцирование. Например, запись $\\nabla_{\\boldsymbol w} f(\\boldsymbol X, \\boldsymbol y, \\boldsymbol w)$ недвусмысленно даёт понять, что мы дифференцируем фукнцию $f$ по вектору $\\boldsymbol w$, а $\\boldsymbol X$ и $\\boldsymbol y$ рассматриваем при этом как фиксированные параметры.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentials of vector functions\n",
    "\n",
    "Посмотрим теперь, как выглядит дифференцирование для функций, которые на выходе выдают не скаляр, а вектор или матрицу.\n",
    "\n",
    "### Applying a function elementwise \n",
    "\n",
    "Пусть \n",
    "\n",
    "```{math}\n",
    ":label: elementwise-func\n",
    "f \\colon \\mathbb R^m \\to \\mathbb R^m,\\quad  \n",
    "f(\\boldsymbol x) = \\begin{pmatrix} \n",
    "\\varphi(x_1)\\\\ \\vdots\\\\ \\varphi(x_m) \n",
    "\\end{pmatrix},\n",
    "```\n",
    "\n",
    "где $\\varphi \\colon \\mathbb R \\to \\mathbb R$ — некоторая гладкая функция. Тогда\n",
    "\n",
    "$$\n",
    "\tf(\\boldsymbol x + \\boldsymbol h) - f(\\boldsymbol x) = \n",
    "\t    \\begin{pmatrix}\n",
    "\t            \\varphi(x_1 + h_1) - \\varphi(x_1)\\\\\n",
    "\t            \\vdots \\\\\n",
    "\t            \\varphi(x_m + h_m) - \\varphi(x_m) \n",
    "\t    \\end{pmatrix}\n",
    "\t    \\approx \n",
    "\t    \\begin{pmatrix}\n",
    "\t            \\varphi'(x_{1}) h_1\\\\\n",
    "\t            \\vdots  \\\\\n",
    "\t            \\varphi'(x_{m}) h_m\n",
    "\t    \\end{pmatrix} \n",
    "\t    =\n",
    "\t        \\begin{pmatrix}\n",
    "\t            \\varphi'(x_{1}) \\\\\n",
    "\t            \\vdots \\\\\n",
    "\t            \\varphi'(x_{m})\n",
    "\t        \\end{pmatrix}\n",
    "\t        \\odot\n",
    "\t           \\boldsymbol h.\n",
    "$$\n",
    "\n",
    "В последнем выражении происходит покомпонентное умножение. Если обозначить через $\\varphi'(\\boldsymbol x)$ покомпонетное применение функции $\\varphi'$ к координатам вектора $\\boldsymbol x = (x_1, \\ldots, x_m)^\\top$, то дифференциал функции $f$ можно записать как\n",
    "\n",
    "$$\n",
    "\tDf(\\boldsymbol x)[\\boldsymbol h] = \\varphi'(\\boldsymbol x) \\odot \\boldsymbol h =\\boldsymbol h \\odot \\varphi'(\\boldsymbol x).\n",
    "$$\n",
    "\n",
    "### Linear matrix function\n",
    "\n",
    "Let $f(\\boldsymbol X) = \\boldsymbol{XW}$, где $\\boldsymbol X$ и $\\boldsymbol W$ — матрицы подходящего размера. Тогда\n",
    "\t\n",
    "$$\n",
    "f(\\boldsymbol X + \\boldsymbol H) - f(\\boldsymbol X) = (\\boldsymbol X + \\boldsymbol H) \\boldsymbol W - \\boldsymbol X \\boldsymbol W = \\boldsymbol H \\boldsymbol W.\n",
    "$$\n",
    "\n",
    "Получилась линейная по $\\boldsymbol H$ функция, поэтому она и является дифференциалом функции $f$ в точке $\\boldsymbol X$: $Df(\\boldsymbol X) [\\boldsymbol H] = \\boldsymbol{HW}$. Допустима также запись $df(\\boldsymbol X) = d\\boldsymbol X\\cdot \\boldsymbol W$.\n",
    "\n",
    "```{admonition} Exercise\n",
    ":class: important\n",
    "Найдите дифференциал функции $f(\\boldsymbol W) = \\boldsymbol{XW}$, где $\\boldsymbol X$ и $\\boldsymbol W$ — матрицы подходящего размера.\n",
    "``` \n",
    "\n",
    "### Vector-to-vector functions\n",
    "\n",
    "Пусть $f\\colon \\mathbb R^m \\to \\mathbb R^n$, тогда дифференциал должен быть линейным оператором из $\\mathbb R^m$ в $\\mathbb R^n$, то есть матрицей размера $n\\times m$. И, действительно, с помощью **матрицы Якоби** \n",
    "\n",
    "```{math}\n",
    ":label: jacobi\n",
    "\\boldsymbol J_f = \\frac{\\partial f}{\\partial \\boldsymbol x} = \n",
    "\t\t    \\begin{pmatrix} \n",
    "\t    \t        \\frac{\\partial f_1}{\\partial x_1} & \\ldots & \n",
    "\t        \t    \\frac{\\partial f_1}{\\partial x_m} \\\\\n",
    "\t        \t\\vdots & & \\vdots \\\\\n",
    "\t            \t\\frac{\\partial f_n}{\\partial x_1} & \\ldots & \n",
    "\t            \t\\frac{\\partial f_n}{\\partial x_m}\\\\\n",
    "\t    \t\\end{pmatrix}\n",
    "```\n",
    "\t\n",
    "\n",
    "\n",
    "дифференциал записывается как $Df(\\boldsymbol x)[\\boldsymbol h] = \\boldsymbol J_f(\\boldsymbol x)\\boldsymbol h$, или $df(\\boldsymbol x) = \\boldsymbol J_f(\\boldsymbol x) d\\boldsymbol x$.\n",
    "\n",
    "```{admonition} Exercise\n",
    ":class: important\n",
    "According to {eq}`jacobi`, the differential of the function {eq}`elementwise-func` should be a $m\\times m$ matrix. How does it look like?\n",
    "```\n",
    "\n",
    "**Вопрос на подумать**. Как записать в векторно-матричной форме дифференциал функции $f\\colon \\mathbb R^m \\to \\mathbb R^n$, отображающей вектор-строку $\\boldsymbol x = (x_1, \\ldots, x_m)$ в вектор-строку $(f_1(\\boldsymbol x), \\ldots, f_n(\\boldsymbol x))$?\n",
    "\n",
    "<details>\n",
    "    <summary markdown=\"span\">Попробуйте прийти к ответу самостоятельно, прежде чем смотреть решение. </summary>\n",
    "    <div>\n",
    "\tЗначение дифференциала $$Df(\\boldsymbol x)[\\boldsymbol h]$$ теперь должно быть вектором-строкой, а не вектором-столбцом. Этого легко добиться транспонированием: применяя её к формуле из предыдущего примера, получаем $$Df(\\boldsymbol x)[\\boldsymbol h] = \\boldsymbol h^T \\boldsymbol J_f(\\boldsymbol x)^T$$. Следует учесть, однако, что теперь приращение $$\\boldsymbol h$$ — это строка, поэтому его транспонировать не нужно. В итоге получаем $$Df(\\boldsymbol x)[\\boldsymbol h] = \\boldsymbol h \\boldsymbol J_f(\\boldsymbol x)^T$$ (альтернативная запись: $$df = d\\boldsymbol x \\cdot \\boldsymbol J_f(\\boldsymbol x)^T$$).\n",
    "\t</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules of differentiation\n",
    "\n",
    "1. **Производная константы**. Если $f(x) = \\mathrm{const}$, то $f(x+h) - f(x) = 0$, и по определению $Df(x)[h] = 0$. По этой же причине градиент постоянной функции векторного аргумента также равен нулю.\n",
    "\n",
    "2. **Дифференциал линейного отображения**. Пусть $f(x)$ — линейное отображение. Тогда\n",
    "\n",
    "$$\n",
    "f(x + h) - f(x) = f(x) + f(h) - f(x) = f(h).\n",
    "$$\n",
    "\n",
    "Поскольку справа линейное отображение, то по определению оно и является дифференциалом $Df(x)[h]$. Мы уже видели примеры таких ситуаций выше, когда рассматривали отображения умножения на матрицу слева или справа.\n",
    "\n",
    "3. **Линейность дифференциала**. Пусть функции $f \\colon X \\to V$ и $g \\colon X \\to V$ дифференцируемы в точке $x\\in X$ и $\\alpha, \\beta \\in \\mathbb R$. Тогда функция $\\alpha f + \\beta g$ дифференцируема в точке $x$, и\n",
    "\n",
    "\t$$\n",
    "\tD(\\alpha f + \\beta g)(x)[h] = \\alpha Df(x)[h] + \\beta Dg(x)[h].\n",
    "\t$$\n",
    "\n",
    "4. **Дифференциал произведения**. Пусть функции $u \\colon X \\to \\mathbb R$, $v \\colon X \\to V$ дифференцируемы в точке $x$. Тогда функция $u\\cdot v$ дифференцируема в точке $x$, и\n",
    "\n",
    "\t$$\n",
    "\t\tD(uv)(x)[h] = Du(x)[h]\\cdot v(x) + u(x)\\cdot Dv(x)[h].\n",
    "\t$$\n",
    " \n",
    "\tЭто же правило сработает и для скалярного произведения:\n",
    "\n",
    "\t$$\n",
    "\t\tD \\langle u(x), v(x)\\rangle [h] = \\langle Du(x)[h], v(x)\\rangle + \\langle u(x), \tDv(x)[h]\\rangle.\n",
    "\t$$\n",
    "\n",
    "5. **Производная сложной функции.** Пусть функция $u \\colon X \\to Y$ дифференцируема в точке $x$, а функция $v \\colon Y \\to Z$ дифференцируема в точке $u(x)$. Тогда их композиция $v\\circ u$ дифференцируема в точке $x$, и\n",
    "\n",
    "\t$$\n",
    "\tD(u\\circ v)(x)[h] = Dv(u(x))\\Big[Du(x)[h]\\Big].\n",
    "\t$$\n",
    "\n",
    "\t```{admonition} Proof\n",
    "\t:class: dropdown\n",
    "\tОбозначим $f(x) = (v\\circ u)(x)$, тогда\n",
    "\n",
    "\t$$\n",
    "\t\tf(x+h) - f(x) = v(u(x+h)) - v(u(x)) \\approx v\\big(u(x) + Du(x)[h]\\big) - v(u(x)).\n",
    "\t$$\n",
    "\n",
    "\tПолучилось приращение функции $v$, взятое в точке $u(x)$ с приращением $Du(x)[h]$. Приближённо оно равно дифференциалу $Dv(u(x))\\Big[Du(x)[h]\\Big]$.\n",
    "\t```\n",
    "\n",
    "\tВ частности, если $Y \\subset \\mathbb R$ и $Z = \\mathbb R$, то $d(v\\circ u)(x) = v'(u(x))du(x)$.\n",
    "\n",
    "6. Важный частный случай: **дифференцирование перестановочно с линейным отображением**. Пусть $f(x) = L(v(x))$, где $L$ — линейное отображение. Тогда $DL(v(x))$ совпадает с самим $L$ и формула упрощается:\n",
    "\n",
    "\t$$\n",
    "\tD(L\\circ v)(x)[h] = L(Dv(x)[h]).\n",
    "\t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{admonition} Exercise\n",
    ":class: important\n",
    "Let $\\boldsymbol A \\in \\mathbb R^{n\\times n}$ and\n",
    "\n",
    "$$\n",
    " f(\\boldsymbol x) = \\langle \\boldsymbol{Ax}, \\boldsymbol x\\rangle = \\boldsymbol x^\\top \\boldsymbol{Ax}.\n",
    "$$\n",
    "\n",
    "Find $\\nabla f(\\boldsymbol x)$.\n",
    "\n",
    "```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "Воспользуемся формулой дифференциала произведения:\n",
    "\n",
    "$$\n",
    "\tdf = \\langle d(\\boldsymbol{Ax}), \\boldsymbol x\\rangle + \\langle \\boldsymbol{Ax}, d\\boldsymbol x\\rangle = \\langle \\boldsymbol A d\\boldsymbol x, \\boldsymbol x\\rangle + \\langle \\boldsymbol{Ax}, d\\boldsymbol x\\rangle. \n",
    "$$\n",
    "\n",
    "Чтобы найти градиент, нам надо это выражение представить в виде $\\langle ?, d\\boldsymbol x\\rangle$. Для этого поменяем местами множители первого произведения и перенесём $\\boldsymbol A$ в другую сторону ($\\boldsymbol A$ перенесётся с транспонированием):\n",
    "\n",
    "$$\n",
    "\tdf = \\langle d\\boldsymbol x, \\boldsymbol A^T\\boldsymbol x\\rangle + \\langle \\boldsymbol{Ax}, d\\boldsymbol x\\rangle = \\langle (\\boldsymbol A^T + \\boldsymbol A)\\boldsymbol x, d\\boldsymbol x\\rangle.\n",
    "$$\n",
    "\n",
    "Отсюда получаем, что $\\nabla f(\\boldsymbol x) = (\\boldsymbol A^T + \\boldsymbol A)\\boldsymbol x$.\n",
    "```\t\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second differential\n",
    "\n",
    "Дифференциал $Df(x)[h]$  функции\n",
    "\n",
    "$$\n",
    "f \\colon X \\to V, \\quad X\\subset U,\n",
    "$$\n",
    "\n",
    "где $U$ и $V$ — некоторые конечномерные пространства, зависит от двух аргументов: $x\\in X$ и $h\\in U$. Зафиксировав приращение $h_1\\in U$, получим функцию от переменной $x$: \n",
    "\n",
    "$$\n",
    "g(x) = Df(x)[h_1].\n",
    "$$\n",
    "\n",
    "А значит, её тоже можно попробовать продифференцировать! Если это возможно, то её дифференциал $Dg(x)[h_2]$ называется **вторым дифференциалом** функции $f$, и обозначается \n",
    "\n",
    "$$\n",
    "D^2f(x)[h_1, h_2] \\text{ или } d^2f(x).\n",
    "$$\n",
    "\n",
    "На сей раз это не линейный оператор, а билинейная форма: при фиксированном $x_0\\in X$ функция $D^2f(x_0)[h_1, h_2]$ линейна по каждому из аргументов приращения.\n",
    "\n",
    "Для функций $f\\colon \\mathbb R^n \\to \\mathbb R$ второй дифференциал, как и любую билинейную форму, можно представить в виде матрицы:\n",
    "\n",
    "$$\n",
    "\tD^2f(\\boldsymbol x)[\\boldsymbol h_1, \\boldsymbol h_2] = \\langle \\boldsymbol H_x \\boldsymbol h_1, \\boldsymbol h_2\\rangle = \\boldsymbol h_2^\\top \\boldsymbol H_x \\boldsymbol h_1, \\quad \\boldsymbol H_x \\in \\mathbb R^{n\\times n}.\n",
    "$$\n",
    "\n",
    "Матрица $\\boldsymbol H_x$ назвается **гессианом** функции $f$ в точке $\\boldsymbol x$ и обозначается $\\nabla^2 f( \\boldsymbol x)$. Гессиан состоит из вторых частных производных:\n",
    "\n",
    "$$\n",
    "\t\\nabla^2 f(\\boldsymbol x) = \\left \\| \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}(\\boldsymbol x)\\right\\|_{i, j=1}^n.\n",
    "$$\n",
    "\n",
    "Если все вторые частные производные функции $f$ непрерывны, то её гессиан представляет собой симметричную матрицу (в этом состоит утверждение [теоремы Шварца](https://en.wikipedia.org/wiki/Symmetry_of_second_derivatives#Theorem_of_Schwarz)). Также для таких функций справедливо разложение по формуле Тейлора до второго порядка:\n",
    "\n",
    "$$\n",
    "\tf(\\boldsymbol x+ \\boldsymbol h) = f(\\boldsymbol x) + Df(\\boldsymbol x)[\\boldsymbol h] + \\frac 12 D^2f(\\boldsymbol x)[\\boldsymbol h, \\boldsymbol h] + o(\\|\\boldsymbol h\\|^2),\n",
    "$$\n",
    "\n",
    "или, в другой записи,\n",
    "\n",
    "$$\n",
    "\tf(\\boldsymbol x+\\boldsymbol h) = f(\\boldsymbol x) + \\nabla f(\\boldsymbol x)^\\top h + \\frac 12 \\boldsymbol h^\\top\\nabla ^2f(\\boldsymbol x)\\boldsymbol h + o(\\|\\boldsymbol h\\|^2).\n",
    "$$\n",
    "\n",
    "Из формулы Тейлора вытекает следующее полезное утверждение для поиска точек локального экстремума.\n",
    "\n",
    "**Теорема.** Пусть функция $f:\\mathbb{R}^m\\to\\mathbb{R}$ имеет непрерывные частные производные второго порядка $\\frac{\\partial^2 f}{\\partial x_i\\partial x_j}$ в окрестности точки $\\boldsymbol x_0$, причём $\\nabla f(\\boldsymbol x_0) = 0$. Тогда точка $\\boldsymbol x_0$ является точкой локального минимума функции $f$, если гессиан $\\nabla^2f(\\boldsymbol x_0)$ положительно определён, и точкой локального максимума, если он отрицательно определён.\n",
    "\n",
    "Проверить матрицу на положительную или отрицательную определённость можно с помощью [критерия Сильвестра](https://ru.wikipedia.org/wiki/Критерий_Сильвестра)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{admonition} Exercise\n",
    ":class: important\n",
    "Let $\\boldsymbol A \\in \\mathbb R^{m\\times n}$, $\\boldsymbol b \\in \\mathbb R^m$ and\n",
    "\n",
    "$$\n",
    " f(\\boldsymbol x) = \\Vert \\boldsymbol{Ax} - \\boldsymbol b\\Vert_2^2.\n",
    "$$\n",
    "\n",
    "Find $\\nabla f(\\boldsymbol x)$ and $\\nabla^2 f(\\boldsymbol x)$. At which point $\\widehat{\\boldsymbol x}$ the global minimum of $f$ is attained? \n",
    "\n",
    "```{seealso}\n",
    "The similar form has the loss function of {ref}`linear regression <linear-regression>`.\n",
    "```\n",
    "````"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
