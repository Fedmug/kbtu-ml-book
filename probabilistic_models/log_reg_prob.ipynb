{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "939f2946",
   "metadata": {},
   "source": [
    "# Probabilistic models for logistic regression\n",
    "\n",
    "Binary logistic regression predicts the probability of positive class:\n",
    "\n",
    "$$\n",
    "    \\mathbb P(y = 1 \\vert \\boldsymbol x,  \\boldsymbol w) = \\sigma(\\boldsymbol x^\\intercal \\boldsymbol w), \\quad \\sigma(t) = \\frac 1{1 + e^{-t}}.\n",
    "$$\n",
    "\n",
    "**Q**. What is the probability of negative class $\\mathbb P(y = 0 \\vert \\boldsymbol x,  \\boldsymbol w)$?\n",
    "\n",
    "It follows that\n",
    "\n",
    "$$\n",
    "p(y \\vert \\boldsymbol x,  \\boldsymbol w) = \\mathrm{Bern}(\\sigma(\\boldsymbol x^\\intercal \\boldsymbol w))\n",
    "$$\n",
    "\n",
    "**Q**. What is $p(y \\vert \\boldsymbol x,  \\boldsymbol w)$ if $y \\in\\{-1, +1\\}$?\n",
    "\n",
    "## MLE\n",
    "\n",
    "### Binary case\n",
    "\n",
    "Note that if $\\xi \\sim \\mathrm{Bern}(q)$ then $\\mathbb P(\\xi = t) = q^{t} (1-q)^{1-t}$.\n",
    "Denote $\\widehat y_i = \\sigma(\\boldsymbol x_i^\\intercal \\boldsymbol w)$ and write down the negative log-likelihood:\n",
    "\n",
    "$$\n",
    "    \\begin{multline*}\n",
    "    \\mathrm{NLL}(\\boldsymbol w) = -\\sum_{i=1}^n \\log p(y_i \\vert \\boldsymbol x_i, \\boldsymbol w) = -\\sum_{i=1}^n \\log\\big(\\widehat y_i^{y_i} (1 - \\widehat y_i)^{1 - y_i}\\big) = \\\\\n",
    "    -\\sum_{i=1}^n y_i\\log(\\widehat y_i) + (1-y_i) \\log(1 - \\widehat y_i).\n",
    "    \\end{multline*}\n",
    "$$\n",
    "\n",
    "This is exactly the binary cross-entropy loss between true labels $y_i$ and predictions $\\widehat y_i$.\n",
    "\n",
    "### Multinomial case\n",
    "\n",
    "Suppose that $\\mathcal Y = \\{1, \\ldots, K\\}$, then prediction on a sample $\\boldsymbol x \\in \\mathbb R^d$ is a vector of probabilities \n",
    "\n",
    "$$\n",
    "    \\boldsymbol{\\widehat y} = \\mathrm{Softmax}(\\boldsymbol x^\\intercal \\boldsymbol W), \\quad \\boldsymbol W \\in \\mathbb R^{d\\times K}.\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "    p(\\boldsymbol y \\vert \\boldsymbol X, \\boldsymbol W) = \\mathrm{Cat}(\\mathrm{Softmax}(\\boldsymbol X \\boldsymbol W)),\n",
    "$$\n",
    "\n",
    "where $\\mathrm{Cat}(\\boldsymbol p)$ is **categorical** (or **multinoully**) distribution over categories $\\{1, \\ldots, K\\}$.\n",
    "\n",
    "Now write the negative log-likelihood:\n",
    "\n",
    "$$\n",
    "\\mathrm{NLL}(\\boldsymbol W) = -\\log\\prod_{i=1}^n \\prod_{k=1}^K \\widehat y_{ik}^{y_{ik}} = -\\sum\\limits_{i=1}^n \\sum_{k=1}^K  y_{ik} \\log \\widehat y_{ik}.\n",
    "$$\n",
    "\n",
    "This is cross-entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3350579e",
   "metadata": {},
   "source": [
    "## MAP\n",
    "\n",
    "Consider binary case. As for linear regression, use gaussian prior:\n",
    "\n",
    "$$\n",
    "    p(\\boldsymbol w) = \\mathcal N(\\boldsymbol 0, \\tau^2\\boldsymbol I).\n",
    "$$\n",
    "\n",
    "Then \n",
    "\n",
    "$$\n",
    "-\\log p(\\boldsymbol w \\vert \\boldsymbol X, \\boldsymbol y) = -\\sum_{i=1}^n \\big(y_i\\log(\\widehat y_i) + (1-y_i) \\log(1 - \\widehat y_i)\\big) +  \\frac 1{2\\tau^2} \\sum\\limits_{j=1}^d w_j^2,\n",
    "$$\n",
    "\n",
    "where $\\widehat y_i = \\sigma(\\boldsymbol x_i^\\intercal \\boldsymbol w)$. Hence,\n",
    "\n",
    "$$\n",
    "\\boldsymbol {\\widehat w}_{\\mathrm{MAP}} = \\arg\\min\\limits_{\\boldsymbol w}\n",
    "\\bigg(-\\sum\\limits_{i=1}^n \\big(y_i\\log(\\widehat y_i) + (1-y_i) \\log(1 - \\widehat y_i)\\big)+ \\frac{1}{2\\tau^2} \\Vert \\boldsymbol w \\Vert_2^2\\bigg).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1240d5ef",
   "metadata": {},
   "source": [
    "This is $L_2$-regularization of the binary logistic regression. Taking laplacian prior as {ref}`here <laplace-prior>`, we obtain $L_1$ regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ce8863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
