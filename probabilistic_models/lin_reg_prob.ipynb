{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b9c51d5-52a0-4e65-9022-9c7fdc1a4a3f",
   "metadata": {},
   "source": [
    "(prob-lin-reg)=\n",
    "# Probabilistic models for linear regression\n",
    "\n",
    "Consider a regression problem with dataset $\\mathcal D = \\{(\\boldsymbol x_i, y_i)\\}_{i=1}^n$, $y_i\\in\\mathbb R$. The probabilistic model for the linear regression model assumes that\n",
    "\n",
    "$$\n",
    "    y_i = \\boldsymbol x_i^{\\mathsf T} \\boldsymbol w + \\varepsilon_i,\n",
    "$$\n",
    "\n",
    "where $\\varepsilon_i$ is some random noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fe9fb7-e006-4458-a57a-35afbe8927c8",
   "metadata": {},
   "source": [
    "## Gaussian model\n",
    "\n",
    "In this setting the random noise is gaussian: $\\varepsilon_i \\sim \\mathcal N (0, \\sigma^2)$. Hence,\n",
    "\n",
    "$$\n",
    "    p(y_i \\vert \\boldsymbol x_i, \\boldsymbol w) = \\mathcal N(\\boldsymbol x_i^{\\mathsf T} \\boldsymbol w, \\sigma^2) = \\frac 1{\\sqrt{2\\pi} \\sigma} \\exp\\Big(-\\frac{(\\boldsymbol x_i^{\\mathsf T} \\boldsymbol w - y_i)^2}{2\\sigma^2}\\Big).\n",
    "$$\n",
    "\n",
    "**Q**. What is $\\mathbb E y_i$? $\\mathbb V y_i$?\n",
    "\n",
    "A picture from [ML Handbook](https://academy.yandex.ru/handbook/ml/article/beta-pervoe-znakomstvo-s-veroyatnostnymi-modelyami#sluchajnost-kak-istochnik-nesovershenstva-modeli):\n",
    "\n",
    "```{figure} lin_reg_gaussians.png\n",
    ":align: center\n",
    "```\n",
    "\n",
    "**Q**. What is $\\boldsymbol x_i$ and $\\boldsymbol w$ in this picture? \n",
    "\n",
    "The likelihood of the dataset $\\mathcal D$ is\n",
    "\n",
    "$$\n",
    "    p(\\boldsymbol y \\vert \\boldsymbol X, \\boldsymbol w) = \\prod_{i=1}^n \\mathcal N(\\boldsymbol x_i^{\\mathsf T} \\boldsymbol w, \\sigma^2) = \\frac 1{(\\sqrt{2\\pi}\\sigma)^n}\\exp\\Big(-\\sum\\limits_{i=1}^n \\frac{(\\boldsymbol x_i^{\\mathsf T} \\boldsymbol w - y_i)^2}{2\\sigma^2}\\Big).\n",
    "$$\n",
    "\n",
    "Hence, the negative log-likelihood is\n",
    "\n",
    "$$\n",
    "    \\mathrm{NLL}(\\boldsymbol w) = -\\log p(\\boldsymbol y \\vert \\boldsymbol X, \\boldsymbol w)=\n",
    "    \\frac 1{2\\sigma^2} \\sum\\limits_{i=1}^n (\\boldsymbol x_i^{\\mathsf T} \\boldsymbol w - y_i)^2 + \\frac n2\\log(2\\pi\\sigma)^2.\n",
    "$$\n",
    "\n",
    "**Q.** How does this NLL relates to the loss function {eq}`lin-reg-loss-opt` of the linear regression model?\n",
    "\n",
    "Thus, the optimal weights {eq}`lin-reg-solution` of the linear regression with MSE loss coincide with MLE:\n",
    "\n",
    "$$\n",
    "    \\boldsymbol{\\widehat w} = \\arg\\max\\limits_{\\boldsymbol w} p(\\boldsymbol y \\vert \\boldsymbol X, \\boldsymbol w) = \\arg\\min\\limits_{\\boldsymbol w} \\mathrm{NLL}(\\boldsymbol w).\n",
    "$$\n",
    "\n",
    "**Q**. Let $\\sigma$ be also a learnable parameter. What is MLE $\\widehat \\sigma^2$ for it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d617d2",
   "metadata": {},
   "source": [
    "## Laplacian model\n",
    "\n",
    "Now suppose that our loss function is MAE. Then \n",
    "\n",
    "$$  \n",
    "    \\boldsymbol{\\widehat w} = \\arg\\min\\limits_{\\boldsymbol w} \\sum\\limits_{i=1}^n \\vert \\boldsymbol x_i^{\\mathsf T} \\boldsymbol w - y_i\\vert = \\arg\\max\\limits_{\\boldsymbol w} \\Big(-\\sum\\limits_{i=1}^n \\vert \\boldsymbol x_i^{\\mathsf T} \\boldsymbol w - y_i\\vert\\Big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e8c6d1",
   "metadata": {},
   "source": [
    "Which probabilistic model will give\n",
    "\n",
    "$$\n",
    "    \\mathrm{NLL}(\\boldsymbol w) = \\frac 1b\\sum\\limits_{i=1}^n \\vert \\boldsymbol x_i^{\\mathsf T} \\boldsymbol w - y_i\\vert + \\mathrm{const}?\n",
    "$$\n",
    "\n",
    "Well, then likelihood should be\n",
    "\n",
    "$$\n",
    "    p(\\boldsymbol y \\vert \\boldsymbol X, \\boldsymbol w) \\propto \\exp\\Big(-\\frac 1b\\sum\\limits_{i=1}^n \\vert\\boldsymbol x_i^{\\mathsf T} \\boldsymbol w - y_i\\vert\\Big) = \\prod\\limits_{i=1}^n \\exp\\Big(-\\frac{\\vert\\boldsymbol x_i^{\\mathsf T} \\boldsymbol w - y_i\\vert} b\\Big).\n",
    "$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\n",
    "    p(y_i \\vert \\boldsymbol x_i, \\boldsymbol w) \\propto \\exp\\Big(-\\frac{\\vert\\boldsymbol x_i^{\\mathsf T} \\boldsymbol w - y_i\\vert}b\\Big),\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "    y_i  \\sim \\mathrm{Laplace}(\\boldsymbol x_i^{\\mathsf T} \\boldsymbol w, b).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc02b7ed",
   "metadata": {},
   "source": [
    "## Bayesian linear regression\n",
    "\n",
    "### Gaussian prior\n",
    "\n",
    "Let prior distribution be Gaussian:\n",
    "\n",
    "$$\n",
    "    p(\\boldsymbol w) = \\mathcal N(\\boldsymbol 0, \\tau^2\\boldsymbol I).\n",
    "$$\n",
    "\n",
    "Then posterior distribution\n",
    "\n",
    "$$\n",
    "    p(\\boldsymbol w \\vert \\boldsymbol X, \\boldsymbol y) \\propto\n",
    "    p( \\boldsymbol y \\vert  \\boldsymbol X, \\boldsymbol w)p(\\boldsymbol w)\n",
    "$$\n",
    "\n",
    " is also Gaussian.\n",
    "\n",
    " ````{admonition} Example\n",
    ":class: important\n",
    "Let $y = ax$ be 1-d linear regression model and $p(a) = \\mathcal N(0, \\tau^2)$.\n",
    "Find posterior $p(a \\vert \\boldsymbol x, \\boldsymbol y)$ after observing a dataset $\\{(x_i, y_i)\\}_{i=1}^n$.\n",
    "\n",
    "```{admonition} Possible answer\n",
    ":class: tip, dropdown\n",
    "\n",
    "$$\n",
    "    p(a \\vert \\boldsymbol x, \\boldsymbol y) = \\mathcal N\\Bigg(\\frac{\\boldsymbol x^\\intercal\\boldsymbol y}{\\frac{\\sigma^2}{\\tau^2}+ \\sum\\limits_{i=1}^n x_i^2}, \\frac 1{\\frac 1{\\tau^2} + \\frac 1{\\sigma^2}\\sum\\limits_{i=1}^n x_i^2}\\Bigg)\n",
    "$$\n",
    "```\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3025e4a9",
   "metadata": {},
   "source": [
    "To find $\\boldsymbol {\\widehat w}_{\\mathrm{MAP}}$, we need to maximize posterior $p(\\boldsymbol w \\vert \\boldsymbol X, \\boldsymbol y)$. This is the same as to minimize\n",
    "\n",
    "$$\n",
    "    -\\log p(\\boldsymbol w \\vert \\boldsymbol X, \\boldsymbol y) = -\\log p( \\boldsymbol y \\vert  \\boldsymbol X, \\boldsymbol w) - \\log p(\\boldsymbol w) + \\mathrm{const}.\n",
    "$$\n",
    "\n",
    "Recall that $p( \\boldsymbol y \\vert  \\boldsymbol X, \\boldsymbol w) = \\mathcal N(\\boldsymbol{Xw}, \\sigma^2 \\boldsymbol I)$. According to calculations from [ML Handbook](https://academy.yandex.ru/handbook/ml/article/beta-bajesovskij-podhod-k-ocenivaniyu#primer-linejnaya-regressiya-s-l-2-regulyarizacziej-kak-model-s-gaussovskim-apriornym-raspredeleniem-na-vesa)\n",
    "\n",
    "$$\n",
    "    -\\log p( \\boldsymbol y \\vert  \\boldsymbol X, \\boldsymbol w) - \\log p(\\boldsymbol w)\n",
    "    = \\frac 1{2\\sigma^2} \\sum\\limits_{i=1}^n (y_i -\\boldsymbol x_i^\\intercal \\boldsymbol w)^2 + \\frac 1{2\\tau^2} \\sum\\limits_{j=1}^d w_j^2 + \\mathrm{const}.\n",
    "$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\n",
    "\\boldsymbol {\\widehat w}_{\\mathrm{MAP}} = \\arg\\min\\limits_{\\boldsymbol w}\n",
    "\\bigg(\\sum\\limits_{i=1}^n (y_i -\\boldsymbol x_i^\\intercal \\boldsymbol w)^2 + \\frac{\\sigma^2}{\\tau^2} \\Vert \\boldsymbol w \\Vert_2^2\\bigg).\n",
    "$$\n",
    "\n",
    "Can you recognize the objective of the {ref}`ridge regression <ridge>` with $C =  \\frac{\\sigma^2}{\\tau^2}$? The analytical solution is\n",
    "\n",
    "$$\n",
    "    \\boldsymbol {\\widehat w}_{\\mathrm{MAP}} = \\Big(\\boldsymbol X^\\intercal \\boldsymbol X + \\frac{\\sigma^2}{\\tau^2}\\boldsymbol I\\Big)^{-1}\\boldsymbol X^{\\intercal}\\boldsymbol y.\n",
    "$$\n",
    "\n",
    "Contnuing calculations, we can find posterior:\n",
    "\n",
    "$$\n",
    "    p(\\boldsymbol w \\vert \\boldsymbol X, \\boldsymbol y) = \\mathcal N\\Big(\\boldsymbol {\\widehat w}_{\\mathrm{MAP}},\\Big(\\frac 1{\\sigma^2}\\boldsymbol X^\\intercal \\boldsymbol X + \\frac{1}{\\tau^2}\\boldsymbol I\\Big)^{-1} \\Big).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226fecc3",
   "metadata": {},
   "source": [
    "(laplace-prior)=\n",
    "### Laplacian prior\n",
    "\n",
    "Consinder the following prior: $w_1, \\ldots, w_d$ are i.i.d laplacian random variables with parameter $\\lambda$. Then\n",
    "\n",
    "$$\n",
    "    p(\\boldsymbol w) = \\prod\\limits_{j=1}^d \\mathrm{Laplace}(\\lambda) = \n",
    "    \\prod\\limits_{j=1}^d \\frac\\lambda 2e^{-\\lambda|w_j|} = \\frac {\\lambda^d}{2^d}\n",
    "    \\exp\\Big(-\\sum\\limits_{j=1}^d\\lambda|w_j|\\Big).\n",
    "$$\n",
    "\n",
    "The likelihood $p( \\boldsymbol y \\vert  \\boldsymbol X, \\boldsymbol w)$ is still \n",
    "$\\mathcal N(\\boldsymbol{Xw}, \\sigma^2 \\boldsymbol I)$. Then\n",
    "\n",
    "$$\n",
    "-\\log p(\\boldsymbol w \\vert \\boldsymbol X, \\boldsymbol y) = \\frac 1{2\\sigma^2} \\sum\\limits_{i=1}^n (y_i -\\boldsymbol x_i^{\\mathsf{T}} \\boldsymbol w)^2 +  \\lambda\\sum\\limits_{j=1}^d \\vert w_j\\vert + \\mathrm{const}.\n",
    "$$\n",
    "\n",
    "Hence, maximum a posteriori estimation is\n",
    "\n",
    "$$\n",
    "\\boldsymbol {\\widehat w}_{\\mathrm{MAP}} = \\arg\\min\\limits_{\\boldsymbol w}\n",
    "\\Big(\\Vert \\boldsymbol{Xw} - \\boldsymbol y\\Vert_2^2 + 2\\sigma^2 \\lambda \\Vert \\boldsymbol w \\Vert_1\\Big).\n",
    "$$\n",
    "\n",
    "This is exactly the objective of {ref}`LASSO <lasso>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f451b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
