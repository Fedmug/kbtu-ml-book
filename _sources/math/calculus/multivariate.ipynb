{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate calculus\n",
    "\n",
    "The idea is the same: approximate the increment $\\Delta f$ by a linear function:\n",
    "\n",
    "$$\n",
    "\tf(x+h) - f(x) = Df(x)[h] + \\mathcal O\\big(\\Vert h\\Vert^2\\big), \\Vert h\\Vert \\to 0\n",
    "$$\n",
    "\n",
    "(cp. with {eq}`derivative-O-def`). Here **differential** $Df(x)[h]$ is a linear function (which is usually called **linear operator** in multidimensional case) with respect to increment $h$ which represents the main linear part of the increment $\\Delta f = f(x+h) - f(x)$.\n",
    "\n",
    "<!--\n",
    "Перейдём теперь к более общей ситуации. Пусть заданы два конечномерных пространства $U$ и $V$ (например, векторов или матриц). Функция $f \\colon X \\to V$, $X \\subset U$, дифференцируема в точке $x\\in X$, если найдётся такой линейный оператор $L\\colon U \\to V$, что справедливо равенство\n",
    "\n",
    "$$\n",
    "\tf(x+h) - f(x) = L[h] + o(\\Vert h\\Vert) \\text{ при } \\Vert h\\Vert \\to 0.\n",
    "$$\n",
    "\n",
    "Линейное отображение $L$ из мира $x$-ов в мир значений $f$ называется **дифференциалом** (**производной**, **производным отображением**) функции $f$ в точке $x$ и обозначается $df(x)$ или $Df(x)[h]$. Фактически дифференциал зависит от двух аргументов: $df \\colon X\\times U \\to V$, причём по второму аргументу он линеен. Среди всех линейных операторов из $U$ в $V$ дифференциал наилучшим образом приближает приращение функции: \n",
    "\n",
    "$$\n",
    "f(x+h) - f(x) \\approx Df(x)[h].\n",
    "$$ \n",
    "\n",
    "\n",
    "```{note}\n",
    "Нотация $Df(x)[h]$ для дифференциала выглядит несколько громоздко, зато в неё явно включены обе переменные, от которых зависит дифференциал. При вычислении дифференциала функции $f$ в произвольной точке $x$ часто используют более короткую запись $df(x)$ или даже $df$; переменная приращения в таких случаях традиционно обозначается через $dx$. Обозначение $dx$ следует воспринимать как единый неделимый символ, по смыслу полностью тождественный приращению $h$. \n",
    "``` \n",
    "-->\n",
    "\n",
    "## Differentials of scalar functions\n",
    "\n",
    "In the previous section we considered the simplest case when $f \\colon \\mathbb R \\to \\mathbb R$ is a real-valued function of real argument. Then its differential is written as $df(x) = f'(x) dx$ or $Df(x)[h] = f'(x)h$. Now see what happens when the function $f$ has multidimensional domain.\n",
    "\t\n",
    "\n",
    "### Functions of vectors\n",
    "\n",
    "let $f\\colon \\mathbb R^n \\to \\mathbb R$ and $\\boldsymbol x = (x_1, \\ldots, x_n)^\\mathsf{T}$. Then \n",
    "\n",
    "```{math}\n",
    ":label: vector-func-diff\n",
    "Df(\\boldsymbol x)[\\boldsymbol h] = \\sum\\limits_{i=1}^n\n",
    "\\frac{\\partial f(\\boldsymbol x)}{\\partial \t x_i} h_i = \n",
    "\\langle \\nabla f(\\boldsymbol x), \\boldsymbol h\\rangle = \n",
    "\\nabla     f       (\\boldsymbol x)^\\mathsf{T} \t\t\\boldsymbol h,\n",
    "```\n",
    "\n",
    "Here $\\frac{\\partial f(\\boldsymbol x)}{\\partial \t x_i}$ are [partial derivatives](https://en.wikipedia.org/wiki/Partial_derivative) of $f$ at point $\\boldsymbol x$. The vector of partial derivatives\n",
    "    \n",
    "$$\n",
    "\\nabla f = \\Big(\\frac{\\partial f}{\\partial x_1}, \\ldots,\\frac{\\partial f}{\\partial x_n}\\Big)^\\mathsf{T}\n",
    "$$\n",
    "\n",
    "is called **gradient** of $f$. Alternative notation for differential:\n",
    "\n",
    "$$\n",
    "df(\\boldsymbol x) = \\sum\\limits_{i=1}^n\n",
    "\\frac{\\partial f(\\boldsymbol x)}{\\partial \t x_i} dx_i = \\nabla f(\\boldsymbol x)^\\mathsf{T} d\\boldsymbol \tx, \\quad\n",
    "d\\boldsymbol x = (dx_1, \\ldots, dx_n)^\\mathsf{T}.\n",
    "$$\n",
    "\n",
    "Note that both increment $d\\boldsymbol x$ and gradient $\\nabla f$ have the same shape as variable $\\boldsymbol x$: they are columns of size $n$. All partial derivatives are functions of $\\boldsymbol x$, therefore, gradient also depends on $\\boldsymbol x$.\n",
    "\n",
    "\n",
    "````{admonition} Question\n",
    ":class: important\n",
    "How to write $df$ in vector form if the argument of $f$ is a row vector $\\boldsymbol x = (x_1, \\ldots, x_n)$?\n",
    "\n",
    "```{admonition} Answer\n",
    ":class: dropdown, tip\n",
    "The coordinate form is the same:\n",
    "\n",
    "$$\n",
    "df = \\sum\\limits_{i=1}^n\n",
    "\\frac{\\partial f(\\boldsymbol x)}{\\partial \t x_i} dx_i.\n",
    "$$\n",
    "\n",
    "Now $\\nabla f(\\boldsymbol x)$ and $d\\boldsymbol x$ are rows, hence\n",
    "\n",
    "$$\n",
    "    df(\\boldsymbol x) = \\nabla f(\\boldsymbol x) d\\boldsymbol \tx^\\mathsf{T}\n",
    "$$\n",
    "\n",
    "Alternative notation:\n",
    "\n",
    "$$\n",
    "\tDf(\\boldsymbol x)[\\boldsymbol h] = \\nabla    f(\\boldsymbol x)\t\t\\boldsymbol h ^\\mathsf{T}.\n",
    "$$\n",
    "```\n",
    "````\n",
    "<!--\n",
    "**Вопрос на подумать**. Как записать в векторной форме дифференциал функции $f\\colon \\mathbb R^n \\to \\mathbb R$, принимающей на вход вектор-строку, а не вектор-столбец?\n",
    "\n",
    "<details>\n",
    "<summary markdown=\"span\">Попробуйте прийти к ответу самостоятельно, прежде чем смотреть решение. </summary>\n",
    "<div>\n",
    "В координатах разницы в записи дифференциала нет, по-прежнему \n",
    "    \n",
    "$$\n",
    "    df = \\sum\\limits_{i=1}^n \\frac{\\partial f}{\\partial \t x_i} dx_i.\n",
    "$$\n",
    "    \n",
    "Поскольку $\\boldsymbol x$ теперь вектор-строка, такую же форму имеют и градиент, и приращение. Подружить их в скалярном произведении, в котором всегда вектор-строка умножается на вектор-столбец, можно так: \n",
    "    \n",
    "$$\n",
    "    df(\\boldsymbol x) = \\nabla f(\\boldsymbol x) d\\boldsymbol \tx^\\top,\n",
    "$$\n",
    "    \n",
    "а в другой нотации — так: \n",
    "\t\n",
    "$$\n",
    "\tDf(\\boldsymbol x)[\\boldsymbol h] = \\nabla    f(\\boldsymbol x)\t\t\\boldsymbol h ^\\top.\n",
    "$$\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "-->\n",
    "\n",
    "\n",
    "<!--\n",
    "### Gradients shape\n",
    "\n",
    "В {eq}`vector-func-diff` и {eq}`matrix-func-diff` пришлось столкнуться с давним знакомым из матанализа: градиентом $\\nabla f$ скалярной функции $f$, который состоит из частных производных этой функции по всем координатам её аргумента $x$. При этом его обычно упаковывают в ту же форму, что и сам аргумент: если $x$ — вектор-строка, то и градиент записывается вектор-строкой, а если $x$ — матрица, то и градиент тоже будет матрицей того же размера. Это важно, например, для алгоритма градиентного спуска, при осуществлении которого мы должны уметь прибавлять градиент к точке, в которой он посчитан.\n",
    "-->\n",
    "\n",
    "```{note}\n",
    "Иногда приходится дифференцировать функции, зависящие от различных наборов аргументов, и тогда во избежание неоднозачности в индексе у буквы $\\nabla$ указывают аргумент, по которому осуществляется дифференцирование. Например, запись $\\nabla_{\\boldsymbol w} f(\\boldsymbol X, \\boldsymbol y, \\boldsymbol w)$ недвусмысленно даёт понять, что мы дифференцируем фукнцию $f$ по вектору $\\boldsymbol w$, а $\\boldsymbol X$ и $\\boldsymbol y$ рассматриваем при этом как фиксированные параметры.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<!--\n",
    "Посмотрим теперь, как выглядит дифференцирование для функций, которые на выходе выдают не скаляр, а вектор или матрицу.\n",
    "\n",
    "(activation-diff)=\n",
    "### Applying a function elementwise \n",
    "\n",
    "Пусть \n",
    "\n",
    "```{math}\n",
    ":label: elementwise-func\n",
    "f \\colon \\mathbb R^m \\to \\mathbb R^m,\\quad  \n",
    "f(\\boldsymbol x) = \\begin{pmatrix} \n",
    "\\varphi(x_1)\\\\ \\vdots\\\\ \\varphi(x_m) \n",
    "\\end{pmatrix},\n",
    "```\n",
    "\n",
    "где $\\varphi \\colon \\mathbb R \\to \\mathbb R$ — некоторая гладкая функция. Тогда\n",
    "\n",
    "$$\n",
    "\tf(\\boldsymbol x + \\boldsymbol h) - f(\\boldsymbol x) = \n",
    "\t    \\begin{pmatrix}\n",
    "\t            \\varphi(x_1 + h_1) - \\varphi(x_1)\\\\\n",
    "\t            \\vdots \\\\\n",
    "\t            \\varphi(x_m + h_m) - \\varphi(x_m) \n",
    "\t    \\end{pmatrix}\n",
    "\t    \\approx \n",
    "\t    \\begin{pmatrix}\n",
    "\t            \\varphi'(x_{1}) h_1\\\\\n",
    "\t            \\vdots  \\\\\n",
    "\t            \\varphi'(x_{m}) h_m\n",
    "\t    \\end{pmatrix} \n",
    "\t    =\n",
    "\t        \\begin{pmatrix}\n",
    "\t            \\varphi'(x_{1}) \\\\\n",
    "\t            \\vdots \\\\\n",
    "\t            \\varphi'(x_{m})\n",
    "\t        \\end{pmatrix}\n",
    "\t        \\odot\n",
    "\t           \\boldsymbol h.\n",
    "$$\n",
    "\n",
    "В последнем выражении происходит покомпонентное умножение. Если обозначить через $\\varphi'(\\boldsymbol x)$ покомпонетное применение функции $\\varphi'$ к координатам вектора $\\boldsymbol x = (x_1, \\ldots, x_m)^\\top$, то дифференциал функции $f$ можно записать как\n",
    "\n",
    "$$\n",
    "\tDf(\\boldsymbol x)[\\boldsymbol h] = \\varphi'(\\boldsymbol x) \\odot \\boldsymbol h =\\boldsymbol h \\odot \\varphi'(\\boldsymbol x).\n",
    "$$\n",
    "\n",
    "(linear-diff)=\n",
    "### Linear matrix function\n",
    "\n",
    "Let $f(\\boldsymbol X) = \\boldsymbol{XW}$, где $\\boldsymbol X$ и $\\boldsymbol W$ — матрицы подходящего размера. Тогда\n",
    "\t\n",
    "$$\n",
    "f(\\boldsymbol X + \\boldsymbol H) - f(\\boldsymbol X) = (\\boldsymbol X + \\boldsymbol H) \\boldsymbol W - \\boldsymbol X \\boldsymbol W = \\boldsymbol H \\boldsymbol W.\n",
    "$$\n",
    "\n",
    "Получилась линейная по $\\boldsymbol H$ функция, поэтому она и является дифференциалом функции $f$ в точке $\\boldsymbol X$: $Df(\\boldsymbol X) [\\boldsymbol H] = \\boldsymbol{HW}$. Допустима также запись $df(\\boldsymbol X) = d\\boldsymbol X\\cdot \\boldsymbol W$.\n",
    "\n",
    "```{admonition} Exercise\n",
    ":class: important\n",
    "Найдите дифференциал функции $f(\\boldsymbol W) = \\boldsymbol{XW}$, где $\\boldsymbol X$ и $\\boldsymbol W$ — матрицы подходящего размера.\n",
    "``` \n",
    "-->\n",
    "\n",
    "## Differentials of vector functions\n",
    "\n",
    "Пусть $f\\colon \\mathbb R^m \\to \\mathbb R^n$, тогда дифференциал должен быть линейным оператором из $\\mathbb R^m$ в $\\mathbb R^n$, то есть матрицей размера $n\\times m$. И, действительно, с помощью **матрицы Якоби** \n",
    "\n",
    "```{math}\n",
    ":label: jacobi\n",
    "\\boldsymbol J_f = \\frac{\\partial f}{\\partial \\boldsymbol x} = \n",
    "\t\t    \\begin{pmatrix} \n",
    "\t    \t        \\frac{\\partial f_1}{\\partial x_1} & \\ldots & \n",
    "\t        \t    \\frac{\\partial f_1}{\\partial x_m} \\\\\n",
    "\t        \t\\vdots & & \\vdots \\\\\n",
    "\t            \t\\frac{\\partial f_n}{\\partial x_1} & \\ldots & \n",
    "\t            \t\\frac{\\partial f_n}{\\partial x_m}\\\\\n",
    "\t    \t\\end{pmatrix}\n",
    "```\n",
    "\t\n",
    "\n",
    "\n",
    "differential is written as \n",
    "\n",
    "```{math}\n",
    ":label: vec2vec-diff\n",
    "Df(\\boldsymbol x)[\\boldsymbol h] = \\boldsymbol J_f(\\boldsymbol x)\\boldsymbol h \\text { or }\n",
    "df(\\boldsymbol x) = \\boldsymbol J_f(\\boldsymbol x) d\\boldsymbol x.\n",
    "```\n",
    "\n",
    "````{admonition} Question\n",
    ":class: important\n",
    "Let $f\\colon \\mathbb R^m \\to \\mathbb R^n$ be a mapping from a row vector $\\boldsymbol x = (x_1, \\ldots, x_m)$ to another row vector $(f_1(\\boldsymbol x), \\ldots, f_n(\\boldsymbol x))$. How to write $df$ in matrix form?\n",
    "\n",
    "\n",
    "```{admonition} Answer\n",
    ":class: dropdown, tip\n",
    "Значение дифференциала $Df(\\boldsymbol x)[\\boldsymbol h]$ теперь должно быть вектором-строкой, а не вектором-столбцом. Этого легко добиться транспонированием {eq}`vec2vec-diff`:\n",
    "\n",
    "$$\n",
    "Df(\\boldsymbol x)[\\boldsymbol h] = \\boldsymbol h^\\mathsf{T} \\boldsymbol J_f(\\boldsymbol x)^{\\mathsf T}.\n",
    "$$\n",
    "\n",
    "Следует учесть, однако, что теперь приращение $\\boldsymbol h$ — это строка, поэтому его транспонировать не нужно. В итоге получаем \n",
    "\n",
    "$$\n",
    "Df(\\boldsymbol x)[\\boldsymbol h] = \\boldsymbol h \\boldsymbol J_f(\\boldsymbol x)^\\mathsf{T}, \\text{ or }\n",
    "df = d\\boldsymbol x \\cdot \\boldsymbol J_f(\\boldsymbol x)^\\mathsf{T}.\n",
    "$$\n",
    "```\n",
    "````\n",
    "\n",
    "<!--\n",
    "**Вопрос на подумать**. Как записать в векторно-матричной форме дифференциал функции $f\\colon \\mathbb R^m \\to \\mathbb R^n$, отображающей вектор-строку $\\boldsymbol x = (x_1, \\ldots, x_m)$ в вектор-строку $(f_1(\\boldsymbol x), \\ldots, f_n(\\boldsymbol x))$?\n",
    "\n",
    "<details>\n",
    "    <summary markdown=\"span\">Попробуйте прийти к ответу самостоятельно, прежде чем смотреть решение. </summary>\n",
    "    <div>\n",
    "\tЗначение дифференциала $$Df(\\boldsymbol x)[\\boldsymbol h]$$ теперь должно быть вектором-строкой, а не вектором-столбцом. Этого легко добиться транспонированием: применяя её к формуле из предыдущего примера, получаем $$Df(\\boldsymbol x)[\\boldsymbol h] = \\boldsymbol h^T \\boldsymbol J_f(\\boldsymbol x)^T$$. Следует учесть, однако, что теперь приращение $$\\boldsymbol h$$ — это строка, поэтому его транспонировать не нужно. В итоге получаем $$Df(\\boldsymbol x)[\\boldsymbol h] = \\boldsymbol h \\boldsymbol J_f(\\boldsymbol x)^T$$ (альтернативная запись: $$df = d\\boldsymbol x \\cdot \\boldsymbol J_f(\\boldsymbol x)^T$$).\n",
    "\t</div>\n",
    "</details>\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential in general case\n",
    "\n",
    "Функция $f \\colon X \\to V$, $X \\subset U$, дифференцируема в точке $x\\in X$, если найдётся такой линейный оператор $L\\colon U \\to V$, что справедливо равенство\n",
    "\n",
    "$$\n",
    "\tf(x+h) - f(x) = L[h] + o(\\Vert h\\Vert) \\text{ при } \\Vert h\\Vert \\to 0.\n",
    "$$\n",
    "\n",
    "Линейное отображение $L$ из мира $x$-ов в мир значений $f$ называется **дифференциалом** (**производной**, **производным отображением**) функции $f$ в точке $x$ и обозначается $df(x)$ или $Df(x)[h]$. Фактически дифференциал зависит от двух аргументов: $df \\colon X\\times U \\to V$, причём по второму аргументу он линеен. Среди всех линейных операторов из $U$ в $V$ дифференциал наилучшим образом приближает приращение функции: \n",
    "\n",
    "$$\n",
    "f(x+h) - f(x) \\approx Df(x)[h].\n",
    "$$\n",
    "\n",
    "```{note}\n",
    "Нотация $Df(x)[h]$ для дифференциала выглядит несколько громоздко, зато в неё явно включены обе переменные, от которых зависит дифференциал. При вычислении дифференциала функции $f$ в произвольной точке $x$ часто используют более короткую запись $df(x)$ или даже $df$; переменная приращения в таких случаях традиционно обозначается через $dx$. Обозначение $dx$ следует воспринимать как единый неделимый символ, по смыслу полностью тождественный приращению $h$. \n",
    "``` \n",
    "\n",
    "### Special cases\n",
    "\n",
    "* $U = V = \\mathbb R$: \n",
    "    \n",
    "    $$\n",
    "        Df(x)[h] = f'(x)h \\text{ or } df = f'(x)dx\n",
    "    $$\n",
    "    \n",
    "* $U = \\mathbb R^n$, $V = \\mathbb R$: \n",
    "\n",
    "    $$\n",
    "     Df(\\boldsymbol x)[\\boldsymbol h] = \\nabla f(\\boldsymbol x)^{\\mathsf T} \\boldsymbol h \\text{ or }\n",
    "     df = \\nabla f^{\\mathsf T} d\\boldsymbol x\n",
    "    $$\n",
    "\n",
    "* $U = \\mathbb R^{m\\times n}$, $V = \\mathbb R$ (see {ref}`next section <matrix-gradient>`: \n",
    "\n",
    "    $$\n",
    "     Df(\\boldsymbol X)[\\boldsymbol H] = \\mathrm{tr}\\big(\\nabla f(\\boldsymbol X)^{\\mathsf T} \\boldsymbol H\\big) \\text{ or }\n",
    "     df = \\mathrm{tr}\\big(\\nabla f^{\\mathsf T} d\\boldsymbol X\\big)\n",
    "    $$\n",
    "    \n",
    "* $U = \\mathbb R^{m}$, $V = \\mathbb R^n$: \n",
    "\n",
    "    $$\n",
    "     Df(\\boldsymbol x)[\\boldsymbol h] = J_f(\\boldsymbol x) \\boldsymbol h \\text{ or }\n",
    "     df = J_f d\\boldsymbol x\n",
    "    $$\n",
    "    \n",
    "```{admonition} Question\n",
    ":class: important\n",
    "If $f\\colon U \\to V$ is differentiable at point $x$, what is the shape of $Df(x)[h]$?\n",
    "```\n",
    "\n",
    "### Rules of differentiation\n",
    "\n",
    "1. **Производная константы**. Если $f(x) = \\mathrm{const}$, то $f(x+h) - f(x) = 0$, и по определению $Df(x)[h] = 0$. По этой же причине градиент постоянной функции векторного или матричного аргумента также равен нулю.\n",
    "\n",
    "2. **Дифференциал линейного отображения**. Пусть $f(x)$ — линейное отображение. Тогда\n",
    "\n",
    "$$\n",
    "f(x + h) - f(x) = f(x) + f(h) - f(x) = f(h).\n",
    "$$\n",
    "\n",
    "Поскольку справа линейное отображение, то по определению оно и является дифференциалом $Df(x)[h]$. \n",
    "\n",
    "3. **Линейность дифференциала**. Пусть функции $f \\colon X \\to V$ и $g \\colon X \\to V$ дифференцируемы в точке $x\\in X$ и $\\alpha, \\beta \\in \\mathbb R$. Тогда функция $\\alpha f + \\beta g$ дифференцируема в точке $x$, и\n",
    "\n",
    "\t$$\n",
    "\tD(\\alpha f + \\beta g)(x)[h] = \\alpha Df(x)[h] + \\beta Dg(x)[h].\n",
    "\t$$\n",
    "\n",
    "4. **Дифференциал произведения**. Пусть функции $u \\colon X \\to \\mathbb R$, $v \\colon X \\to V$ дифференцируемы в точке $x$. Тогда функция $u\\cdot v$ дифференцируема в точке $x$, и\n",
    "\n",
    "\t$$\n",
    "\t\tD(uv)(x)[h] = Du(x)[h]\\cdot v(x) + u(x)\\cdot Dv(x)[h].\n",
    "\t$$\n",
    " \n",
    "\tЭто же правило сработает и для скалярного произведения:\n",
    "\n",
    "\t$$\n",
    "\t\tD \\langle u(x), v(x)\\rangle [h] = \\langle Du(x)[h], v(x)\\rangle + \\langle u(x), \tDv(x)[h]\\rangle.\n",
    "\t$$\n",
    "\n",
    "5. **Производная сложной функции.** Пусть функция $u \\colon X \\to Y$ дифференцируема в точке $x$, а функция $v \\colon Y \\to Z$ дифференцируема в точке $u(x)$. Тогда их композиция $v\\circ u$ дифференцируема в точке $x$, и\n",
    "\n",
    "\t$$\n",
    "\tD(u\\circ v)(x)[h] = Dv(u(x))\\Big[Du(x)[h]\\Big].\n",
    "\t$$\n",
    "\n",
    "\t```{admonition} Proof\n",
    "\t:class: dropdown, tip\n",
    "\tОбозначим $f(x) = (v\\circ u)(x)$, тогда\n",
    "\n",
    "\t$$\n",
    "\t\tf(x+h) - f(x) = v(u(x+h)) - v(u(x)) \\approx v\\big(u(x) + Du(x)[h]\\big) - v(u(x)).\n",
    "\t$$\n",
    "\n",
    "\tПолучилось приращение функции $v$, взятое в точке $u(x)$ с приращением $Du(x)[h]$. Приближённо оно равно дифференциалу $Dv(u(x))\\Big[Du(x)[h]\\Big]$.\n",
    "\t```\n",
    "\n",
    "\tВ частности, если $Y \\subset \\mathbb R$ и $Z = \\mathbb R$, то $d(v\\circ u)(x) = v'(u(x))du(x)$.\n",
    "\n",
    "6. Важный частный случай: **дифференцирование перестановочно с линейным отображением**. Пусть $f(x) = L(v(x))$, где $L$ — линейное отображение. Тогда $DL(v(x))$ совпадает с самим $L$ и формула упрощается:\n",
    "\n",
    "\t$$\n",
    "\tD(L\\circ v)(x)[h] = L(Dv(x)[h]).\n",
    "\t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "```{admonition} Exercise\n",
    ":class: important\n",
    "Let $\\boldsymbol A \\in \\mathbb R^{n\\times n}$ and\n",
    "\n",
    "$$\n",
    " f(\\boldsymbol x) = \\langle \\boldsymbol{Ax}, \\boldsymbol x\\rangle = \\boldsymbol x^\\top \\boldsymbol{Ax}.\n",
    "$$\n",
    "\n",
    "Find $\\nabla f(\\boldsymbol x)$.\n",
    "\n",
    "```{admonition} Solution\n",
    ":class: tip, dropdown\n",
    "\n",
    "Воспользуемся формулой дифференциала произведения:\n",
    "\n",
    "$$\n",
    "\tdf = \\langle d(\\boldsymbol{Ax}), \\boldsymbol x\\rangle + \\langle \\boldsymbol{Ax}, d\\boldsymbol x\\rangle = \\langle \\boldsymbol A d\\boldsymbol x, \\boldsymbol x\\rangle + \\langle \\boldsymbol{Ax}, d\\boldsymbol x\\rangle. \n",
    "$$\n",
    "\n",
    "Чтобы найти градиент, нам надо это выражение представить в виде $\\langle ?, d\\boldsymbol x\\rangle$. Для этого поменяем местами множители первого произведения и перенесём $\\boldsymbol A$ в другую сторону ($\\boldsymbol A$ перенесётся с транспонированием):\n",
    "\n",
    "$$\n",
    "\tdf = \\langle d\\boldsymbol x, \\boldsymbol A^T\\boldsymbol x\\rangle + \\langle \\boldsymbol{Ax}, d\\boldsymbol x\\rangle = \\langle (\\boldsymbol A^T + \\boldsymbol A)\\boldsymbol x, d\\boldsymbol x\\rangle.\n",
    "$$\n",
    "\n",
    "Отсюда получаем, что $\\nabla f(\\boldsymbol x) = (\\boldsymbol A^T + \\boldsymbol A)\\boldsymbol x$.\n",
    "```\t\n",
    "````\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second differential\n",
    "\n",
    "Дифференциал $Df(x)[h]$  функции\n",
    "\n",
    "$$\n",
    "f \\colon X \\to V, \\quad X\\subset U,\n",
    "$$\n",
    "\n",
    "где $U$ и $V$ — некоторые конечномерные пространства, зависит от двух аргументов: $x\\in X$ и $h\\in U$. Зафиксировав приращение $h_1\\in U$, получим функцию от переменной $x$: \n",
    "\n",
    "$$\n",
    "g(x) = Df(x)[h_1].\n",
    "$$\n",
    "\n",
    "А значит, её тоже можно попробовать продифференцировать! Если это возможно, то её дифференциал $Dg(x)[h_2]$ называется **вторым дифференциалом** функции $f$, и обозначается \n",
    "\n",
    "$$\n",
    "D^2f(x)[h_1, h_2] \\text{ или } d^2f(x).\n",
    "$$\n",
    "\n",
    "На сей раз это не линейный оператор, а билинейная форма: при фиксированном $x_0\\in X$ функция $D^2f(x_0)[h_1, h_2]$ линейна по каждому из аргументов приращения.\n",
    "\n",
    "Для функций $f\\colon \\mathbb R^n \\to \\mathbb R$ второй дифференциал, как и любую билинейную форму, можно представить в виде матрицы:\n",
    "\n",
    "$$\n",
    "\tD^2f(\\boldsymbol x)[\\boldsymbol h_1, \\boldsymbol h_2] = \\langle \\boldsymbol H_x \\boldsymbol h_1, \\boldsymbol h_2\\rangle = \\boldsymbol h_2^\\mathsf{T} \\boldsymbol H_x \\boldsymbol h_1, \\quad \\boldsymbol H_x \\in \\mathbb R^{n\\times n}.\n",
    "$$\n",
    "\n",
    "Матрица $\\boldsymbol H_x$ назвается **гессианом** функции $f$ в точке $\\boldsymbol x$ и обозначается $\\nabla^2 f( \\boldsymbol x)$. Гессиан состоит из вторых частных производных:\n",
    "\n",
    "$$\n",
    "\t\\nabla^2 f(\\boldsymbol x) = \\left \\| \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}(\\boldsymbol x)\\right\\|_{i, j=1}^n.\n",
    "$$\n",
    "\n",
    "Если все вторые частные производные функции $f$ непрерывны, то её гессиан представляет собой симметричную матрицу (в этом состоит утверждение [теоремы Шварца](https://en.wikipedia.org/wiki/Symmetry_of_second_derivatives#Theorem_of_Schwarz)). Также для таких функций справедливо разложение по формуле Тейлора до второго порядка:\n",
    "\n",
    "$$\n",
    "\tf(\\boldsymbol x+ \\boldsymbol h) = f(\\boldsymbol x) + Df(\\boldsymbol x)[\\boldsymbol h] + \\frac 12 D^2f(\\boldsymbol x)[\\boldsymbol h, \\boldsymbol h] + o(\\|\\boldsymbol h\\|^2),\n",
    "$$\n",
    "\n",
    "или, в другой записи,\n",
    "\n",
    "$$\n",
    "\tf(\\boldsymbol x+\\boldsymbol h) = f(\\boldsymbol x) + \\nabla f(\\boldsymbol x)^\\mathsf{T} \\boldsymbol h + \\frac 12 \\boldsymbol h^\\mathsf{T}\\nabla ^2f(\\boldsymbol x)\\boldsymbol h + o(\\|\\boldsymbol h\\|^2).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Let $\\boldsymbol a \\in \\mathbb R^n$ be a fixed vector and\n",
    "\n",
    "    $$\n",
    "        f(\\boldsymbol x) = \\sum\\limits_{k=1}^n a_k x_k = \\boldsymbol a^\\mathsf{T} \\boldsymbol x.\n",
    "    $$\n",
    "\n",
    "    Find $\\nabla f(\\boldsymbol x)$.\n",
    "    \n",
    "2. Let $\\varphi \\in C^1(\\mathbb R)$, $\\boldsymbol x \\in \\mathbb R^m$, and\n",
    "\n",
    "    ```{math}\n",
    "    :label: elementwise-func\n",
    "    f(\\boldsymbol x) = \\begin{pmatrix} \n",
    "    \\varphi(x_1)\\\\ \\vdots\\\\ \\varphi(x_m) \n",
    "    \\end{pmatrix}, \\quad \n",
    "    g(\\boldsymbol x) = \\begin{pmatrix} \n",
    "    \\varphi'(x_1)\\\\ \\vdots\\\\ \\varphi'(x_m) \n",
    "    \\end{pmatrix}\n",
    "    ```\n",
    "\n",
    "    Show that \n",
    "    \n",
    "    ```{math}\n",
    "    :label: elementwise-diff \n",
    "        Df(\\boldsymbol x)[\\boldsymbol h] = g(\\boldsymbol x) \\odot \\boldsymbol h =\\boldsymbol h \\odot g(\\boldsymbol x).\n",
    "    ```\n",
    "\n",
    "3. According to {eq}`jacobi`, the differential of the function $f$ from {eq}`elementwise-func` should be an $m\\times m$ matrix. How does it look like?\n",
    "\n",
    "4. Let $\\boldsymbol A \\in \\mathbb R^{n\\times n}$ and\n",
    "\n",
    "    $$\n",
    "     f(\\boldsymbol x) = \\langle \\boldsymbol{Ax}, \\boldsymbol x\\rangle = \\boldsymbol x^\\mathsf{T} \\boldsymbol{Ax}.\n",
    "    $$\n",
    "\n",
    "    Find $\\nabla f(\\boldsymbol x)$ and $\\nabla^2 f(\\boldsymbol x)$.\n",
    "\n",
    "5. *Logistic function* is\n",
    "\n",
    "    $$\n",
    "    f(\\boldsymbol x) = \\log\\big(1 + \\exp(\\boldsymbol a^\\mathsf{T} \\boldsymbol x)\\big), \\quad\n",
    "    \\boldsymbol a \\in \\mathbb R^n.\n",
    "    $$\n",
    "    \n",
    "    Find $\\nabla f(\\boldsymbol x)$ and $\\nabla^2 f(\\boldsymbol x)$.\n",
    "    \n",
    "6. Let $\\boldsymbol A = \\boldsymbol A^\\mathsf{T}$ and $f(\\boldsymbol x) = \\frac 12 \\Vert\\boldsymbol{xx}^\\mathsf{T} - \\boldsymbol A\\Vert_F^2$. Find $\\nabla f(\\boldsymbol x)$ and $\\nabla^2 f(\\boldsymbol x)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
