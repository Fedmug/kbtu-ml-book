{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(backprop)=\n",
    "# Back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outline:**\n",
    "\n",
    "* Make the forward pass and calculate all hidden representations along with output and loss function:\n",
    "\n",
    "$$\n",
    "\\boldsymbol X_1, \\boldsymbol X_2, \\ldots \\boldsymbol X_{L-1}, \\boldsymbol X_L = \\boldsymbol{\\widehat Y}, \\mathcal L(\\boldsymbol {\\widehat Y}, \\boldsymbol Y) \n",
    "$$\n",
    "\n",
    "* Calculate the gradient of the loss function with respect to the output:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\boldsymbol X_L} \\mathcal L(\\boldsymbol X_L, \\boldsymbol Y) \n",
    "$$\n",
    "\n",
    "* Make the **backward pass** from $\\boldsymbol X_{L-1}$ to $\\boldsymbol X_1$ and calculate $\\nabla_{\\boldsymbol X_i}\\mathcal L$, $\\nabla_{\\boldsymbol W_i} \\mathcal L$, $\\nabla_{\\boldsymbol B_i}\\mathcal L$ on each step\n",
    "\n",
    "## Backprop through one layer\n",
    "\n",
    "```{figure} backprop.png\n",
    ":align: center\n",
    "```\n",
    "\n",
    "All you need is... **chain rule**!\n",
    "\n",
    "### Backprop through activation layer\n",
    "\n",
    "Suppose that we know the **upstream** gradient $\\nabla_{\\boldsymbol Z}\\mathcal L(\\boldsymbol Z)$. The goal is to derive the formula for the **downstream** gradient $\\nabla_{\\boldsymbol V}\\tilde{\\mathcal L}(\\boldsymbol V)$ where $\\tilde{\\mathcal L}(\\boldsymbol V)=\\mathcal L(\\psi(\\boldsymbol V))$. According to matrix calculus rules\n",
    "\n",
    "$$\n",
    "    d\\tilde{\\mathcal L}(\\boldsymbol V) = \\mathrm{tr}\\big(\\nabla \\tilde{\\mathcal L}(\\boldsymbol V)^\\mathsf{T} d\\boldsymbol V\\big) = \\mathrm{tr}\\big(\\nabla \\mathcal L(\\boldsymbol Z)^\\mathsf{T} d\\boldsymbol Z\\big).\n",
    "$$\n",
    "\n",
    "\n",
    "From {eq}`elementwise-diff` we know that $d \\boldsymbol Z = d\\psi(\\boldsymbol V) = \\psi'(\\boldsymbol V) \\odot d\\boldsymbol V =  d\\boldsymbol V \\odot \\psi'(\\boldsymbol V)$. Hence,\n",
    "\n",
    "$$\n",
    "\\mathrm{tr}\\big(\\nabla \\mathcal L(\\boldsymbol Z)^\\mathsf{T} d\\boldsymbol Z\\big) = \n",
    "\\mathrm{tr}\\big(\\nabla \\mathcal L(\\boldsymbol Z)^\\mathsf{T} (\\psi'(\\boldsymbol V) \\odot d\\boldsymbol V)\\big)\n",
    "$$\n",
    "\n",
    "```{admonition} Exercise\n",
    ":class: important\n",
    "Let $\\boldsymbol A, \\boldsymbol B, \\boldsymbol C \\in \\mathbb R^{m\\times n}$. Show that \n",
    "\n",
    "$$\n",
    "\\mathrm{tr}\\big(\\boldsymbol A^\\mathsf{T} (\\boldsymbol B \\odot \\boldsymbol C)\\big) = \\mathrm{tr}\\big((\\boldsymbol A \\odot \\boldsymbol B)^\\mathsf{T} \\boldsymbol C\\big)\n",
    "$$\n",
    "\n",
    "```\n",
    "\n",
    "From this exercise we obtain\n",
    "\n",
    "$$\n",
    "    d\\tilde{\\mathcal L}(\\boldsymbol V) = \\mathrm{tr}\\big(\\big(\\nabla \\mathcal L(\\boldsymbol Z) \\odot \\psi'(\\boldsymbol V)\\big)^\\mathsf{T} d\\boldsymbol V\\big),\n",
    "$$\n",
    "\n",
    "and $\\nabla_{\\boldsymbol V}\\tilde{\\mathcal L}(\\boldsymbol V) = \\nabla \\mathcal L(\\boldsymbol Z) \\odot \\psi'(\\boldsymbol V)$.\n",
    "\n",
    "### Backprop through linear layer\n",
    "\n",
    "Now our upstream gradient is $\\nabla_{\\boldsymbol V}\\mathcal L(\\boldsymbol V)$, and we want to find the downstream gradient $\\nabla_{\\boldsymbol X}\\tilde{\\mathcal L}(\\boldsymbol X, \\boldsymbol W, \\boldsymbol B)$ where $\\tilde{\\mathcal L}(\\boldsymbol X, \\boldsymbol W, \\boldsymbol B)=\\mathcal L(\\boldsymbol{XW} + \\boldsymbol B)$. Moreover, we also need formulas for $\\nabla_{\\boldsymbol W}\\tilde{\\mathcal L}(\\boldsymbol X, \\boldsymbol W, \\boldsymbol B)$ and $\\nabla_{\\boldsymbol B}\\tilde{\\mathcal L}(\\boldsymbol X, \\boldsymbol W, \\boldsymbol B)$ in order to make the gradient step.\n",
    "\n",
    "Once again use chain rule and simple {ref}`facts <linear-diff>` from calculus:\n",
    "\n",
    "$$\n",
    "    d\\tilde{\\mathcal L}(\\boldsymbol X) = \\mathrm{tr}\\big(\\nabla \\mathcal L(\\boldsymbol V)^\\mathsf{T} d\\boldsymbol V\\big) = \\mathrm{tr}\\big(\\nabla \\mathcal L(\\boldsymbol V)^\\mathsf{T} d\\boldsymbol X \\cdot \\boldsymbol W\\big) = \\mathrm{tr}\\big(\\boldsymbol W\\cdot\\nabla \\mathcal L(\\boldsymbol V)^\\mathsf{T} d\\boldsymbol X \\big),\n",
    "$$\n",
    "\n",
    "$$\n",
    "    d\\tilde{\\mathcal L}(\\boldsymbol W) = \\mathrm{tr}\\big(\\nabla \\mathcal L(\\boldsymbol V)^\\mathsf{T} d\\boldsymbol V\\big) = \\mathrm{tr}\\big(\\nabla \\mathcal L(\\boldsymbol V)^\\mathsf{T} \\boldsymbol X d\\boldsymbol W\\big),\n",
    "$$\n",
    "\n",
    "$$\n",
    "    d\\tilde{\\mathcal L}(\\boldsymbol B) = \\mathrm{tr}\\big(\\nabla \\mathcal L(\\boldsymbol V)^\\mathsf{T} d\\boldsymbol V\\big) = \\mathrm{tr}\\big(\\nabla \\mathcal L(\\boldsymbol V)^\\mathsf{T} d\\boldsymbol B\\big)\n",
    "$$\n",
    "\n",
    "Hence, \n",
    "\n",
    "$$\n",
    "    \\nabla_{\\boldsymbol X}\\tilde{\\mathcal L} = \\nabla\\mathcal L(\\boldsymbol V) \\boldsymbol W^\\mathsf{T}, \\quad\n",
    "    \\nabla_{\\boldsymbol W}\\tilde{\\mathcal L} = \\boldsymbol X^\\mathsf{T} \\nabla\\mathcal L(\\boldsymbol V), \\quad\n",
    "    \\nabla_{\\boldsymbol B}\\tilde{\\mathcal L} = \\nabla \\mathcal L(\\boldsymbol V).\n",
    "$$\n",
    "\n",
    "```{admonition} Problem with bias\n",
    ":class: dropdown\n",
    "Usually bias is a vector, not matrix. If $\\boldsymbol X \\in \\mathbb R^{B\\times m}$, $\\boldsymbol W \\in \\mathbb R^{m\\times n}$ then bias is $\\boldsymbol b \\in \\mathbb R^{n}$, and linear layer looks like\n",
    "\n",
    "$$\n",
    "    \\boldsymbol{XW} + \\boldsymbol B, \\quad \\boldsymbol B = \\begin{pmatrix}\n",
    "        \\boldsymbol b^\\mathsf{T} \\\\\n",
    "        \\vdots \\\\\n",
    "        \\boldsymbol b^\\mathsf{T}\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Therefore, we want to find $\\nabla_{\\boldsymbol b}\\tilde{\\mathcal L}$, which is also not matrix, but vector. It  can be found from the equality\n",
    "\n",
    "$$\n",
    "    d\\tilde{\\mathcal L}(\\boldsymbol b) = \\nabla_{\\boldsymbol b} \\tilde{\\mathcal L}^\\mathsf{T} d\\boldsymbol b.\n",
    "$$\n",
    "\n",
    "Denote\n",
    "\n",
    "$$\n",
    "    \\nabla \\mathcal L(\\boldsymbol V) = \\boldsymbol A = \\begin{pmatrix}\n",
    "        \\boldsymbol a_1^\\mathsf{T} \\\\\n",
    "        \\vdots \\\\\n",
    "        \\boldsymbol a_B^\\mathsf{T}\n",
    "    \\end{pmatrix} \\in \\mathbb R^{B\\times n},\n",
    "$$\n",
    "\n",
    "then\n",
    "\n",
    "$$\n",
    "    d\\tilde{\\mathcal L}(\\boldsymbol b) = \\mathrm{tr}\\big(\\boldsymbol A^\\mathsf{T} d\\boldsymbol B\\big) =\n",
    "    = \\mathrm{tr}\\left([\\boldsymbol a_1 \\ldots \\boldsymbol a_B]  \\begin{pmatrix}d\\boldsymbol b^\\mathsf{T} \\\\\n",
    "        \\vdots \\\\\n",
    "        d\\boldsymbol b^\\mathsf{T}\n",
    "    \\end{pmatrix}\\right) = \\sum\\limits_{i=1}^B \\mathrm{tr}(\\boldsymbol a_i d\\boldsymbol b^\\mathsf{T})\n",
    "$$\n",
    "\n",
    "Since $\\mathrm{tr}(\\boldsymbol a_i d\\boldsymbol b^\\mathsf{T}) = \\mathrm{tr}(d\\boldsymbol b^\\mathsf{T}\\boldsymbol a_i) = d\\boldsymbol b^\\mathsf{T}\\boldsymbol a_i = \\boldsymbol a_i^\\mathsf{T}d\\boldsymbol b$, we finally get\n",
    "\n",
    "$$\n",
    "    d\\tilde{\\mathcal L}(\\boldsymbol b) = \\sum\\limits_{i=1}^B \\boldsymbol a_i^\\mathsf{T}d\\boldsymbol b = \n",
    "    \\Big(\\underbrace{\\sum\\limits_{i=1}^B \\boldsymbol a_i}_{\\nabla\\tilde{\\mathcal L}(\\boldsymbol b)}\\Big)^\\mathsf{T} d\\boldsymbol b.\n",
    "$$\n",
    "\n",
    "Hence, the downstream gradient $\\nabla\\tilde{\\mathcal L}(\\boldsymbol b)$ is equal to the sum of the rows of the upstream gradient $\\nabla \\mathcal L(\\boldsymbol V)$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop through loss function\n",
    "\n",
    "### Regression\n",
    "\n",
    "The most common choice for (multiple) regression is MSE: if target is $\\boldsymbol Y \\in \\mathbb R^{B\\times m}$, model output is $\\widehat{\\boldsymbol Y} \\in \\mathbb R^{B\\times m}$, then\n",
    "\n",
    "$$\n",
    "    \\mathcal L(\\boldsymbol Y, \\widehat{\\boldsymbol Y}) = \\frac 1{Bm} \\Vert \\boldsymbol Y - \\widehat{\\boldsymbol Y}\\Vert_F^2 = \\frac 1{Bm}\\sum\\limits_{i=1}^B\\sum\\limits_{j=1}^m (Y_{ij} - \\widehat Y_{ij})^2.\n",
    "$$\n",
    "\n",
    "Since $\\Vert \\boldsymbol Y - \\widehat{\\boldsymbol Y}\\Vert_F^2 = \\mathrm{tr}\\big((\\boldsymbol Y - \\widehat{\\boldsymbol Y})^\\mathsf{T}(\\boldsymbol Y - \\widehat{\\boldsymbol Y})\\big)$,\n",
    "\n",
    "$$\n",
    "    d\\mathcal L(\\boldsymbol Y, \\widehat{\\boldsymbol Y}) = \\frac2{Bm} \\mathrm{tr}\\big((\\widehat{\\boldsymbol Y}-\\boldsymbol Y )^\\mathsf{T}\\widehat{\\boldsymbol Y}\\big).\n",
    "$$\n",
    "\n",
    "Consequently,\n",
    "\n",
    "$$\n",
    "    \\nabla_{\\widehat{\\boldsymbol Y}} \\mathcal L(\\boldsymbol Y, \\widehat{\\boldsymbol Y}) = \\frac2{Bm} (\\widehat{\\boldsymbol Y}-\\boldsymbol Y).\n",
    "$$\n",
    "\n",
    "Sometimes they use sum of squares instead of MSE, then the multiplier $\\frac 1{Bm}$ is omitted.\n",
    "\n",
    "### Classification\n",
    "\n",
    "The usual choice of loss here is **cross-entropy loss**:\n",
    "\n",
    "```{math}\n",
    ":label: cross-entropy-mlp\n",
    "\t\\mathcal L(\\boldsymbol Y, \\boldsymbol {\\widehat Y}) = -\\frac 1N \\sum\\limits_{i=1}^N\\sum\\limits_{j=1}^K Y_{ij} \\log \\widehat Y_{ij} = -\\frac 1N \\mathrm{tr}\\big(\\boldsymbol Y^T\\log\\boldsymbol {\\widehat Y}\\big).\n",
    "```\n",
    "\n",
    "The target matrix $\\boldsymbol Y \\in \\mathbb R^{B\\times K}$ consists of one-hot encoded rows, each of which contains $1$ one and $K-1$ zeros. Each row of the prediction matrix $\\boldsymbol {\\widehat Y} \\in \\mathbb R^{B\\times K}$ must be a valid probability distribution, which is achived by applying Softmax:\n",
    "\n",
    "```{math}\n",
    ":label: y-hat-softmax\n",
    "\t\\widehat Y_{ij} = \\mathrm{Softmax}(\\boldsymbol X^{\\mathrm{out}})_{ij} = \\frac{e^{X^{\\mathrm{out}}_{ij}}}{\\sum\\limits_{k=1}^K e^{X^{\\mathrm{out}}_{ik}}}.\n",
    "```\n",
    "\n",
    "<!--\n",
    "**Упражнение.** Запишите формулу для Softmax'а в матричном виде.\n",
    "\n",
    "<details>\n",
    "<summary markdown=\"span\">Ответ (не открывайте сразу, сначала подумайте самостоятельно) </summary>\n",
    "<div>\n",
    "С числителем всё просто: он равен $$\\exp(\\boldsymbol X^{\\mathrm{out}})$$ (экспонента применяется поэлементно). А что со знаменателем? При любом $$j$$ там стоит скалярное произведение $$i$$-й строки матрицы $$\\exp(\\boldsymbol X^{\\mathrm{out}})$$ с вектором из сплошных единиц. В матричном виде всё вместе можно записать как\n",
    "\n",
    "$$\n",
    "\\mathrm{Softmax}(\\boldsymbol X^{\\mathrm{out}}) = \\exp(\\boldsymbol X^{\\mathrm{out}}) \\oslash (\\exp(\\boldsymbol X^{\\mathrm{out}}) \\cdot \\boldsymbol 1_{K\\times K}),\n",
    "$$\n",
    "\n",
    "где $$\\boldsymbol 1_{K\\times K}$$ — матрица размера $$K\\times K$$ из одних единиц, а $$\\oslash$$ означает поэлементное деление матриц.\n",
    "</div>\n",
    "</details>\n",
    "-->\n",
    "\n",
    "Формально Softmax и кросс-энтропия — это два подряд идущих слоя, однако, на практике в целях повышения вычислительной стабильности их часто объединяют в один (например, в [pytorch](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html) Softmax и поэлементное взятие логарифма объединены в один слой LogSoftmax). \n",
    "\n",
    "Plugging {eq}`y-hat-softmax` into {eq}`cross-entropy-mlp`, we obtain\n",
    "\n",
    "$$\n",
    "\t\\mathcal L\\big(\\boldsymbol Y, \\mathrm{Softmax}(\\boldsymbol X^{\\mathrm{out}})\\big) = -\\frac 1N \\sum\\limits_{i=1}^N\\sum\\limits_{j=1}^K Y_{ij} \\Big(X^{\\mathrm{out}}_{ij} - \\log\\Big(\\sum\\limits_{k=1}^K e^{X^{\\mathrm{out}}_{ik}}\\Big)\\Big).\n",
    "$$\n",
    "\n",
    "Now differentiate it with respect to $X_{mn}$:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial \\mathcal L}{\\partial X_{mn}^{\\mathrm{out}}} = -\\frac 1N \\sum\\limits_{i=1}^N\\sum\\limits_{j=1}^K Y_{ij} \\bigg(\\delta_{im}\\delta_{jn}  - \\frac{e^{X^{\\mathrm{out}}_{in}}\\delta_{im}}{\\sum_{k=1}^K e^{X^{\\mathrm{out}}_{ik}}}\\bigg),\n",
    "$$\n",
    "\n",
    "where $\\delta_{ij}$ is [Kronecker delta](https://en.wikipedia.org/wiki/Kronecker_delta). Since target matrix $\\boldsymbol Y$ enjoys the property $\\sum\\limits_{j=1}^K Y_{ij} = 1$ for all $i=1,\\ldots, B$, we get\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial \\mathcal L}{\\partial X_{mn}^{\\mathrm{out}}} = -\\frac 1N Y_{mn} + \\frac 1N \\sum\\limits_{j=1}^K Y_{mj} \\frac{e^{X_{mn}^{\\mathrm{out}}}}{\\sum\\limits_{k=1}^K e^{X^{\\mathrm{out}}_{mk}}} = \\frac 1N\\big(\\mathrm{Softmax}(\\boldsymbol X^{\\mathrm{out}})_{mn} - Y_{mn}\\big).\n",
    "$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\n",
    "\t\\nabla_{\\boldsymbol X^{\\mathrm{out}}}\\mathcal L(\\boldsymbol Y, \\widehat{\\boldsymbol Y}) = \\frac 1N\\big(\\widehat{\\boldsymbol Y} - \\boldsymbol Y\\big)= \\frac 1N\\big(\\mathrm{Softmax}(\\boldsymbol X^{\\mathrm{out}}) - \\boldsymbol Y\\big).\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
