{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"display:none\" id=\"gru_q9\">W3sicXVlc3Rpb24iOiAiV2h5IGFyZSBMU1RNIG1vZGVscyBvZnRlbiBwcmVmZXJyZWQgd2hlbiB3b3JraW5nIHdpdGggbGFyZ2VyIGRhdGFzZXRzPyIsICJ0eXBlIjogIm11bHRpcGxlX2Nob2ljZSIsICJhbnN3ZXJzIjogW3siYW5zd2VyIjogIlRoZXkgaGF2ZSBhIGNvbXBsZXggYXJjaGl0ZWN0dXJlLCB3aGljaCBpcyBtb3JlIHN1aXRhYmxlIGZvciBsYXJnZSBkYXRhc2V0cy4iLCAiY29ycmVjdCI6IGZhbHNlLCAiZmVlZGJhY2siOiAiRmFsc2UuIn0sIHsiYW5zd2VyIjogIlRoZXkgcmVxdWlyZSBsb25nZXIgdHJhaW5pbmcgdGltZSBidXQgcHJvdmlkZSBtb3JlIGFjY3VyYXRlIHJlc3VsdHMgb24gbGFyZ2VyIGRhdGFzZXRzLiIsICJjb3JyZWN0IjogZmFsc2UsICJmZWVkYmFjayI6ICJGYWxzZS4ifSwgeyJhbnN3ZXIiOiAiVGhlaXIgbWVtb3J5IGNlbGxzIGFsbG93IHRoZW0gdG8gY2FwdHVyZSBsb25nLXJhbmdlIGRlcGVuZGVuY2llcyBpbiBleHRlbnNpdmUgZGF0YS4iLCAiY29ycmVjdCI6IHRydWUsICJmZWVkYmFjayI6ICJMU1RNIG1vZGVscyBhcmUgb2Z0ZW4gcHJlZmVycmVkIGZvciBsYXJnZXIgZGF0YXNldHMgYmVjYXVzZSB0aGVpciBtZW1vcnkgY2VsbHMgYWxsb3cgdGhlbSB0byBjYXB0dXJlIGxvbmctcmFuZ2UgZGVwZW5kZW5jaWVzIGluIGV4dGVuc2l2ZSBkYXRhLiJ9LCB7ImFuc3dlciI6ICIgVGhleSBhcmUgY29tcHV0YXRpb25hbGx5IGVmZmljaWVudCwgbWFraW5nIHRoZW0gcHJhY3RpY2FsIGZvciBsYXJnZSBkYXRhc2V0cy4iLCAiY29ycmVjdCI6IGZhbHNlLCAiZmVlZGJhY2siOiAiRmFsc2UuIn1dfV0=</span>\n",
    "<span style=\"display:none\" id=\"gru_q8\">W3sicXVlc3Rpb24iOiAiV2h5IGFyZSBHYXRlZCBSZWN1cnJlbnQgVW5pdHMgKEdSVXMpIG9mdGVuIGNvbnNpZGVyZWQgYWR2YW50YWdlb3VzIHdoZW4gd29ya2luZyB3aXRoIHNtYWxsIGRhdGFzZXRzPyIsICJ0eXBlIjogIm11bHRpcGxlX2Nob2ljZSIsICJhbnN3ZXJzIjogW3siYW5zd2VyIjogIlRoZXkgaGF2ZSBhIG1vcmUgY29tcGxleCBhcmNoaXRlY3R1cmUsIHdoaWNoIGlzIGJldHRlciBzdWl0ZWQgZm9yIHNtYWxsIGRhdGFzZXRzLiIsICJjb3JyZWN0IjogZmFsc2UsICJmZWVkYmFjayI6ICJGYWxzZS4ifSwgeyJhbnN3ZXIiOiAiVGhleSBjb252ZXJnZSBzbG93ZXIgYnV0IHByb3ZpZGUgbW9yZSBhY2N1cmF0ZSByZXN1bHRzIG9uIHNtYWxsIGRhdGFzZXRzLiIsICJjb3JyZWN0IjogZmFsc2UsICJmZWVkYmFjayI6ICJGYWxzZS4ifSwgeyJhbnN3ZXIiOiAiVGhlaXIgc2ltcGxpY2l0eSBhbmQgZmFzdGVyIGNvbnZlcmdlbmNlIG1ha2UgdGhlbSBlZmZlY3RpdmUgaW4gY2FwdHVyaW5nIHBhdHRlcm5zIHdpdGggbGltaXRlZCBkYXRhLiIsICJjb3JyZWN0IjogdHJ1ZSwgImZlZWRiYWNrIjogIlRoYXQncyBjb3JyZWN0ISBUaGUgc2ltcGxpY2l0eSBhbmQgZmFzdGVyIGNvbnZlcmdlbmNlIG9mIEdhdGVkIFJlY3VycmVudCBVbml0cyAoR1JVcykgbWFrZSB0aGVtIGVmZmVjdGl2ZSBpbiBjYXB0dXJpbmcgcGF0dGVybnMgd2l0aCBsaW1pdGVkIGRhdGEuIEdvb2Qgam9iISBJIn0sIHsiYW5zd2VyIjogIlRoZXkgcmVxdWlyZSBtb3JlIGNvbXB1dGF0aW9uYWwgcmVzb3VyY2VzLCB3aGljaCBpcyBiZW5lZmljaWFsIGZvciBzbWFsbCBkYXRhc2V0cy4iLCAiY29ycmVjdCI6IGZhbHNlLCAiZmVlZGJhY2siOiAiRmFsc2UuIn1dfV0=</span>\n",
    "<span style=\"display:none\" id=\"gru_q7\">W3sicXVlc3Rpb24iOiAiUk5OIHNoYXJlcyAuLi4uIHdlaWdodHMgd2l0aGluZyAuLi4uIG9mIHRoZSBuZXR3b3JrIiwgInR5cGUiOiAibXVsdGlwbGVfY2hvaWNlIiwgImFuc3dlcnMiOiBbeyJhbnN3ZXIiOiAibm8gd2VpZ2h0cywgZmlyc3QgYW5kIHNlY29uZCBsYXllcnMiLCAiY29ycmVjdCI6IGZhbHNlLCAiZmVlZGJhY2siOiAiRmFsc2UuIn0sIHsiYW5zd2VyIjogInNhbWUsIGZpcnN0IDUgbGF5ZXJzIiwgImNvcnJlY3QiOiBmYWxzZSwgImZlZWRiYWNrIjogIkZhbHNlLiJ9LCB7ImFuc3dlciI6ICJzYW1lLCBldmVyeSBzaW5nbGUgbGF5ZXIiLCAiY29ycmVjdCI6IHRydWUsICJmZWVkYmFjayI6ICJDb3JyZWN0LiJ9LCB7ImFuc3dlciI6ICJubyB3ZWlnaHRzLCBlYWNoIGxheWVyIiwgImNvcnJlY3QiOiBmYWxzZSwgImZlZWRiYWNrIjogIkZhbHNlLiJ9XX1d</span>\n",
    "<span style=\"display:none\" id=\"gru_q6\">W3sicXVlc3Rpb24iOiAiTm93LCBsZXQncyBjb25zaWRlciB0aGUgYm9vayByZXZpZXcgZXhhbXBsZSBhZ2Fpbi4gVGhpcyB0aW1lLCB0aGUgbW9zdCBpbXBvcnRhbnQgc3R1ZmYgaXMgcmlnaHQgYXQgdGhlIGJlZ2lubmluZyBvZiB0aGUgdGV4dC4gVGhlIG1vZGVsIGNhbiBsZWFybiB0byBtYWtlIHRoZSB2ZWN0b3IgJFpfdCQgYWxtb3N0IGVxdWFsIHRvIDEuIFdoYXQgZG9lcyBpdCBtZWFuPyIsICJ0eXBlIjogIm1hbnlfY2hvaWNlIiwgImFuc3dlcnMiOiBbeyJhbnN3ZXIiOiAiUmVzZXQgZ2F0ZSB3b250IGFjdGl2YXRlLiIsICJjb3JyZWN0IjogZmFsc2UsICJmZWVkYmFjayI6ICJXYXN0ZWQhIn0sIHsiYW5zd2VyIjogIk1vc3QgdXNlZnVsbCBpbmZvcm1hdGlvbiBhdCB0aGUgc3RhcnQuIiwgImNvcnJlY3QiOiB0cnVlLCAiZmVlZGJhY2siOiAiSXQgbWVhbnMgaXQgd2lsbCBob2xkIG9udG8gbW9zdCBvZiB0aGUgZWFybGllciBpbmZvcm1hdGlvbi4gQmVjYXVzZSAkWl90JCBpcyBuZWFybHkgMSBhdCB0aGlzIHN0ZXAsICQxLVpfdCQgd2lsbCBiZSBjbG9zZSB0byAwLCBhbmQgdGhpcyB3aWxsIG1ha2UgdGhlIG1vZGVsIGlnbm9yZSBhIGJpZyBwYXJ0IG9mIHRoZSBjdXJyZW50IGNvbnRlbnQsIGxpa2UgdGhlIGxhc3QgcGFydCBvZiB0aGUgcmV2aWV3IHRoYXQgdGFsa3MgYWJvdXQgdGhlIGJvb2sncyBwbG90LCBiZWNhdXNlIGl0J3Mgbm90IGltcG9ydGFudCBmb3Igb3VyIHByZWRpY3Rpb24uIn0sIHsiYW5zd2VyIjogIlRydWUgdmFsdWUgb2YgdGhlIG91dHB1dCB3aWxsIGJlIGRldGVybWluZWQgYnkgZW5kIG9mIHRoZSByZXZpZXcuIiwgImNvcnJlY3QiOiBmYWxzZSwgImZlZWRiYWNrIjogIkdvb2QgTHVjayBOZXh0IFRpbWUhIn0sIHsiYW5zd2VyIjogIkl0IGRvZXMgbm90IG1lYW4gYW55dGhpbmciLCAiY29ycmVjdCI6IGZhbHNlLCAiZmVlZGJhY2siOiAiPSgifV19XQ==</span>\n",
    "<span style=\"display:none\" id=\"gru_q4\">W3sicXVlc3Rpb24iOiAibiBhIEdhdGVkIFJlY3VycmVudCBVbml0IChHUlUpIG1vZGVsLCBpZiB5b3Ugc2V0IHRoZSByZXNldCBnYXRlIHRvIGFsbCAxJ3MgYW5kIHRoZSB1cGRhdGUgZ2F0ZSB0byBhbGwgMCdzLCB3aGF0IGJlaGF2aW9yIGRvZXMgdGhlIEdSVSBleGhpYml0PyIsICJ0eXBlIjogIm1hbnlfY2hvaWNlIiwgImFuc3dlcnMiOiBbeyJhbnN3ZXIiOiAiVGhlIEdSVSBvcGVyYXRlcyBhcyBhIG1vcmUgY29tcGxleCB2YXJpYW50IG9mIGFuIExTVE0uIiwgImNvcnJlY3QiOiBmYWxzZSwgImZlZWRiYWNrIjogIldhc3RlZCEifSwgeyJhbnN3ZXIiOiAiVGhlIEdSVSBiZWhhdmVzIGxpa2UgYSBwbGFpbiBSTk4sIHJldGFpbmluZyB0aGUgZW50aXJlIHByZXZpb3VzIHN0YXRlIGFzIGlzLiIsICJjb3JyZWN0IjogdHJ1ZSwgImZlZWRiYWNrIjogIldoZW4geW91IHNldCB0aGUgcmVzZXQgZ2F0ZSB0byBhbGwgMSdzIGFuZCB0aGUgdXBkYXRlIGdhdGUgdG8gYWxsIDAncyBpbiBhIEdSVSwgeW91IGVmZmVjdGl2ZWx5IGRpc2FibGUgdGhlIGdhdGluZyBtZWNoYW5pc21zLCBjYXVzaW5nIHRoZSBHUlUgdG8gYmVoYXZlIGxpa2UgYSBwbGFpbiBSTk4uIFRoaXMgbWVhbnMgdGhhdCB0aGUgZW50aXJlIHByZXZpb3VzIHN0YXRlIGlzIHJldGFpbmVkIHdpdGhvdXQgc2VsZWN0aXZlIHJlc2V0dGluZyBvciB1cGRhdGluZy4ifSwgeyJhbnN3ZXIiOiAiVGhlIEdSVSBleGhpYml0cyBpdHMgdXN1YWwgYmVoYXZpb3IsIHNlbGVjdGl2ZWx5IHJldGFpbmluZyBvciB1cGRhdGluZyBpbmZvcm1hdGlvbi4iLCAiY29ycmVjdCI6IGZhbHNlLCAiZmVlZGJhY2siOiAiR29vZCBMdWNrIE5leHQgVGltZSEifSwgeyJhbnN3ZXIiOiAiVGhlIEdSVSBiZWNvbWVzIHVuc3RhYmxlIGFuZCBmYWlscyB0byBjb252ZXJnZSBkdXJpbmcgdHJhaW5pbmcuIiwgImNvcnJlY3QiOiBmYWxzZSwgImZlZWRiYWNrIjogIj0oIn1dfV0=</span>\n",
    "<span style=\"display:none\" id=\"gru_q5\">W3sicXVlc3Rpb24iOiAiSW1hZ2luZSB3ZSB3YW50IHRvIGZpZ3VyZSBvdXQgaWYgc29tZW9uZSBsaWtlZCBvciBkaWQgbm90IGxpa2UgYSBib29rIGJhc2VkIG9uIGEgcmV2aWV3IHRoZXkgd3JvdGUuIFRoZSByZXZpZXcgc3RhcnRzIGJ5IHRhbGtpbmcgYWJvdXQgdGhlIGJvb2sgYW5kIHRoZW4gZ2l2ZXMgYW4gb3BpbmlvbiBhdCB0aGUgZW5kLiBXaGF0IHBhcnQgb2YgdGhlIHJldmlldyBzaG91bGQgR1JVIHRha2UgaW50byBhY2NvdW50PyIsICJ0eXBlIjogIm1hbnlfY2hvaWNlIiwgImFuc3dlcnMiOiBbeyJhbnN3ZXIiOiAiTWlkZGxlIHBhcnQgaXMgdGhlIG1vc3QgaW1wb3J0YW50LiIsICJjb3JyZWN0IjogZmFsc2UsICJmZWVkYmFjayI6ICJXYXN0ZWQhIn0sIHsiYW5zd2VyIjogIk9ubHkgdGhlIGxhc3QgcGFydCBtYXR0ZXIiLCAiY29ycmVjdCI6IHRydWUsICJmZWVkYmFjayI6ICJUbyBkZWNpZGUgaWYgdGhleSBsaWtlZCBpdCBvciBub3QsIHdlIG9ubHkgbmVlZCB0byB1c2UgYXQgdGhlIGxhc3QgcGFydCB3aGVyZSB0aGV5IHNoYXJlIHRoZWlyIHRob3VnaHRzLiBTbywgYXMgb3VyIG5ldXJhbCBuZXR3b3JrIHJlYWRzIHRocm91Z2ggdGhlIHRleHQsIGl0IHdpbGwgc3RhcnQgcGF5aW5nIGxlc3MgYXR0ZW50aW9uIHRvIHRoZSBlYXJsaWVyIHN0dWZmIGFuZCBwYXkgbW9yZSBhdHRlbnRpb24gdG8gdGhlIGxhc3Qgc2VudGVuY2VzLiBUaGlzIGhlbHBzIHRoZSBuZXR3b3JrIG1ha2UgYSBiZXR0ZXIgZGVjaXNpb24gYWJvdXQgd2hldGhlciB0aGUgcGVyc29uIGxpa2VkIHRoZSBib29rIG9yIG5vdC4ifSwgeyJhbnN3ZXIiOiAiVGhlIGJlZ2dpbmluZyBob2xkcyB0aGUgbW9zdCB2YWx1ZS4iLCAiY29ycmVjdCI6IGZhbHNlLCAiZmVlZGJhY2siOiAiR29vZCBMdWNrIE5leHQgVGltZSEifSwgeyJhbnN3ZXIiOiAiR1JVIGNhbnQgc29sdmUgdGhpcyBwcm9ibGVtLiIsICJjb3JyZWN0IjogZmFsc2UsICJmZWVkYmFjayI6ICI9KCJ9XX1d</span>\n",
    "<span style=\"display:none\" id=\"gru_q1\">W3sicXVlc3Rpb24iOiAiV2hhdCBkb2VzIHRoZSB0ZXJtICd2YW5pc2hpbmcgZ3JhZGllbnQnIHJlZmVyIHRvIGluIHRoZSBjb250ZXh0IG9mIFJOTj8iLCAidHlwZSI6ICJtYW55X2Nob2ljZSIsICJhbnN3ZXJzIjogW3siYW5zd2VyIjogIkEgc2l0dWF0aW9uIHdoZXJlIGdyYWRpZW50cyBhcmUgdG9vIGxhcmdlLCBjYXVzaW5nIGluc3RhYmlsaXR5IGR1cmluZyB0cmFpbmluZy4iLCAiY29ycmVjdCI6IGZhbHNlLCAiZmVlZGJhY2siOiAiV2FzdGVkISJ9LCB7ImFuc3dlciI6ICJBIHNpdHVhdGlvbiB3aGVyZSBncmFkaWVudHMgYmVjb21lIGV4dHJlbWVseSBzbWFsbCBhcyB0aGV5IGFyZSBwcm9wYWdhdGVkIGJhY2t3YXJkIHRocm91Z2ggbmV0d29yayBsYXllcnMgZHVyaW5nIHRyYWluaW5nLiIsICJjb3JyZWN0IjogdHJ1ZSwgImZlZWRiYWNrIjogIk5pY2Ugam9iISJ9LCB7ImFuc3dlciI6ICJBIHNpdHVhdGlvbiB3aGVyZSBncmFkaWVudHMgcmVtYWluIGNvbnN0YW50IHRocm91Z2hvdXQgdGhlIHRyYWluaW5nIHByb2Nlc3MuIiwgImNvcnJlY3QiOiBmYWxzZSwgImZlZWRiYWNrIjogIkdvb2QgTHVjayBOZXh0IFRpbWUhIn0sIHsiYW5zd2VyIjogIkEgc2l0dWF0aW9uIHdoZXJlIGdyYWRpZW50cyBhcmUgY2FsY3VsYXRlZCBvbmx5IGZvciB0aGUgaW5wdXQgbGF5ZXIgb2YgYSBuZXVyYWwgbmV0d29yay4iLCAiY29ycmVjdCI6IGZhbHNlLCAiZmVlZGJhY2siOiAiPSgifV19XQ==</span>\n",
    "<span style=\"display:none\" id=\"gru_q2\">W3sicXVlc3Rpb24iOiAiV2hhdCBhcmUgdGhlIG5hbWVzIG9mIHRoZSBnYXRlcyB0aGF0IGFyZSB1c2VkIGluIEdSVSB0byBzb2x2ZSAndmFuaXNoaW5nIGdyYWRpZW50JyBwcm9ibGVtPyIsICJ0eXBlIjogIm11bHRpcGxlX2Nob2ljZSIsICJhbnN3ZXJzIjogW3siYW5zd2VyIjogIk1lbW9yeSBHYXRlIiwgImNvcnJlY3QiOiBmYWxzZX0sIHsiYW5zd2VyIjogIlJlc2V0IEdhdGUiLCAiY29ycmVjdCI6IHRydWV9LCB7ImFuc3dlciI6ICJKdW1wIEdhdGUiLCAiY29ycmVjdCI6IGZhbHNlfSwgeyJhbnN3ZXIiOiAiVXBkYXRlIEdhdGUiLCAiY29ycmVjdCI6IHRydWV9XX1d</span>\n",
    "<span style=\"display:none\" id=\"gru_q3\">W3sicXVlc3Rpb24iOiAiV2h5IGRvZXMgdGhlIEdSVSAoR2F0ZWQgUmVjdXJyZW50IFVuaXQpIG9mdGVuIGV4aGliaXQgYSBmYXN0ZXIgY29udmVyZ2VuY2UgcmF0ZSBjb21wYXJlZCB0byBMU1RNIChMb25nIFNob3J0LVRlcm0gTWVtb3J5KSBpbiByZWN1cnJlbnQgbmV1cmFsIG5ldHdvcmtzPyIsICJ0eXBlIjogIm1hbnlfY2hvaWNlIiwgImFuc3dlcnMiOiBbeyJhbnN3ZXIiOiAiR1JVIGhhcyBhIGxhcmdlciBudW1iZXIgb2YgaGlkZGVuIHVuaXRzLiIsICJjb3JyZWN0IjogZmFsc2UsICJmZWVkYmFjayI6ICJXYXN0ZWQhIn0sIHsiYW5zd2VyIjogIkdSVSBoYXMgZmV3ZXIgZ2F0ZXMgdG8gY29tcHV0ZSBkdXJpbmcgdHJhaW5pbmcuIiwgImNvcnJlY3QiOiB0cnVlLCAiZmVlZGJhY2siOiAiR1JVIGhhcyB0d28gZ2F0ZXMgKFJlc2V0IEdhdGUgYW5kIFVwZGF0ZSBHYXRlKSB0byBjb21wdXRlIGR1cmluZyB0cmFpbmluZywgd2hlcmVhcyBMU1RNIGhhcyB0aHJlZSBnYXRlcyAoRm9yZ2V0IEdhdGUsIElucHV0IEdhdGUsIGFuZCBPdXRwdXQgR2F0ZSkuIFRoZSByZWR1Y2VkIG51bWJlciBvZiBnYXRlcyBpbiBHUlUgY29udHJpYnV0ZXMgdG8gaXRzIGZhc3RlciBjb252ZXJnZW5jZSByYXRlLCBhcyBpdCBpbnZvbHZlcyBmZXdlciBjb21wdXRhdGlvbnMgZHVyaW5nIHRyYWluaW5nLiJ9LCB7ImFuc3dlciI6ICJMU1RNIHVzZXMgYSBzaW1wbGVyIGFjdGl2YXRpb24gZnVuY3Rpb24uIiwgImNvcnJlY3QiOiBmYWxzZSwgImZlZWRiYWNrIjogIkdvb2QgTHVjayBOZXh0IFRpbWUhIn0sIHsiYW5zd2VyIjogIkxTVE0gaGFzIGEgbW9yZSBlZmZpY2llbnQgbWVtb3J5IGNlbGwuIiwgImNvcnJlY3QiOiBmYWxzZSwgImZlZWRiYWNrIjogIj0oIn1dfV0=</span>\n",
    "<span style=\"display:none\" id=\"gru_q4\">W3sicXVlc3Rpb24iOiAibiBhIEdhdGVkIFJlY3VycmVudCBVbml0IChHUlUpIG1vZGVsLCBpZiB5b3Ugc2V0IHRoZSByZXNldCBnYXRlIHRvIGFsbCAxJ3MgYW5kIHRoZSB1cGRhdGUgZ2F0ZSB0byBhbGwgMCdzLCB3aGF0IGJlaGF2aW9yIGRvZXMgdGhlIEdSVSBleGhpYml0PyIsICJ0eXBlIjogIm1hbnlfY2hvaWNlIiwgImFuc3dlcnMiOiBbeyJhbnN3ZXIiOiAiVGhlIEdSVSBvcGVyYXRlcyBhcyBhIG1vcmUgY29tcGxleCB2YXJpYW50IG9mIGFuIExTVE0uIiwgImNvcnJlY3QiOiBmYWxzZSwgImZlZWRiYWNrIjogIldhc3RlZCEifSwgeyJhbnN3ZXIiOiAiVGhlIEdSVSBiZWhhdmVzIGxpa2UgYSBwbGFpbiBSTk4sIHJldGFpbmluZyB0aGUgZW50aXJlIHByZXZpb3VzIHN0YXRlIGFzIGlzLiIsICJjb3JyZWN0IjogdHJ1ZSwgImZlZWRiYWNrIjogIldoZW4geW91IHNldCB0aGUgcmVzZXQgZ2F0ZSB0byBhbGwgMSdzIGFuZCB0aGUgdXBkYXRlIGdhdGUgdG8gYWxsIDAncyBpbiBhIEdSVSwgeW91IGVmZmVjdGl2ZWx5IGRpc2FibGUgdGhlIGdhdGluZyBtZWNoYW5pc21zLCBjYXVzaW5nIHRoZSBHUlUgdG8gYmVoYXZlIGxpa2UgYSBwbGFpbiBSTk4uIFRoaXMgbWVhbnMgdGhhdCB0aGUgZW50aXJlIHByZXZpb3VzIHN0YXRlIGlzIHJldGFpbmVkIHdpdGhvdXQgc2VsZWN0aXZlIHJlc2V0dGluZyBvciB1cGRhdGluZy4ifSwgeyJhbnN3ZXIiOiAiVGhlIEdSVSBleGhpYml0cyBpdHMgdXN1YWwgYmVoYXZpb3IsIHNlbGVjdGl2ZWx5IHJldGFpbmluZyBvciB1cGRhdGluZyBpbmZvcm1hdGlvbi4iLCAiY29ycmVjdCI6IGZhbHNlLCAiZmVlZGJhY2siOiAiR29vZCBMdWNrIE5leHQgVGltZSEifSwgeyJhbnN3ZXIiOiAiVGhlIEdSVSBiZWNvbWVzIHVuc3RhYmxlIGFuZCBmYWlscyB0byBjb252ZXJnZSBkdXJpbmcgdHJhaW5pbmcuIiwgImNvcnJlY3QiOiBmYWxzZSwgImZlZWRiYWNrIjogIj0oIn1dfV0=</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "Gated Recurrent Unit, is a type of Recurrent Neural Network that was introduced by [Kyunghyun Cho](https://kyunghyuncho.me/) in [2014](https://arxiv.org/abs/1412.3555), it aims to solve the __vanishing gradient problem__ which comes with a standard recurrent neural network. To solve the vanishing gradient problem of a standard RNN, GRU uses, so-called, update gate and reset gate. Basically, these are two vectors which decide what information should be passed to the output. GRU can also be considered as a variation on the LSTM because both are designed similarly and, in some cases, produce equally excellent results.\n",
    "\n",
    "```{admonition} What is RNN?\n",
    ":class: dropdown\n",
    "\n",
    "A recurrent neural network (RNN) is the type of artificial neural network (ANN) that is used to address the limitations of traditional neural networks, when it comes to processing sequential data. Traditional approaches to Neural Network Architecture posses a significant drawback, due to which it is unable to handle sequential data effectively and capture the dependencies between inputs. RNN remembers past inputs due to an internal memory which is useful for predicting target values.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "from jupyterquiz import display_quiz\n",
    "display_quiz(\"#gru_q1\", shuffle_answers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind __GRU__ is to use gating mechanisms to selectively update the hidden state of the network at each time step. The gating mechanisms are used to control the flow of information in and out of the network. The GRU has two gating mechanisms, called the reset gate and the update gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "display_quiz(\"#gru_q2\", shuffle_answers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundation\n",
    "![Architecture Comparison](./gru_images/RNN-vs-LSTM-vs-GRU.png)\n",
    "\n",
    "### GRU distinctive characteristics\n",
    "\n",
    "GRU is similar to LSTM, but it has fewer gates. As it was already mentioned above GRU attempts to solve the vanishing gradient problem of a standard RNN. GRU has a simplified architecture with two gates: the `update gate (z)` and `reset gate (r)`. The update gate controls how much of the previous hidden state should be retained, and the reset gate determines how much of the past information to forget.\n",
    "\n",
    ">\n",
    "> - Reset gates are employed to capture short-term dependencies within sequences.\n",
    "> \n",
    "> - Update gates are utilized to capture long-term dependencies within sequences.\n",
    ">\n",
    "\n",
    "```{admonition} Why do we need to solve vanishing gradient problem?\n",
    ":class: dropdown\n",
    "\n",
    "Short-term memory is caused by the infamous vanishing gradient problem, which is also prevalent in other neural network architectures. As the Vanilla RNN models process more steps, it has troubles retaining information from previous steps. \n",
    "\n",
    "![Architecture Comparison](./gru_images/memoryshortness.png)\n",
    "\n",
    "\n",
    "As you can see, the information from the word “Turkic” and “people” is almost non-existent at the final time step. Short-Term memory and the vanishing gradient is due to the nature of back-propagation.\n",
    "\n",
    "```\n",
    "\n",
    "```{admonition} Why does GRU even matter?\n",
    ":class: dropdown\n",
    "\n",
    "In many cases, the performance difference between LSTM and GRU is not significant, and GRU is often preferred due to its simplicity, efficiency and convergence time.\n",
    "\n",
    "```\n",
    "\n",
    "```{admonition} Who is hiding behind the mask?\n",
    ":class: dropdown\n",
    "\n",
    "![PretendingToBe](./gru_images/vanillarnn.jpg)\n",
    "\n",
    "If we set the reset to all 1’s and  update gate to all 0’s we again arrive at our plain RNN model.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "display_quiz(\"#gru_q4\", shuffle_answers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The basic idea of using a gating mechanism to learn long-term dependencies is the same as in a LSTM, but there are a few key differences:\n",
    "\n",
    "* A GRU has two gates, an LSTM has three gates. \n",
    "\n",
    "* GRU doesn’t possess an internal memory.\n",
    "\n",
    "* With three gates, GRU may have faster training times, making it more efficient for larger datasets.\n",
    "\n",
    "* GRU has few parameters. It is computationally efficient and less prone to overfitting, excellent choice for smaller datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "display_quiz(\"#gru_q3\", shuffle_answers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "Now lets’ understand how GRU works. Here we have a GRU cell which more or less similar to an LSTM cell or RNN cell.\n",
    "\n",
    "![GRU_ARCH](./gru_images/GRU.png)\n",
    "\n",
    "At each timestamp $t$, it takes an input $X_t$ and the hidden state $H_{t-1}$ from the previous timestamp $t-1$. Later it outputs a new hidden state $H_t$ which again passed to the next timestamp.\n",
    "\n",
    "### Gates\n",
    "\n",
    "In GRU we have two gates the reset gate and the update gate. The outputs of the gates are given by two fully connected layers with a sigmoid activation function.\n",
    "\n",
    "Gates are given sigmoid activations, forcing their values to lie in the interval ${0,1}$.\n",
    "\n",
    "### Reset Gate (Short term memory) $R$\n",
    "\n",
    "![GRU_ARCH](./gru_images/GRU_RESET_GATE.png)\n",
    "\n",
    "Intuitively, the reset gate controls how much of the previous state we might still want to remember.\n",
    "\n",
    "- The Reset Gate is responsible for the short-term memory of the network i.e the hidden state ($H_t$).\n",
    "- The value of $r_t$ will range from 0 to 1 because of the sigmoid function. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    R_t &= \\sigma(X_t W_{r} + H_{t-1} U_{r} + b_r) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    ">\n",
    "> $t$ - episode in the time space.<br> \n",
    "> $H$ - previous state.<br>\n",
    "> $W_{r}, W_{z}, U_{r}, U_{z}$ - weight parameters.<br>\n",
    "> $b_r, b_z$ - bias parameters.\n",
    ">\n",
    "\n",
    "\n",
    "### Update Gate (Long term memory) $Z$\n",
    "\n",
    "![GRU_ARCH](./gru_images/GRU_UPDATE_GATE.png)\n",
    "\n",
    "\n",
    "On the other hand, an update gate would allow us to control how much of the new state is just a copy of the old one\n",
    "\n",
    "- The update gate ($z_t$) is responsible for determining the amount of previous information that needs to be passed along the next state.\n",
    "- The value of $z_t$ will range from 0 to 1 because of the sigmoid function. \n",
    "- It helps the model to decide to copy all the information from the past and eliminate the risk of vanishing gradient problem.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    Z_t &= \\sigma(X_t W_{z} + H_{t-1} U_{z} + b_z) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    ">\n",
    "> $t$ - episode in the time space.<br> \n",
    "> $H$ - previous state.<br>\n",
    "> $W_{r}, W_{z}, U_{r}, U_{z}$ - weight parameters.<br>\n",
    "> $b_r, b_z$ - bias parameters.\n",
    ">\n",
    "\n",
    "```{tip} Fact\n",
    "This formulas for both gates are the same. The difference comes in the weights and the gate’s usage.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "display_quiz(\"#gru_q7\", shuffle_answers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Candidate Hidden State ($\\tilde{H}$)\n",
    "\n",
    "Initially, we employ the reset gate. Next, we combine the <b>reset gate</b> $R_t$ with the standard updating mechanism, resulting in a <b>candidate hidden state</b> $\\tilde{H}_t$ at time step $t$. \n",
    "\n",
    "This calculation proceeds as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\tilde{H}_t &= \\tanh(X_t W_{h} + R_t \\odot H_{t-1}U_h) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "![GRU_ARCH](./gru_images/GRU_H_CALCULATION.png)\n",
    "\n",
    "1. Multiply the input $X_t$ with a weight $W_h$ and $H_(t-1)$ with a weight $U_h$. \n",
    "2. Calculate the __Hadamard__ (element-wise) product between the reset gate $R_t$ and $U_h H_(t-1)$.\n",
    "3. Sum up the results of step 1 and 2.\n",
    "4. Apply the nonlinear activation function $tanh$.\n",
    "\n",
    "\n",
    "```{tip} Fact\n",
    "If the reset gate $R_t$ are close to 1, the model acts like a regular [Recurrent Neural Network (RNN)](https://fedmug.github.io/kbtu-ml-book/rnn/vanilla_rnn.html). Conversely, when the values in $R_t$ are close to 0, the candidate hidden state is computed using a [Multi-Layer Perceptron (MLP)](https://fedmug.github.io/kbtu-ml-book/mlp/mlp.html) with the current input $X_t$.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "display_quiz(\"#gru_q5\", shuffle_answers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__In the final step__, the network has to compute $H_t$, a vector that carries information for the current unit and passes it along to the network. To accomplish this, we use something called the update gate. This gate decides what information to keep from the current memory content ($\\tilde{Ht}$) and what to retain from previous steps ($H_{t-1}$).\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H_t &= (1-Z_t) \\odot H_{t-1} + Z_t \\odot  \\tilde{H_t} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "![GRU_ARCH](./gru_images/GRU_H__CALCULATION.png)\n",
    "\n",
    "1. Apply element-wise multiplication to the update gate $Z_t$ and $H_{t-1}$.\n",
    "2. Apply element-wise multiplication to $1-Z_t$ and $\\tilde{H_t}$.\n",
    "3. Sum the results from step 1 and 2.\n",
    "\n",
    "\n",
    "\n",
    "```{tip} Fact\n",
    "When $Z_t$ is close to 1, we keep the old state, ignoring the information from the current input $X_t$ and effectively skipping the current time step in the dependency chain. \n",
    "\n",
    "On the other hand, when $Z_t$ is close to 0, the new latent state $H_t$ approaches the candidate latent state $\\tilde{H}_t$.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "display_quiz(\"#gru_q6\", shuffle_answers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture outline\n",
    "Now we can explicitly define crucial elements of GRU\n",
    "\n",
    "1. __Input layer:__ This part takes in a series of things, like words in a sentence or numbers over time.\n",
    "2. __Hidden layer:__ It's like the brain of the machine. It remembers stuff from the past and uses it to understand what's happening now.\n",
    "3. __Reset gate:__ This decides what old stuff to forget. It looks at what it knew before and what's new, then decides how much of the old stuff to throw away.\n",
    "4. __Update gate:__ This figures out how much of the new stuff to use. It looks at what it knew before and what's new, then decides how much of the new stuff to keep.\n",
    "5. __Candidate activation vector:__ This is like a revised version of the old stuff. It gets changed based on the reset gate and mixed with the new stuff. It's like having a fresh perspective on things.\n",
    "6. __Output layer:__ This part looks at the final understanding and tells you what the machine thinks. It could be a single answer, a series of answers, or even the chances of different possibilities, depending on what the machine needs to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Calculation Example\n",
    "Imagine we have input batch: \n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "X = \\begin{bmatrix}\n",
    "        \\begin{bmatrix} 0 & 0 & 0 & 1 \\\\ 1 & 0 & 0 & 0 \\end{bmatrix} &\n",
    "        \\begin{bmatrix} 0 & 1 & 0 & 0 \\\\ 1 & 0 & 0 & 1 \\end{bmatrix} &\n",
    "        \\begin{bmatrix} 0 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\end{bmatrix}\n",
    "    \\end{bmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Weights are randomly initialized at the start:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    W_z &= \\begin{bmatrix} 0.874 & 1.519 \\\\ 1.882 & 1.952 \\\\ -0.527 & 0.565 \\\\ -0.226 & -0.232 \\end{bmatrix}\n",
    "    W_r &= \\begin{bmatrix} 0.157 & -0.16 \\\\ 0.684 & 0.118 \\\\ 0.128 & 0.318 \\\\ 0.139 & 0.41 \\end{bmatrix} \\\\\n",
    "    W_h &= \\begin{bmatrix} -1.218 & 1.131 \\\\ 0.173 & 0.386 \\\\ -0.837 & -0.298 \\\\ 0.34 & 0.465 \\end{bmatrix} \n",
    "    U_z &= \\begin{bmatrix} -0.918 & 0.45 \\\\ -0.254 & 1.204 \\end{bmatrix} \\\\\n",
    "    U_r &= \\begin{bmatrix} 0.356 & 0.36 \\\\ 0.235 & 0.023 \\end{bmatrix}\n",
    "    \n",
    "    U_h &= \\begin{bmatrix} 2.142 & 0.485 \\\\ 1.76 & 0.288 \\end{bmatrix}\n",
    "    b_z, b_r, b_h &= \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "```{tip} Fact\n",
    "\n",
    "At the start of processing a sequence, there's typically no prior context to consider. The model doesn't have any information from previous time steps, so setting the candidate state to 0 is a neutral initialization. It means the model starts with no influence from past information, which is reasonable when processing the first element of a sequence.\n",
    "$\n",
    "\\begin{align}\n",
    "H_{-1} = \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0 & 0 \\\\\n",
    "  0 & 0\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$\n",
    "```\n",
    "\n",
    "### Reset Gate 0 Calculation\n",
    "\n",
    "````{admonition} Question\n",
    ":class: important\n",
    "What is the value of the reset gate at t=0?\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    R_t &= \\sigma(X_t W_{r} + H_{t-1} U_{r} + b_r) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "```{admonition} Answer\n",
    ":class: tip, dropdown\n",
    "$$\n",
    "\\begin{align}\n",
    "X_0 W_{r} = \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0 & 0 & 0 & 1 \\\\\n",
    "  1 & 0 & 0 & 0\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\times \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.1567 & -0.1600\\\\\n",
    "  0.6844 & 0.1181\\\\\n",
    "  0.1278 & 0.3181\\\\\n",
    "  0.1388 & 0.4100\n",
    "  \\end{array}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.1388 & 0.4100\\\\\n",
    "  0.1567 & -0.1600\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "H_{-1} U_{r} = \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0 & 0 \\\\\n",
    "  0 & 0\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\times \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.3557 & 0.3600\\\\\n",
    "  0.2347 & 0.0231\n",
    "  \\end{array}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0 & 0 \\\\\n",
    "  0 & 0\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "    R_0 &= \\sigma\\left(\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.1388 & 0.4100\\\\\n",
    "  0.1567 & -0.1600\n",
    "  \\end{array}\n",
    "\\right]+\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0 & 0 \\\\\n",
    "  0 & 0\n",
    "  \\end{array}\n",
    "\\right]+\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0 & 0 \\\\\n",
    "  0 & 0\n",
    "  \\end{array}\n",
    "\\right]\\right)\n",
    "=\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.5346 & 0.6010 \\\\\n",
    "  0.5390 & 0.4600\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Update Gate 0 Calculation\n",
    "\n",
    "````{admonition} Question\n",
    ":class: important\n",
    "What is the value of the update gate at t=0?\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    Z_t &= \\sigma(X_t W_{z} + H_{t-1} U_{z} + b_z)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "```{admonition} Answer\n",
    ":class: tip, dropdown\n",
    "$$\n",
    "\\begin{align}\n",
    "X_0 W_{z} = \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0 & 0 & 0 & 1 \\\\\n",
    "  1 & 0 & 0 & 0\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\times \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.8740 & 1.5188\\\\\n",
    "  1.8818 & 1.9524\\\\\n",
    "  -0.5267 & 0.5650\\\\\n",
    "  -0.2255 & -0.2316\n",
    "  \\end{array}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  -0.2255 & -0.2316\\\\\n",
    "  0.874 & 1.5188\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H_{-1} U_{z} = \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0 & 0 \\\\\n",
    "  0 & 0\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\times\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  -0.9176 & 0.4503\\\\\n",
    "  -0.2539 & 1.2043\n",
    "  \\end{array}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0 & 0 \\\\\n",
    "  0 & 0\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "b_r = \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0 & 0 \\\\\n",
    "  0 & 0\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "    Z_0 &= \\sigma\\left(\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  -0.2255 & -0.2316\\\\\n",
    "  0.874 & 1.5188\n",
    "  \\end{array}\n",
    "\\right]+\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0 & 0 \\\\\n",
    "  0 & 0\n",
    "  \\end{array}\n",
    "\\right]+\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0 & 0 \\\\\n",
    "  0 & 0\n",
    "  \\end{array}\n",
    "\\right]\\right)\n",
    "=\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.4438 & 0.4423\\\\\n",
    "  0.7055 & 0.8203\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "```\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Candidate State 0 Calculation\n",
    "\n",
    "````{admonition} Question\n",
    ":class: important\n",
    "What is the value of the candidate state at t=0?\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\tilde{H}_t &= \\tanh(X_t W_{h} + (R_t \\odot H_{t-1}) U_{h} + b_h)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "```{admonition} Answer\n",
    ":class: tip, dropdown\n",
    "$$\n",
    "\\begin{align}\n",
    "X_0 W_{h} = \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0 & 0 & 0 & 1 \\\\\n",
    "  1 & 0 & 0 & 0\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\times \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  -1.2179 & 1.1311\\\\\n",
    "  0.1728 & 0.3855\\\\\n",
    "  -0.8367 & -0.2980\\\\\n",
    "  0.3399 & 0.4652\n",
    "  \\end{array}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.3399 & 0.4652\\\\\n",
    "  -1.2179 & 1.1311\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "(R_t \\odot H_{t-1}) U_{h} = \n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0 & 0 \\\\\n",
    "  0 & 0\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\tilde{H}_0 &= \\tanh\\left(\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.3399 & 0.4652\\\\\n",
    "  -1.2179 & 1.1311\n",
    "  \\end{array}\n",
    "\\right] + \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0 & 0 \\\\\n",
    "  0 & 0\n",
    "  \\end{array}\n",
    "\\right] + \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0 & 0 \\\\\n",
    "  0 & 0\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\right)\n",
    "=\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.3273 & 0.4343 \\\\\n",
    "  -0.8390 & 0.8113\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "```\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Hidden State 0 Calculation\n",
    "\n",
    "````{admonition} Question\n",
    ":class: important\n",
    "What is the value of the hidden state at t=0?\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H_t &= (1-Z_t) \\odot H_{t-1} + Z_t \\odot  \\tilde{H}_t \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "```{admonition} Answer\n",
    ":class: tip, dropdown\n",
    "$$\n",
    "\\begin{align}\n",
    "(1-Z_0) \\odot H_{-1} =\n",
    "\\left(\n",
    "1 - \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.4438 & 0.4423\\\\\n",
    "  0.7055 & 0.8203\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\right)  \\odot \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0 & 0 \\\\\n",
    "  0 & 0\n",
    "  \\end{array}\n",
    "\\right] = \n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0 & 0 \\\\\n",
    "  0 & 0\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "Z_t \\odot \\tilde{H}_t =\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.4438 & 0.4423\\\\\n",
    "  0.7055 & 0.8203\n",
    "  \\end{array}\n",
    "\\right] \\odot\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.3273 & 0.4343 \\\\\n",
    "  -0.8390 & 0.8113\n",
    "  \\end{array}\n",
    "\\right] =\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0,1452 & 0.1920 \\\\\n",
    "  -0.5919 & 0.6655\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "H_0 &= \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0,1452 & 0.1920 \\\\\n",
    "  -0.5919 & 0.6655\n",
    "  \\end{array}\n",
    "\\right] \n",
    "\\end{align}\n",
    "$$\n",
    "```\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### $H_1$ Calculation\n",
    "\n",
    "````{admonition} Question\n",
    ":class: important\n",
    "What is the value of the hidden state at t=1?\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "X_1 = \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0 & 1 & 0 & 0 \\\\\n",
    "  1 & 0 & 0 & 1\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    Z_t = \\sigma(X_t W_{z} + H_{t-1} U_{z} + b_z), R_t = \\sigma(X_t W_{r} + H_{t-1} U_{r} + b_r) \n",
    "$$\n",
    "$$\n",
    "\\tilde{H}_t = \\tanh(X_t W_{h} + (R_t \\odot H_{t-1}) U_{h} + b_h)\n",
    "$$\n",
    "$$\n",
    "H_t = (1-Z_t) \\odot H_{t-1} + Z_t \\odot  \\tilde{H}_t \n",
    "$$\n",
    "\n",
    "```{admonition} Update Gate\n",
    ":class: tip, dropdown\n",
    "\n",
    "Update Gate\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "X_1 W_{z} = \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0 & 1 & 0 & 0 \\\\\n",
    "  1 & 0 & 0 & 1\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\times \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.8740 & 1.5188\\\\\n",
    "  1.8818 & 1.9524\\\\\n",
    "  -0.5267 & 0.5650\\\\\n",
    "  -0.2255 & -0.2316\n",
    "  \\end{array}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  1.8818 & 1.9524\\\\\n",
    "  0.6485 & 1.2872\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "H_0 U_{z} = \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0,1452 & 0.1920 \\\\\n",
    "  -0.5919 & 0.6655\n",
    "  \\end{array}\n",
    "\\right] \n",
    "\\times\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  -0.9176 & 0.4503\\\\\n",
    "  -0.2539 & 1.2043\n",
    "  \\end{array}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  -0.1819 & 0.2966 \\\\\n",
    "  -0.3741 & 0.5349\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "    Z_1 &= \\sigma\\left(\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  1.8818 & 1.9524\\\\\n",
    "  0.6485 & 1.2872\n",
    "  \\end{array}\n",
    "\\right]+\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  -0.1819 & 0.2966 \\\\\n",
    "  -0.3741 & 0.5349\n",
    "  \\end{array}\n",
    "\\right]+\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0 & 0 \\\\\n",
    "  0 & 0\n",
    "  \\end{array}\n",
    "\\right]\\right)\n",
    "=\n",
    "\\sigma\\left(\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  1.6999 & 2.249\\\\\n",
    "  0.2744 & 1.8221\n",
    "  \\end{array}\n",
    "\\right]\\right) \n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "  0.4430 & 0.4694\\\\\n",
    "  0.5351 & 0.7156\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "```\n",
    "\n",
    "\n",
    "```{admonition} Reset Gate\n",
    ":class: tip, dropdown\n",
    "\n",
    "Reset Gate\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "X_1 W_{r} = \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0 & 1 & 0 & 0 \\\\\n",
    "  1 & 0 & 0 & 1\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\times \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  -1.2181 & -0.8113\\\\\n",
    "  0.6912 & -1.8349\\\\\n",
    "  1.2232 & -0.0376\\\\\n",
    "  0.7532 & -0.4603\n",
    "  \\end{array}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.6912 & -1.8349\\\\\n",
    "  -0.4649 & -1.2716\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H_{0} U_{r} = \n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0,1452 & 0.1920 \\\\\n",
    "  -0.5919 & 0.6655\n",
    "  \\end{array}\n",
    "\\right] \n",
    "\\times\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.8122 & 0.5938\\\\\n",
    "  -1.9665 & -0.3766\n",
    "  \\end{array}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  -0.2596 & 0.0139 \\\\\n",
    "  -1.7894 & -0.6020\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    R_1 = \\sigma\\left(\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.6912 & -1.8349\\\\\n",
    "  -0.4649 & -1.2716\n",
    "  \\end{array}\n",
    "\\right]+\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  -0.2596 & 0.0139 \\\\\n",
    "  -1.7894 & -0.6020\n",
    "  \\end{array}\n",
    "\\right]+\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0 & 0 \\\\\n",
    "  0 & 0\n",
    "  \\end{array}\n",
    "\\right]\\right)\n",
    "=\n",
    "\\sigma\\left(\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.4316 & -1.8210 \\\\\n",
    "  -2.2543 & -1.8736\n",
    "  \\end{array}\n",
    "\\right]\\right)\n",
    "=\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.6062 & 0.1393 \\\\\n",
    "  0.0949 & 0.1331\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```{admonition} Candidate for Hidden State\n",
    ":class: tip, dropdown\n",
    "\n",
    "Candidate for Hidden State\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "X_1 W_{h} = \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0 & 1 & 0 & 0 \\\\\n",
    "  1 & 0 & 0 & 1\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\times \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.7263 & 1.0992\\\\\n",
    "  0.1728 & 0.1083\\\\\n",
    "  0.5077 & -0.6043\\\\\n",
    " -0.0136 & 1.7521\n",
    "  \\end{array}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.1728 & 0.1083\\\\\n",
    "  0.7127 & 2.8513\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "(R_1 \\odot H_0) U_{h} = \n",
    "\\left(\n",
    "  \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.6062 & 0.1393 \\\\\n",
    "  0.0949 & 0.1331\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\odot\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0,1452 & 0.1920 \\\\\n",
    "  -0.5919 & 0.6655\n",
    "  \\end{array}\n",
    "\\right] \n",
    "\\right)\n",
    "\\times\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  -0.0504 & 0.6106 \\\\\n",
    "  0.7263 & 0.0643\n",
    "  \\end{array}\n",
    "\\right] \n",
    "=\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.0149 & 0.0554 \\\\\n",
    "  0.0671 & -0.0286\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "b_h = \n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0 & 0 \\\\\n",
    "  0 & 0\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\tilde{H}_1 &= \\tanh\\left(\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.1728 & 0.1083\\\\\n",
    "  0.7127 & 2.8513\n",
    "  \\end{array}\n",
    "\\right] + \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.0149 & 0.0554 \\\\\n",
    "  0.0671 & -0.0286\n",
    "  \\end{array}\n",
    "\\right] + \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0 & 0 \\\\\n",
    "  0 & 0\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\right)\n",
    "=\n",
    "\\tanh\\left(\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.1877 & 0.1637 \\\\\n",
    "  0.7798 & 2.8227\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\right)\n",
    "=\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.1855 & 0.1622 \\\\\n",
    "  0.6525 & 0.9929\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "```\n",
    "\n",
    "```{admonition} Hidden State\n",
    ":class: tip, dropdown\n",
    "\n",
    "Hidden State\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "(1-Z_1) \\odot H_0 =\n",
    "\\left(\n",
    "1 - \\left[\n",
    "\\begin{array}{cccc}\n",
    "  0.4430 & 0.4694\\\\\n",
    "  0.5351 & 0.7156\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\right)  \\odot \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0,1452 & 0.1920 \\\\\n",
    "  -0.5919 & 0.6655\n",
    "  \\end{array}\n",
    "\\right]  = \n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0 & 0 \\\\\n",
    "  0 & 0\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "Z_1 \\odot \\tilde{H}_1 =\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.4438 & 0.4423\\\\\n",
    "  0.7055 & 0.8203\n",
    "  \\end{array}\n",
    "\\right] \\odot\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0.3273 & 0.4343 \\\\\n",
    "  -0.8390 & 0.8113\n",
    "  \\end{array}\n",
    "\\right] =\n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0,1452 & 0.1920 \\\\\n",
    "  -0.5919 & 0.6655\n",
    "  \\end{array}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "H_1 &= \\left[\n",
    "  \\begin{array}{cccc}\n",
    "  0,1452 & 0.1920 \\\\\n",
    "  -0.5919 & 0.6655\n",
    "  \\end{array}\n",
    "\\right] \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "```\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU VS LSTM\n",
    "\n",
    "### Glaucoma Detection (Small Dataset)\n",
    "\n",
    "Glaucoma is a sight-threatening eye disease that requires early detection and accurate diagnosis for effective management. Machine learning techniques, such as recurrent neural networks (RNNs), have shown promise in glaucoma detection. We compare the performance of Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), in terms of their accuracies and convergence speeds on a glaucoma dataset. Our goal is to assess which model achieves higher accuracy in detecting glaucoma and which one converges faster during the training process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```{dropdown} Glaucoma code\n",
    "````python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout, GRU, LSTM\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "import plotly.express as px \n",
    "data = pd.read_csv('./gru_datasets//GlaucomaM.csv')\n",
    "data.head()\n",
    "print(data[\"Class\"].value_counts())\n",
    "y = data['Class'].apply(lambda x: 0 if x == 'normal' else 1).to_numpy()\n",
    "x = data.drop(['Class'], axis=1).to_numpy()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "model_gru = Sequential()\n",
    "model_gru.add(GRU(62, return_sequences=True, input_shape=(62, 1)))\n",
    "model_gru.add(Dropout(0.2))\n",
    "model_gru.add(GRU(50, return_sequences=True))\n",
    "model_gru.add(Dropout(0.2))\n",
    "model_gru.add(GRU(50, return_sequences=False))\n",
    "model_gru.add(Dropout(0.2))\n",
    "model_gru.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "model_gru.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_gru.fit(x_train, y_train, epochs=12, batch_size=30, validation_data=(x_test, y_test))\n",
    "y_hat = model_gru.predict(x_test)\n",
    "y_hat = np.round(y_hat).flatten()\n",
    "gru_acc = accuracy_score(y_test, y_hat)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_hat)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Normal', 'Glaucoma'])\n",
    "disp.plot()\n",
    "plt.show()\n",
    "print(\"GRU Accuracy: \", gru_acc)\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(62, return_sequences=True, input_shape=(62, 1)))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(LSTM(50, return_sequences=True))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(LSTM(50, return_sequences=False))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.fit(x_train, y_train, epochs=12, batch_size=30, validation_data=(x_test, y_test))\n",
    "y_hat = model_lstm.predict(x_test)\n",
    "y_hat = np.round(y_hat).flatten()\n",
    "lstm_acc = accuracy_score(y_test, y_hat)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_hat)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Normal', 'Glaucoma'])\n",
    "disp.plot()\n",
    "plt.show()\n",
    "print(\"LSTM Accuracy: \", lstm_acc)\n",
    "plt.bar(['GRU', 'LSTM'], [gru_acc, lstm_acc], color=['green', 'red'])\n",
    "plt.text(0, gru_acc, str(round(gru_acc*100, 3)))\n",
    "plt.text(1, lstm_acc, str(round(lstm_acc*100, 3)))\n",
    "plt.title('GRU vs LSTM')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Model')\n",
    "plt.show()\n",
    "````\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset size of __98__ samples for each class, with 98 samples for \"normal\" and 98 samples for \"glaucoma,\" can be considered relatively small.\n",
    "\n",
    "Comparing the performance of Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM) models on a relatively small dataset for glaucoma detection can provide valuable insights into which recurrent neural network (RNN) architecture is better suited for the task.\n",
    "\n",
    "__Confusion Matrix for GRU__\n",
    "\n",
    "![GRU_ARCH](./gru_images/gru_matrix.png)\n",
    "\n",
    "__Confusion Matrix for LSTM__\n",
    "\n",
    "![GRU_ARCH](./gru_images/lstm_matrix.png)\n",
    "\n",
    "Based on the confusion matrices, we observe that the GRU model had a higher number of correct predictions (true positives and true negatives) compared to the LSTM model. Specifically, the GRU model correctly identified both \"Normal\" and \"Glaucoma\" cases more accurately.\n",
    "\n",
    "__Accuracy comparison__\n",
    "\n",
    "![GRU_ARCH](./gru_images/glaucoma_comparison.png)\n",
    "\n",
    "The GRU model exhibited superior performance on a smaller dataset for glaucoma detection. Its higher accuracy and faster convergence highlight its effectiveness and efficiency in handling limited data resources. This observation underscores the potential of the GRU architecture for accurate and rapid glaucoma detection in scenarios where dataset size is constrained. Further optimization and fine-tuning can enhance its capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "display_quiz(\"#gru_q8\", shuffle_answers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM STOCK (Large Dataset)\n",
    "\n",
    "We're turning our attention to a much bigger dataset - the IBM stock dataset. It's different from the smaller glaucoma dataset we used before because it contains a lot more financial data that spans a long period of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{dropdown} IBM STOCK code\n",
    "````python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, GRU\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interactive\n",
    "dataset = pd.read_csv('./gru_datasets/IBM_Stock_Data.csv', index_col='Date', parse_dates=['Date'])\n",
    "dataset.head()\n",
    "training_set = dataset[:'2016'].iloc[:,1:2].values\n",
    "test_set = dataset['2017':].iloc[:,1:2].values\n",
    "dataset[\"High\"][:'2016'].plot(figsize=(16,4),legend=True)\n",
    "dataset[\"High\"]['2017':].plot(figsize=(16,4),legend=True)\n",
    "plt.legend(['Training set (Before 2017)','Test set (2017 and beyond)'])\n",
    "plt.title('IBM stock price')\n",
    "plt.show()\n",
    "sc = MinMaxScaler(feature_range=(0,1))\n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "len(training_set_scaled)\n",
    "# Some functions to help out with\n",
    "def plot_predictions(test,predicted):\n",
    "    plt.plot(test, color='red',label='Real IBM Stock Price')\n",
    "    plt.plot(predicted, color='blue',label='Predicted IBM Stock Price')\n",
    "    plt.title('IBM Stock Price Prediction')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('IBM Stock Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def return_rmse(test,predicted):\n",
    "    rmse = math.sqrt(mean_squared_error(test, predicted))\n",
    "    print(\"The root mean squared error is {}.\".format(rmse))\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(60,2769):\n",
    "    X_train.append(training_set_scaled[i-60:i,0])\n",
    "    y_train.append(training_set_scaled[i,0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))\n",
    "model_gru = Sequential()\n",
    "model_gru.add(GRU(62, return_sequences=True, input_shape=X_train.shape[1:]))\n",
    "model_gru.add(Dropout(0.2))\n",
    "model_gru.add(GRU(50, return_sequences=True))\n",
    "model_gru.add(Dropout(0.2))\n",
    "model_gru.add(GRU(50, return_sequences=False))\n",
    "model_gru.add(Dropout(0.2))\n",
    "model_gru.add(Dense(1, activation = \"sigmoid\"))\n",
    "model_gru.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "model_gru.fit(X_train,y_train,epochs=15,batch_size=150)\n",
    "dataset_total = pd.concat((dataset[\"High\"][:'2016'],dataset[\"High\"]['2017':]),axis=0)\n",
    "inputs = dataset_total[len(dataset_total)-len(test_set) - 60:].values\n",
    "inputs = inputs.reshape(-1,1)\n",
    "inputs  = sc.transform(inputs)\n",
    "X_test = []\n",
    "for i in range(60,311):\n",
    "    X_test.append(inputs[i-60:i,0])\n",
    "X_test = np.array(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n",
    "predicted_stock_price = model_gru.predict(X_test)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n",
    "plot_predictions(test_set,predicted_stock_price)\n",
    "return_rmse(test_set,predicted_stock_price)\n",
    "model = Sequential()\n",
    "model.add(LSTM(62, return_sequences=True, input_shape=X_train.shape[1:]))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "model.fit(X_train,y_train,epochs=15,batch_size=150)\n",
    "lstm_predicted_stock_price = model.predict(X_test)\n",
    "lstm_predicted_stock_price = sc.inverse_transform(lstm_predicted_stock_price)\n",
    "plot_predictions(test_set,lstm_predicted_stock_price)\n",
    "return_rmse(test_set,lstm_predicted_stock_price)\n",
    "len(test_set), len(predicted_stock_price), len(lstm_predicted_stock_price)\n",
    "plt.plot(test_set, color='red',label='Real IBM Stock Price')\n",
    "plt.plot(predicted_stock_price, color='blue',label='Predicted IBM Stock Price')\n",
    "plt.plot(lstm_predicted_stock_price, color='green',label='Predicted IBM Stock Price')\n",
    "````\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a dataset of 3,020 data points from the IBM stock dataset at our disposal, our goal is to assess how well our models perform under various conditions. This larger dataset allows for a thorough comparison of our model architectures, enabling us to evaluate their effectiveness on a broader scale as they handle a more substantial amount of financial data.\n",
    "\n",
    "__GRU RMSE__\n",
    "\n",
    "`The root mean squared error is 4.329638911537309.`\n",
    "\n",
    "![GRU_ARCH](./gru_images/GRU_IBM.png)\n",
    "\n",
    "__LSTM RMSE__\n",
    "\n",
    "`The root mean squared error is 4.0172206725101836.`\n",
    "\n",
    "![GRU_ARCH](./gru_images/LSTM_IBM.png)\n",
    "\n",
    "We noticed something interesting. When we looked closely at how well the Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM) models performed, we found that the GRU model was a bit less accurate than the LSTM model.\n",
    "\n",
    "Both models did a good job with the big dataset, but the LSTM model performed a little better.\n",
    "\n",
    "The dataset we used was larger and more complicated, which made the task challenging. While the LSTM model had a slight advantage in capturing certain patterns in the financial data, it's important to note that the GRU model wasn't far behind in terms of its ability to do the job well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "display_quiz(\"#gru_q9\", shuffle_answers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The Gated Recurrent Unit (GRU) emerges as a pivotal player in the realm of financial prediction and beyond. In our evaluation of this model alongside its counterparts, it becomes evident that the GRU offers a unique blend of effectiveness, efficiency, and simplicity.\n",
    "\n",
    "\n",
    "1. Comparable Performance: The GRU model demonstrates its mettle by delivering performance on par with other sophisticated models, such as the Long Short-Term Memory (LSTM).\n",
    "\n",
    "2. Efficiency: One of the standout features of the GRU is its swiftness. In our assessments, it not only matches the accuracy of other models but often does so with faster convergence. \n",
    "\n",
    "3. Simplicity: The GRU's elegance lies in its simplicity. It manages to achieve robust results without the complexity associated with some of its counterparts.\n",
    "\n",
    "4. Versatility: Beyond financial prediction, the GRU's versatility extends to various sequence-related tasks, including natural language processing, time series analysis, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "```{toggle}\n",
    "\n",
    "- Books:\n",
    "\n",
    "    1. \"Deep Learning\" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville - This comprehensive book covers various aspects of deep learning, including recurrent neural networks (RNNs).\n",
    "\n",
    "    2. \"Recurrent Neural Networks in Deep Learning\" by Felix Gers, Jürgen Schmidhuber, and Fred Cummins - This book provides an in-depth exploration of recurrent neural networks, including GRUs and LSTMs.\n",
    "\n",
    "    3. \"Sequence to Sequence Learning with Neural Networks\" by Ilya Sutskever, Oriol Vinyals, and Quoc V. Le - This paper introduced the concept of sequence-to-sequence models, which are widely used with RNNs, GRUs, and LSTMs.\n",
    "\n",
    "- Scientific Papers:\n",
    "\n",
    "    1. Hochreiter, S., & Schmidhuber, J. (1997). \"Long Short-Term Memory.\" Neural Computation, 9(8), 1735-1780. - This seminal paper introduces the LSTM architecture.\n",
    "\n",
    "    2. Cho, K., Van Merriënboer, B., Bahdanau, D., & Bengio, Y. (2014). \"On the Properties of Neural Machine Translation: Encoder-Decoder Approaches.\" arXiv preprint arXiv:1409.1259. - This paper discusses the use of RNNs in neural machine translation.\n",
    "\n",
    "    3. Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). \"Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.\" arXiv preprint arXiv:1412.3555. - This paper examines the performance of GRUs in sequence modeling tasks.\n",
    "\n",
    "    4. Graves, A., Mohamed, A. R., & Hinton, G. (2013). \"Speech Recognition with Deep Recurrent Neural Networks.\" In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (pp. 6645-6649). IEEE. - This paper explores the application of RNNs in speech recognition.\n",
    "\n",
    "    5. Schuster, M., & Paliwal, K. K. (1997). \"Bidirectional recurrent neural networks.\" IEEE Transactions on Signal Processing, 45(11), 2673-2681. - This paper discusses bidirectional RNNs, which can be used in conjunction with LSTM and GRU models.\n",
    "\n",
    "- WebPages\n",
    "\n",
    "    1. [GRU Explained using matrices](https://towardsdatascience.com/gate-recurrent-units-explained-using-matrices-part-1-3c781469fc18)\n",
    "\n",
    "    2. [Illustrated Guide to LSTM’s and GRU’s: A step by step explanation](https://towardsdatascience.com/llustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n",
    "\n",
    "    3. [Wikipedia](https://en.wikipedia.org/wiki/Gated_recurrent_unit)\n",
    "\n",
    "    4. [GRU Recurrent Neural Networks — A Smart Way to Predict Sequences in Python](https://towardsdatascience.com/gru-recurrent-neural-networks-a-smart-way-to-predict-sequences-in-python-80864e4fe9f6)\n",
    "\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
